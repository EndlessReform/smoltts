{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create LM initialization for DualAR transformer\n",
    "\n",
    "As of 2024-12-30 we're using Huggingface [SmolLM2-135M](https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct) for pretrained LM initialization. However, it needs some minor formatting changes to work with the Fish Speech / fish_speech.rs format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "checkpoint_dir = \"../checkpoints\"\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "checkpoint_pretrained_dir = f\"../checkpoints/{MODEL.split('/')[-1]}\"\n",
    "os.makedirs(checkpoint_pretrained_dir, exist_ok=True)\n",
    "\n",
    "# Step (b): Download the HuggingFace model and save to ../checkpoints\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "\n",
    "print(\"Downloading model...\")\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ../checkpoints...\n",
      "Model downloaded and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Saving model to {checkpoint_dir}...\")\n",
    "model.save_pretrained(checkpoint_pretrained_dir)\n",
    "tokenizer.save_pretrained(checkpoint_pretrained_dir)\n",
    "\n",
    "print(\"Model downloaded and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import load_file\n",
    "\n",
    "tensors = load_file(\"../checkpoints/SmolLM2-135M-Instruct/model.safetensors\")\n",
    "list(tensors.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the [Fish Speech](https://github.com/fishaudio/fish-speech) DualAR backbone has different weight keys despite being vanilla Llama 3 architecture, so we have to rename them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['embeddings.weight',\n",
       " 'layers.0.ffn_norm.weight',\n",
       " 'layers.0.feed_forward.w2.weight',\n",
       " 'layers.0.feed_forward.w1.weight',\n",
       " 'layers.0.feed_forward.w3.weight',\n",
       " 'layers.0.attention_norm.weight',\n",
       " 'layers.0.attention.wk.weight',\n",
       " 'layers.0.attention.wo.weight',\n",
       " 'layers.0.attention.wq.weight',\n",
       " 'layers.0.attention.wv.weight',\n",
       " 'layers.1.ffn_norm.weight',\n",
       " 'layers.1.feed_forward.w2.weight',\n",
       " 'layers.1.feed_forward.w1.weight',\n",
       " 'layers.1.feed_forward.w3.weight',\n",
       " 'layers.1.attention_norm.weight',\n",
       " 'layers.1.attention.wk.weight',\n",
       " 'layers.1.attention.wo.weight',\n",
       " 'layers.1.attention.wq.weight',\n",
       " 'layers.1.attention.wv.weight',\n",
       " 'layers.10.ffn_norm.weight',\n",
       " 'layers.10.feed_forward.w2.weight',\n",
       " 'layers.10.feed_forward.w1.weight',\n",
       " 'layers.10.feed_forward.w3.weight',\n",
       " 'layers.10.attention_norm.weight',\n",
       " 'layers.10.attention.wk.weight',\n",
       " 'layers.10.attention.wo.weight',\n",
       " 'layers.10.attention.wq.weight',\n",
       " 'layers.10.attention.wv.weight',\n",
       " 'layers.11.ffn_norm.weight',\n",
       " 'layers.11.feed_forward.w2.weight',\n",
       " 'layers.11.feed_forward.w1.weight',\n",
       " 'layers.11.feed_forward.w3.weight',\n",
       " 'layers.11.attention_norm.weight',\n",
       " 'layers.11.attention.wk.weight',\n",
       " 'layers.11.attention.wo.weight',\n",
       " 'layers.11.attention.wq.weight',\n",
       " 'layers.11.attention.wv.weight',\n",
       " 'layers.12.ffn_norm.weight',\n",
       " 'layers.12.feed_forward.w2.weight',\n",
       " 'layers.12.feed_forward.w1.weight',\n",
       " 'layers.12.feed_forward.w3.weight',\n",
       " 'layers.12.attention_norm.weight',\n",
       " 'layers.12.attention.wk.weight',\n",
       " 'layers.12.attention.wo.weight',\n",
       " 'layers.12.attention.wq.weight',\n",
       " 'layers.12.attention.wv.weight',\n",
       " 'layers.13.ffn_norm.weight',\n",
       " 'layers.13.feed_forward.w2.weight',\n",
       " 'layers.13.feed_forward.w1.weight',\n",
       " 'layers.13.feed_forward.w3.weight',\n",
       " 'layers.13.attention_norm.weight',\n",
       " 'layers.13.attention.wk.weight',\n",
       " 'layers.13.attention.wo.weight',\n",
       " 'layers.13.attention.wq.weight',\n",
       " 'layers.13.attention.wv.weight',\n",
       " 'layers.14.ffn_norm.weight',\n",
       " 'layers.14.feed_forward.w2.weight',\n",
       " 'layers.14.feed_forward.w1.weight',\n",
       " 'layers.14.feed_forward.w3.weight',\n",
       " 'layers.14.attention_norm.weight',\n",
       " 'layers.14.attention.wk.weight',\n",
       " 'layers.14.attention.wo.weight',\n",
       " 'layers.14.attention.wq.weight',\n",
       " 'layers.14.attention.wv.weight',\n",
       " 'layers.15.ffn_norm.weight',\n",
       " 'layers.15.feed_forward.w2.weight',\n",
       " 'layers.15.feed_forward.w1.weight',\n",
       " 'layers.15.feed_forward.w3.weight',\n",
       " 'layers.15.attention_norm.weight',\n",
       " 'layers.15.attention.wk.weight',\n",
       " 'layers.15.attention.wo.weight',\n",
       " 'layers.15.attention.wq.weight',\n",
       " 'layers.15.attention.wv.weight',\n",
       " 'layers.16.ffn_norm.weight',\n",
       " 'layers.16.feed_forward.w2.weight',\n",
       " 'layers.16.feed_forward.w1.weight',\n",
       " 'layers.16.feed_forward.w3.weight',\n",
       " 'layers.16.attention_norm.weight',\n",
       " 'layers.16.attention.wk.weight',\n",
       " 'layers.16.attention.wo.weight',\n",
       " 'layers.16.attention.wq.weight',\n",
       " 'layers.16.attention.wv.weight',\n",
       " 'layers.17.ffn_norm.weight',\n",
       " 'layers.17.feed_forward.w2.weight',\n",
       " 'layers.17.feed_forward.w1.weight',\n",
       " 'layers.17.feed_forward.w3.weight',\n",
       " 'layers.17.attention_norm.weight',\n",
       " 'layers.17.attention.wk.weight',\n",
       " 'layers.17.attention.wo.weight',\n",
       " 'layers.17.attention.wq.weight',\n",
       " 'layers.17.attention.wv.weight',\n",
       " 'layers.18.ffn_norm.weight',\n",
       " 'layers.18.feed_forward.w2.weight',\n",
       " 'layers.18.feed_forward.w1.weight',\n",
       " 'layers.18.feed_forward.w3.weight',\n",
       " 'layers.18.attention_norm.weight',\n",
       " 'layers.18.attention.wk.weight',\n",
       " 'layers.18.attention.wo.weight',\n",
       " 'layers.18.attention.wq.weight',\n",
       " 'layers.18.attention.wv.weight',\n",
       " 'layers.19.ffn_norm.weight',\n",
       " 'layers.19.feed_forward.w2.weight',\n",
       " 'layers.19.feed_forward.w1.weight',\n",
       " 'layers.19.feed_forward.w3.weight',\n",
       " 'layers.19.attention_norm.weight',\n",
       " 'layers.19.attention.wk.weight',\n",
       " 'layers.19.attention.wo.weight',\n",
       " 'layers.19.attention.wq.weight',\n",
       " 'layers.19.attention.wv.weight',\n",
       " 'layers.2.ffn_norm.weight',\n",
       " 'layers.2.feed_forward.w2.weight',\n",
       " 'layers.2.feed_forward.w1.weight',\n",
       " 'layers.2.feed_forward.w3.weight',\n",
       " 'layers.2.attention_norm.weight',\n",
       " 'layers.2.attention.wk.weight',\n",
       " 'layers.2.attention.wo.weight',\n",
       " 'layers.2.attention.wq.weight',\n",
       " 'layers.2.attention.wv.weight',\n",
       " 'layers.20.ffn_norm.weight',\n",
       " 'layers.20.feed_forward.w2.weight',\n",
       " 'layers.20.feed_forward.w1.weight',\n",
       " 'layers.20.feed_forward.w3.weight',\n",
       " 'layers.20.attention_norm.weight',\n",
       " 'layers.20.attention.wk.weight',\n",
       " 'layers.20.attention.wo.weight',\n",
       " 'layers.20.attention.wq.weight',\n",
       " 'layers.20.attention.wv.weight',\n",
       " 'layers.21.ffn_norm.weight',\n",
       " 'layers.21.feed_forward.w2.weight',\n",
       " 'layers.21.feed_forward.w1.weight',\n",
       " 'layers.21.feed_forward.w3.weight',\n",
       " 'layers.21.attention_norm.weight',\n",
       " 'layers.21.attention.wk.weight',\n",
       " 'layers.21.attention.wo.weight',\n",
       " 'layers.21.attention.wq.weight',\n",
       " 'layers.21.attention.wv.weight',\n",
       " 'layers.22.ffn_norm.weight',\n",
       " 'layers.22.feed_forward.w2.weight',\n",
       " 'layers.22.feed_forward.w1.weight',\n",
       " 'layers.22.feed_forward.w3.weight',\n",
       " 'layers.22.attention_norm.weight',\n",
       " 'layers.22.attention.wk.weight',\n",
       " 'layers.22.attention.wo.weight',\n",
       " 'layers.22.attention.wq.weight',\n",
       " 'layers.22.attention.wv.weight',\n",
       " 'layers.23.ffn_norm.weight',\n",
       " 'layers.23.feed_forward.w2.weight',\n",
       " 'layers.23.feed_forward.w1.weight',\n",
       " 'layers.23.feed_forward.w3.weight',\n",
       " 'layers.23.attention_norm.weight',\n",
       " 'layers.23.attention.wk.weight',\n",
       " 'layers.23.attention.wo.weight',\n",
       " 'layers.23.attention.wq.weight',\n",
       " 'layers.23.attention.wv.weight',\n",
       " 'layers.24.ffn_norm.weight',\n",
       " 'layers.24.feed_forward.w2.weight',\n",
       " 'layers.24.feed_forward.w1.weight',\n",
       " 'layers.24.feed_forward.w3.weight',\n",
       " 'layers.24.attention_norm.weight',\n",
       " 'layers.24.attention.wk.weight',\n",
       " 'layers.24.attention.wo.weight',\n",
       " 'layers.24.attention.wq.weight',\n",
       " 'layers.24.attention.wv.weight',\n",
       " 'layers.25.ffn_norm.weight',\n",
       " 'layers.25.feed_forward.w2.weight',\n",
       " 'layers.25.feed_forward.w1.weight',\n",
       " 'layers.25.feed_forward.w3.weight',\n",
       " 'layers.25.attention_norm.weight',\n",
       " 'layers.25.attention.wk.weight',\n",
       " 'layers.25.attention.wo.weight',\n",
       " 'layers.25.attention.wq.weight',\n",
       " 'layers.25.attention.wv.weight',\n",
       " 'layers.26.ffn_norm.weight',\n",
       " 'layers.26.feed_forward.w2.weight',\n",
       " 'layers.26.feed_forward.w1.weight',\n",
       " 'layers.26.feed_forward.w3.weight',\n",
       " 'layers.26.attention_norm.weight',\n",
       " 'layers.26.attention.wk.weight',\n",
       " 'layers.26.attention.wo.weight',\n",
       " 'layers.26.attention.wq.weight',\n",
       " 'layers.26.attention.wv.weight',\n",
       " 'layers.27.ffn_norm.weight',\n",
       " 'layers.27.feed_forward.w2.weight',\n",
       " 'layers.27.feed_forward.w1.weight',\n",
       " 'layers.27.feed_forward.w3.weight',\n",
       " 'layers.27.attention_norm.weight',\n",
       " 'layers.27.attention.wk.weight',\n",
       " 'layers.27.attention.wo.weight',\n",
       " 'layers.27.attention.wq.weight',\n",
       " 'layers.27.attention.wv.weight',\n",
       " 'layers.28.ffn_norm.weight',\n",
       " 'layers.28.feed_forward.w2.weight',\n",
       " 'layers.28.feed_forward.w1.weight',\n",
       " 'layers.28.feed_forward.w3.weight',\n",
       " 'layers.28.attention_norm.weight',\n",
       " 'layers.28.attention.wk.weight',\n",
       " 'layers.28.attention.wo.weight',\n",
       " 'layers.28.attention.wq.weight',\n",
       " 'layers.28.attention.wv.weight',\n",
       " 'layers.29.ffn_norm.weight',\n",
       " 'layers.29.feed_forward.w2.weight',\n",
       " 'layers.29.feed_forward.w1.weight',\n",
       " 'layers.29.feed_forward.w3.weight',\n",
       " 'layers.29.attention_norm.weight',\n",
       " 'layers.29.attention.wk.weight',\n",
       " 'layers.29.attention.wo.weight',\n",
       " 'layers.29.attention.wq.weight',\n",
       " 'layers.29.attention.wv.weight',\n",
       " 'layers.3.ffn_norm.weight',\n",
       " 'layers.3.feed_forward.w2.weight',\n",
       " 'layers.3.feed_forward.w1.weight',\n",
       " 'layers.3.feed_forward.w3.weight',\n",
       " 'layers.3.attention_norm.weight',\n",
       " 'layers.3.attention.wk.weight',\n",
       " 'layers.3.attention.wo.weight',\n",
       " 'layers.3.attention.wq.weight',\n",
       " 'layers.3.attention.wv.weight',\n",
       " 'layers.4.ffn_norm.weight',\n",
       " 'layers.4.feed_forward.w2.weight',\n",
       " 'layers.4.feed_forward.w1.weight',\n",
       " 'layers.4.feed_forward.w3.weight',\n",
       " 'layers.4.attention_norm.weight',\n",
       " 'layers.4.attention.wk.weight',\n",
       " 'layers.4.attention.wo.weight',\n",
       " 'layers.4.attention.wq.weight',\n",
       " 'layers.4.attention.wv.weight',\n",
       " 'layers.5.ffn_norm.weight',\n",
       " 'layers.5.feed_forward.w2.weight',\n",
       " 'layers.5.feed_forward.w1.weight',\n",
       " 'layers.5.feed_forward.w3.weight',\n",
       " 'layers.5.attention_norm.weight',\n",
       " 'layers.5.attention.wk.weight',\n",
       " 'layers.5.attention.wo.weight',\n",
       " 'layers.5.attention.wq.weight',\n",
       " 'layers.5.attention.wv.weight',\n",
       " 'layers.6.ffn_norm.weight',\n",
       " 'layers.6.feed_forward.w2.weight',\n",
       " 'layers.6.feed_forward.w1.weight',\n",
       " 'layers.6.feed_forward.w3.weight',\n",
       " 'layers.6.attention_norm.weight',\n",
       " 'layers.6.attention.wk.weight',\n",
       " 'layers.6.attention.wo.weight',\n",
       " 'layers.6.attention.wq.weight',\n",
       " 'layers.6.attention.wv.weight',\n",
       " 'layers.7.ffn_norm.weight',\n",
       " 'layers.7.feed_forward.w2.weight',\n",
       " 'layers.7.feed_forward.w1.weight',\n",
       " 'layers.7.feed_forward.w3.weight',\n",
       " 'layers.7.attention_norm.weight',\n",
       " 'layers.7.attention.wk.weight',\n",
       " 'layers.7.attention.wo.weight',\n",
       " 'layers.7.attention.wq.weight',\n",
       " 'layers.7.attention.wv.weight',\n",
       " 'layers.8.ffn_norm.weight',\n",
       " 'layers.8.feed_forward.w2.weight',\n",
       " 'layers.8.feed_forward.w1.weight',\n",
       " 'layers.8.feed_forward.w3.weight',\n",
       " 'layers.8.attention_norm.weight',\n",
       " 'layers.8.attention.wk.weight',\n",
       " 'layers.8.attention.wo.weight',\n",
       " 'layers.8.attention.wq.weight',\n",
       " 'layers.8.attention.wv.weight',\n",
       " 'layers.9.ffn_norm.weight',\n",
       " 'layers.9.feed_forward.w2.weight',\n",
       " 'layers.9.feed_forward.w1.weight',\n",
       " 'layers.9.feed_forward.w3.weight',\n",
       " 'layers.9.attention_norm.weight',\n",
       " 'layers.9.attention.wk.weight',\n",
       " 'layers.9.attention.wo.weight',\n",
       " 'layers.9.attention.wq.weight',\n",
       " 'layers.9.attention.wv.weight',\n",
       " 'norm.weight']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "renamed_tensors = {\n",
    "    key.replace('model.embed_tokens', 'model.embeddings')\n",
    "       .replace('self_attn', 'attention')\n",
    "       .replace('post_attention_layernorm', 'attention_norm')\n",
    "       .replace('input_layernorm', 'ffn_norm')\n",
    "       .replace('mlp', 'feed_forward')\n",
    "       .replace('k_proj', 'wk')\n",
    "       .replace('q_proj', 'wq')\n",
    "       .replace('v_proj', 'wv')\n",
    "       .replace('o_proj', 'wo')\n",
    "       .replace('gate_proj', 'w1')\n",
    "       .replace('down_proj', 'w2')\n",
    "       .replace('up_proj', 'w3')\n",
    "       .split('model.')[1]: tensor \n",
    "    for key, tensor in tensors.items()\n",
    "}\n",
    "list(renamed_tensors.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following existing literature, we initialize the semantic codebook embedding embeddings from the mean of the existing token embeddings, to lower the initial loss from random init. Empirically this lowers base loss from 140 to 25 at the beginning of training, which though still far above `ln(52000)=10` for the base is good enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([51200, 576])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "new_tokens = renamed_tensors['embeddings.weight'].mean(dim=0, keepdim=True).repeat(2048, 1)\n",
    "# nn.Embedding(2048, 576)\n",
    "extended_embeddings = torch.cat([\n",
    "    renamed_tensors['embeddings.weight'],\n",
    "    new_tokens\n",
    "], dim=0)\n",
    "\n",
    "renamed_tensors['embeddings.weight'] = extended_embeddings\n",
    "renamed_tensors['embeddings.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "source_dir = Path(checkpoint_pretrained_dir)\n",
    "dest_dir = Path(\"../checkpoints/smoltts_init\")\n",
    "\n",
    "os.makedirs(dest_dir, exist_ok=True)\n",
    "torch.save(renamed_tensors, dest_dir / \"model.pth\")\n",
    "\n",
    "\n",
    "# Ensure the destination directory exists\n",
    "dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy all .json and .txt files\n",
    "for extension in (\"*.json\", \"*.txt\"):\n",
    "    for file in source_dir.glob(extension):\n",
    "        shutil.copy(file, dest_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fish Speech uses a different config format than HF Transformers, so I'm going to define it by fiat here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_qkv_bias': False,\n",
       " 'codebook_size': 2048,\n",
       " 'dim': 576,\n",
       " 'dropout': 0.1,\n",
       " 'fast_attention_qkv_bias': False,\n",
       " 'fast_dim': 576,\n",
       " 'fast_head_dim': 64,\n",
       " 'fast_intermediate_size': 1536,\n",
       " 'fast_n_head': 9,\n",
       " 'fast_n_local_heads': 3,\n",
       " 'head_dim': 64,\n",
       " 'initializer_range': 0.041666666666666664,\n",
       " 'intermediate_size': 1536,\n",
       " 'is_reward_model': False,\n",
       " 'max_seq_len': 8192,\n",
       " 'model_type': 'dual_ar',\n",
       " 'n_fast_layer': 4,\n",
       " 'n_head': 9,\n",
       " 'n_local_heads': 3,\n",
       " 'norm_eps': 1e-05,\n",
       " 'num_codebooks': 8,\n",
       " 'rope_base': 100000,\n",
       " 'scale_codebook_embeddings': False,\n",
       " 'share_codebook_embeddings': True,\n",
       " 'tie_word_embeddings': True,\n",
       " 'use_gradient_checkpointing': True,\n",
       " 'vocab_size': 51200}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(dest_dir / \"config.json\") as f:\n",
    "    hf_config = json.load(f)\n",
    "\n",
    "# Mimi codebook dimension\n",
    "CODEBOOK_SIZE = 2048\n",
    "\n",
    "config = {\n",
    "    \"attention_qkv_bias\": False,\n",
    "    \"codebook_size\": CODEBOOK_SIZE,\n",
    "    \"dim\": hf_config[\"hidden_size\"],\n",
    "    \"dropout\": 0.1,\n",
    "    \"fast_attention_qkv_bias\": False,\n",
    "    # TODO: Following Fish Speech, keeping fast layer dimensions the same for now. May revisit this later\n",
    "    \"fast_dim\": hf_config[\"hidden_size\"],\n",
    "    \"fast_head_dim\": hf_config[\"head_dim\"],\n",
    "    \"fast_intermediate_size\": hf_config[\"intermediate_size\"],\n",
    "    \"fast_n_head\": hf_config[\"num_attention_heads\"],\n",
    "    \"fast_n_local_heads\": hf_config[\"num_key_value_heads\"],\n",
    "    \"head_dim\": hf_config[\"head_dim\"],\n",
    "    \"initializer_range\": hf_config[\"initializer_range\"],\n",
    "    \"intermediate_size\": hf_config[\"intermediate_size\"],\n",
    "    \"is_reward_model\": False,\n",
    "    \"max_seq_len\": hf_config[\"max_position_embeddings\"],\n",
    "    \"model_type\": \"dual_ar\",\n",
    "    # TODO: Following Fish Speech for now\n",
    "    \"n_fast_layer\": 4,\n",
    "    \"n_head\": hf_config[\"num_attention_heads\"],\n",
    "    \"n_local_heads\": hf_config[\"num_key_value_heads\"],\n",
    "    \"norm_eps\": hf_config[\"rms_norm_eps\"],\n",
    "    # Mimi\n",
    "    \"num_codebooks\": 8,\n",
    "    \"rope_base\": hf_config[\"rope_theta\"],\n",
    "    \"scale_codebook_embeddings\": False,\n",
    "    \"share_codebook_embeddings\": True,\n",
    "    \"tie_word_embeddings\": hf_config[\"tie_word_embeddings\"],\n",
    "    \"use_gradient_checkpointing\": True,\n",
    "    # TODO: handle control tokens\n",
    "    \"vocab_size\": hf_config[\"vocab_size\"] + CODEBOOK_SIZE\n",
    "}\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = dest_dir / \"config.json\"\n",
    "with output_path.open('w') as f:\n",
    "    json.dump(config, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model now must:\n",
    "- Randomly initialize the fast transformer\n",
    "- Merge attention qkv into a single tensor (to save on kernel launch overhead and improve hardware utilization) \n",
    "\n",
    "The DualARTransformer modeling code will do this, but we need to load the model once.\n",
    "\n",
    "TODO: find more principled initialization strategies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (19371 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../checkpoints/smoltts_init, config: DualARModelArgs(model_type='dual_ar', vocab_size=51200, n_layer=32, n_head=9, dim=576, intermediate_size=1536, n_local_heads=3, head_dim=64, rope_base=100000, norm_eps=1e-05, max_seq_len=8192, dropout=0.1, tie_word_embeddings=True, attention_qkv_bias=False, codebook_size=2048, num_codebooks=8, use_gradient_checkpointing=True, initializer_range=0.041666666666666664, is_reward_model=False, share_codebook_embeddings=True, scale_codebook_embeddings=False, n_fast_layer=4, fast_dim=576, fast_n_head=9, fast_n_local_heads=3, fast_head_dim=64, fast_intermediate_size=1536, fast_attention_qkv_bias=False)\n",
      "No weight for codebook_embeddings.weight\n",
      "No weight for layers.0.attention.wqkv.weight\n",
      "No weight for layers.1.attention.wqkv.weight\n",
      "No weight for layers.2.attention.wqkv.weight\n",
      "No weight for layers.3.attention.wqkv.weight\n",
      "No weight for layers.4.attention.wqkv.weight\n",
      "No weight for layers.5.attention.wqkv.weight\n",
      "No weight for layers.6.attention.wqkv.weight\n",
      "No weight for layers.7.attention.wqkv.weight\n",
      "No weight for layers.8.attention.wqkv.weight\n",
      "No weight for layers.9.attention.wqkv.weight\n",
      "No weight for layers.10.attention.wqkv.weight\n",
      "No weight for layers.11.attention.wqkv.weight\n",
      "No weight for layers.12.attention.wqkv.weight\n",
      "No weight for layers.13.attention.wqkv.weight\n",
      "No weight for layers.14.attention.wqkv.weight\n",
      "No weight for layers.15.attention.wqkv.weight\n",
      "No weight for layers.16.attention.wqkv.weight\n",
      "No weight for layers.17.attention.wqkv.weight\n",
      "No weight for layers.18.attention.wqkv.weight\n",
      "No weight for layers.19.attention.wqkv.weight\n",
      "No weight for layers.20.attention.wqkv.weight\n",
      "No weight for layers.21.attention.wqkv.weight\n",
      "No weight for layers.22.attention.wqkv.weight\n",
      "No weight for layers.23.attention.wqkv.weight\n",
      "No weight for layers.24.attention.wqkv.weight\n",
      "No weight for layers.25.attention.wqkv.weight\n",
      "No weight for layers.26.attention.wqkv.weight\n",
      "No weight for layers.27.attention.wqkv.weight\n",
      "No weight for layers.28.attention.wqkv.weight\n",
      "No weight for layers.29.attention.wqkv.weight\n",
      "No weight for layers.30.attention.wqkv.weight\n",
      "No weight for layers.30.attention.wo.weight\n",
      "No weight for layers.30.feed_forward.w1.weight\n",
      "No weight for layers.30.feed_forward.w3.weight\n",
      "No weight for layers.30.feed_forward.w2.weight\n",
      "No weight for layers.30.ffn_norm.weight\n",
      "No weight for layers.30.attention_norm.weight\n",
      "No weight for layers.31.attention.wqkv.weight\n",
      "No weight for layers.31.attention.wo.weight\n",
      "No weight for layers.31.feed_forward.w1.weight\n",
      "No weight for layers.31.feed_forward.w3.weight\n",
      "No weight for layers.31.feed_forward.w2.weight\n",
      "No weight for layers.31.ffn_norm.weight\n",
      "No weight for layers.31.attention_norm.weight\n",
      "No weight for fast_embeddings.weight\n",
      "No weight for fast_layers.0.attention.wqkv.weight\n",
      "No weight for fast_layers.0.attention.wo.weight\n",
      "No weight for fast_layers.0.feed_forward.w1.weight\n",
      "No weight for fast_layers.0.feed_forward.w3.weight\n",
      "No weight for fast_layers.0.feed_forward.w2.weight\n",
      "No weight for fast_layers.0.ffn_norm.weight\n",
      "No weight for fast_layers.0.attention_norm.weight\n",
      "No weight for fast_layers.1.attention.wqkv.weight\n",
      "No weight for fast_layers.1.attention.wo.weight\n",
      "No weight for fast_layers.1.feed_forward.w1.weight\n",
      "No weight for fast_layers.1.feed_forward.w3.weight\n",
      "No weight for fast_layers.1.feed_forward.w2.weight\n",
      "No weight for fast_layers.1.ffn_norm.weight\n",
      "No weight for fast_layers.1.attention_norm.weight\n",
      "No weight for fast_layers.2.attention.wqkv.weight\n",
      "No weight for fast_layers.2.attention.wo.weight\n",
      "No weight for fast_layers.2.feed_forward.w1.weight\n",
      "No weight for fast_layers.2.feed_forward.w3.weight\n",
      "No weight for fast_layers.2.feed_forward.w2.weight\n",
      "No weight for fast_layers.2.ffn_norm.weight\n",
      "No weight for fast_layers.2.attention_norm.weight\n",
      "No weight for fast_layers.3.attention.wqkv.weight\n",
      "No weight for fast_layers.3.attention.wo.weight\n",
      "No weight for fast_layers.3.feed_forward.w1.weight\n",
      "No weight for fast_layers.3.feed_forward.w3.weight\n",
      "No weight for fast_layers.3.feed_forward.w2.weight\n",
      "No weight for fast_layers.3.ffn_norm.weight\n",
      "No weight for fast_layers.3.attention_norm.weight\n",
      "No weight for fast_norm.weight\n",
      "No weight for fast_output.weight\n",
      "Loaded weights with error: _IncompatibleKeys(missing_keys=['codebook_embeddings.weight', 'layers.30.attention.wqkv.weight', 'layers.30.attention.wo.weight', 'layers.30.feed_forward.w1.weight', 'layers.30.feed_forward.w3.weight', 'layers.30.feed_forward.w2.weight', 'layers.30.ffn_norm.weight', 'layers.30.attention_norm.weight', 'layers.31.attention.wqkv.weight', 'layers.31.attention.wo.weight', 'layers.31.feed_forward.w1.weight', 'layers.31.feed_forward.w3.weight', 'layers.31.feed_forward.w2.weight', 'layers.31.ffn_norm.weight', 'layers.31.attention_norm.weight', 'fast_embeddings.weight', 'fast_layers.0.attention.wqkv.weight', 'fast_layers.0.attention.wo.weight', 'fast_layers.0.feed_forward.w1.weight', 'fast_layers.0.feed_forward.w3.weight', 'fast_layers.0.feed_forward.w2.weight', 'fast_layers.0.ffn_norm.weight', 'fast_layers.0.attention_norm.weight', 'fast_layers.1.attention.wqkv.weight', 'fast_layers.1.attention.wo.weight', 'fast_layers.1.feed_forward.w1.weight', 'fast_layers.1.feed_forward.w3.weight', 'fast_layers.1.feed_forward.w2.weight', 'fast_layers.1.ffn_norm.weight', 'fast_layers.1.attention_norm.weight', 'fast_layers.2.attention.wqkv.weight', 'fast_layers.2.attention.wo.weight', 'fast_layers.2.feed_forward.w1.weight', 'fast_layers.2.feed_forward.w3.weight', 'fast_layers.2.feed_forward.w2.weight', 'fast_layers.2.ffn_norm.weight', 'fast_layers.2.attention_norm.weight', 'fast_layers.3.attention.wqkv.weight', 'fast_layers.3.attention.wo.weight', 'fast_layers.3.feed_forward.w1.weight', 'fast_layers.3.feed_forward.w3.weight', 'fast_layers.3.feed_forward.w2.weight', 'fast_layers.3.ffn_norm.weight', 'fast_layers.3.attention_norm.weight', 'fast_norm.weight', 'fast_output.weight'], unexpected_keys=[])\n"
     ]
    }
   ],
   "source": [
    "from dual_ar.model.dual_ar import DualARTransformer\n",
    "\n",
    "model = DualARTransformer.from_pretrained(\n",
    "    path=\"../checkpoints/smoltts_init\",\n",
    "    load_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()\n",
    "torch.save(state_dict, dest_dir / \"model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now done with modeling code. Now we need to extend the tokenizer to handle semantic tokens.\n",
    "\n",
    "TODO: Add control / modality tokens, PAD / EPAD and do ablations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokenizer():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL, use_system_prompt=False)\n",
    "    semantic_tokens = [f\"<|semantic:{i}|>\" for i in range(0, CODEBOOK_SIZE)]\n",
    "    additional_special_tokens = [*semantic_tokens]\n",
    "    tokenizer.add_special_tokens({\n",
    "        \"additional_special_tokens\": additional_special_tokens\n",
    "    })\n",
    "    # Remove inane overly clever chat template\n",
    "    if MODEL == \"HuggingFaceTB/SmolLM2-135M-Instruct\":\n",
    "        tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\n",
    "    \n",
    "    tokenizer.save_pretrained(dest_dir)\n",
    "\n",
    "make_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: test model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\"\n",
    "model = model.to(device)\n",
    "model = model.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "model = torch.compile(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritsuko/ai/audio/dual-ar/.venv/lib/python3.9/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  3.5625,  -6.3438,  -5.9688,  ...,  -4.8438, -16.5000,   9.3125]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.zeros(1, 9, 1, dtype=torch.int32).to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    out = model.forward(tensor, None)\n",
    "    print(out.token_logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
