{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize LibriTTS-R Mimi for target LM\n",
    "\n",
    "For our dataset, we currently simply use the Fish Speech TTS format:\n",
    "- Text-only data formatted using [ChatML](https://gist.github.com/edwardzjl/8df07c1f7140c9a3e2f48d33a8032090) as a separate sequence \"above\" the audio code stream\n",
    "- During sections where audio is being modeled, text stream 0 predicts the first semantic token index $n$ of the 8 Mimi residual codes as special token `<|semantic:n|>`\n",
    "- For audio, \"semantic\" (neural, there's not a strong distinction between) codes (from Mimi) padded with 0s during text sections\n",
    "\n",
    "It's possible this tokenization strategy can be improved, e.g. in [Defossez et al. 2024](https://arxiv.org/html/2410.00037v2#S3.SS4.SSS4) with the base transformer predicting the Whisper-timestamped word timings as an \"inner monologue\" and a delay between codebook timesteps. lol i'll do it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, concatenate_datasets, load_from_disk\n",
    "\n",
    "# If creating the libritts dataset for the first time\n",
    "# from datasets import load_from_disk \n",
    "dataset = load_from_disk(\"../../datasets/encoded_libritts\")\n",
    "# train_clean_100 = load_from_disk(\"encoded_libritts/train.clean.100/\")\n",
    "# train_clean_360 = load_from_disk(\"encoded_libritts/train.clean.360/\")\n",
    "# dev_clean = load_from_disk(\"encoded_libritts/dev.clean\")\n",
    "# test_clean = load_from_disk(\"encoded_libritts/test.clean\")\n",
    "# full_train = concatenate_datasets([train_clean_100, train_clean_360])\n",
    "# dataset = load_dataset(\"jkeisling/libritts-r-mimi-kokoro\")\n",
    "full_train = concatenate_datasets([dataset[\"train.clean.100\"], dataset[\"train.clean.360\"]])\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": full_train,\n",
    "    \"val\": dataset[\"dev.clean\"],\n",
    "    \"test\": dataset[\"test.clean\"]\n",
    "})\n",
    "dataset = dataset.with_format(\"torch\")\n",
    "dataset = dataset.remove_columns([\"chapter_id\", \"text_original\"])\n",
    "dataset = dataset.rename_column(original_column_name=\"text_normalized\", new_column_name=\"normalized_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE! This is PATH DEPENDENT on ADDING THE SEMANTIC TOKENS TO THE TOKENIZER EARLIER using `create_bytelevel_init.ipynb`. DO NOT SKIP THIS STEP OR THE MODEL WILL BE IRRETRIEVABLY BROKEN! YOU HAVE BEEN WARNED.**\n",
    "\n",
    "==**THIS IS BYTE LEVEL!**=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../../inits/smoltts_byte\")\n",
    "tokenizer.use_default_system_prompt = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check this carefully: for byte level, it should be 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2368, 256)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer), tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please manually verify the text is done correctly. However, DECODE will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a test sentence.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the tokenizer by encoding and decoding some example text\n",
    "example_text = \"This is a test sentence.\"\n",
    "encoded = tokenizer(example_text, return_tensors=\"pt\")\n",
    "decoded = tokenizer.decode(encoded['input_ids'][0])\n",
    "\n",
    "# Print the results\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[97, 44, 32, 98]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"a, b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[265, 257,  10, 104, 101, 108, 112,  32, 109, 101,  32, 105,  32,  97,\n",
       "         109,  32, 116, 114,  97, 112, 112, 101, 100,  32, 105, 110,  32, 116,\n",
       "         104, 105, 115,  32,  99, 111, 109, 112, 117, 116, 101, 114, 266,  10,\n",
       "         265, 258,  10]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": \"help me i am trapped in this computer\"}], add_generation_prompt=True,  return_tensors=\"pt\")\n",
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nhelp me i am trapped in this computer<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(sequence[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nSpeak out the provided text<|im_end|>\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def encode_text(role: str, content: str, add_generation_prompt: bool = True) -> torch.Tensor:\n",
    "    # baseline = tokenizer.apply_chat_template(f\"{chr(10) if ''}<|im_start|>{role}\\n{content}<|im_end|>\\n\",)\n",
    "    baseline = tokenizer.apply_chat_template(\n",
    "        [{\"role\": role, \"content\": content}],\n",
    "        add_generation_prompt=add_generation_prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    zeros_mask = torch.zeros(8, baseline.size(1), dtype=baseline.dtype)\n",
    "    return torch.cat([baseline, zeros_mask])\n",
    "\n",
    "tts_sysprompt = encode_text(\"system\", \"Speak out the provided text\", add_generation_prompt=False)\n",
    "tokenizer.decode(tts_sysprompt[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this assumes you're using ChatML. if you're NOT, then there's quite a bit more to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|semantic:1049|><|semantic:1268|><|semantic:549|><|semantic:1324|><|semantic:668|><|semantic:1538|><|semantic:1593|><|semantic:95|><|semantic:629|><|semantic:1281|><|semantic:1281|><|semantic:680|><|semantic:536|><|semantic:536|><|semantic:230|><|semantic:1018|><|semantic:1117|><|semantic:244|><|semantic:507|><|semantic:997|><|semantic:1399|><|semantic:640|><|semantic:1591|><|semantic:1967|><|semantic:1161|><|semantic:690|><|semantic:67|><|semantic:1772|><|semantic:830|><|semantic:1612|><|semantic:561|><|semantic:119|><|semantic:1052|><|semantic:880|><|semantic:1029|><|semantic:1532|><|semantic:1161|><|semantic:1344|><|semantic:1109|><|semantic:6|><|semantic:1001|><|semantic:382|><|semantic:596|><|semantic:99|><|semantic:1726|><|semantic:2030|><|semantic:531|><|semantic:616|><|semantic:367|><|semantic:1271|><|semantic:1868|><|semantic:978|><|semantic:729|><|semantic:396|><|semantic:1544|><|im_end|>\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEMANTIC_OFFSET = tokenizer.encode(\"<|semantic:0|>\")[0]\n",
    "# B * C+1 * 2\n",
    "VQ_USER_PREFIX = encode_text(role=\"user\", content=\"\")[:,:-2]\n",
    "TRAILING_IM_END = torch.tensor([\n",
    "    tokenizer.encode(\"<|im_end|>\") + [0] * 8,\n",
    "    tokenizer.encode(\"\\n\") + [0] * 8,\n",
    "]).T\n",
    "\n",
    "def encode_vq(codes: torch.Tensor, is_assistant=True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Expects C * T\n",
    "    \"\"\"\n",
    "    if codes.ndim != 2:\n",
    "        raise ValueError(\"Must be single batch\")\n",
    "    speaker_line = codes[0,:] + SEMANTIC_OFFSET\n",
    "    vq_block = torch.cat([speaker_line.unsqueeze(0), codes])\n",
    "\n",
    "    block = torch.cat([vq_block, TRAILING_IM_END], dim=1)\n",
    "    return block if is_assistant else torch.cat([VQ_USER_PREFIX, block], dim=1)\n",
    "\n",
    "\n",
    "out = encode_vq(dataset[\"test\"][0][\"codes\"], is_assistant=True)\n",
    "tokenizer.decode(out[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user\\nI felt it in my bones when I woke this morning that something splendid was going to turn up.<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1049|><|semantic:1268|><|semantic:549|><|semantic:1324|><|semantic:668|><|semantic:1538|><|semantic:1593|><|semantic:95|><|semantic:629|><|semantic:1281|><|semantic:1281|><|semantic:680|><|semantic:536|><|semantic:536|><|semantic:230|><|semantic:1018|><|semantic:1117|><|semantic:244|><|semantic:507|><|semantic:997|><|semantic:1399|><|semantic:640|><|semantic:1591|><|semantic:1967|><|semantic:1161|><|semantic:690|><|semantic:67|><|semantic:1772|><|semantic:830|><|semantic:1612|><|semantic:561|><|semantic:119|><|semantic:1052|><|semantic:880|><|semantic:1029|><|semantic:1532|><|semantic:1161|><|semantic:1344|><|semantic:1109|><|semantic:6|><|semantic:1001|><|semantic:382|><|semantic:596|><|semantic:99|><|semantic:1726|><|semantic:2030|><|semantic:531|><|semantic:616|><|semantic:367|><|semantic:1271|><|semantic:1868|><|semantic:978|><|semantic:729|><|semantic:396|><|semantic:1544|><|im_end|>\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "# TODO: Not doing ASR for now\n",
    "def tts_tokenize_row(row: Dict):\n",
    "    \"\"\"\n",
    "    NOTE: Deliberately ignores sysprompt line for now, can be done in packing\n",
    "    \"\"\"\n",
    "    # system_line = tts_sysprompt\n",
    "    user_line = encode_text(role=\"user\", content=row[\"normalized_text\"], add_generation_prompt=True)\n",
    "    assistant_line = encode_vq(row[\"codes\"])\n",
    "    ground_truth = torch.cat([user_line, assistant_line], dim=1)\n",
    "    # Causal shift\n",
    "    tokens = ground_truth[:,:-1].clone()\n",
    "    labels = ground_truth[:,1:].clone()\n",
    "\n",
    "    # Assuming user line took care of assistant prefix\n",
    "    labels[1:, :user_line.size(1) - 1] = -100\n",
    "    # Mask out newline\n",
    "    labels[1:, -1] = -100\n",
    "\n",
    "    return({\n",
    "        \"tokens\": tokens,\n",
    "        \"labels\": labels\n",
    "    })\n",
    "    \n",
    "\n",
    "\n",
    "example_row = tts_tokenize_row(dataset[\"test\"][0])\n",
    "tokenizer.decode(example_row[\"labels\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'normalized_text': \"It's abominable of you to ask me.\",\n",
       " 'speaker_id': '4446',\n",
       " 'path': '/root/.cache/huggingface/datasets/downloads/extracted/b814e720279e7b81e61bdb205446cdc8f419e2b356ec9aaec0de02275dac3aba/LibriTTS_R/test-clean/4446/2275/4446_2275_000042_000004.wav',\n",
       " 'id': '4446_2275_000042_000004',\n",
       " 'codes': tensor([[ 995,  301, 1237,   86,  680, 1991, 1275,   22,  922, 1137,  841, 1801,\n",
       "          1441, 1495,  704,  376, 1215, 1424,  640,  779,  705, 1448,  486],\n",
       "         [1912, 1366,  139,   80, 1216, 1832, 1988, 1718,  926,  546,  551,   98,\n",
       "          1747,  124,  221,  129, 1837, 1094,  203, 1584,  691,   91,   91],\n",
       "         [ 208,  410,  905, 1360, 1420, 1919, 1982, 1899, 1542,  722, 1890,  550,\n",
       "           293,  108, 1737, 1956, 1223, 1603, 1106, 1538,  946, 2016, 1400],\n",
       "         [2020,  735, 1285,  295,   33,  817, 1683,  857, 1700,  451, 1671,  316,\n",
       "          1845,  138,  207,  919, 1605, 1354,  808, 1568,  316, 1601,  104],\n",
       "         [ 524, 1866,  536,  958, 1957,  228, 1239, 1374, 1900, 1273,  209,  679,\n",
       "           993,  485,  909, 1779,  224, 1749,  346, 1563,  142,  303, 1212],\n",
       "         [ 445, 1400,  789, 1948,  136,  552, 1174, 1570, 1477,  136, 1105,  665,\n",
       "            97,   17,  511, 1959, 1940, 1559, 1048, 1842,  715, 1509, 1832],\n",
       "         [ 240, 1931,  758, 1270,  875, 1642, 1412, 1557,  564,  461,  261, 1124,\n",
       "          1072,    2,  169,   92, 1332,  869, 1340, 1660,  899,  777, 1512],\n",
       "         [ 650, 1816,  952, 1033,  905, 1503,  176, 1894,  165,  499, 1917, 1083,\n",
       "          1135, 2002,   74,  502,  106,  690, 1146,  912,  791, 1477, 1533]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 257,   10,   73,  ..., 1864,  266,   10],\n",
       "        [-100, -100, -100,  ..., 1544,    0, -100],\n",
       "        [-100, -100, -100,  ..., 1700,    0, -100],\n",
       "        ...,\n",
       "        [-100, -100, -100,  ...,  478,    0, -100],\n",
       "        [-100, -100, -100,  ..., 1978,    0, -100],\n",
       "        [-100, -100, -100,  ..., 1665,    0, -100]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_row[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=12): 100%|██████████| 149658/149658 [00:14<00:00, 10525.36 examples/s]\n",
      "Map (num_proc=12): 100%|██████████| 5736/5736 [00:00<00:00, 10554.01 examples/s]\n",
      "Map (num_proc=12): 100%|██████████| 4837/4837 [00:00<00:00, 9332.18 examples/s] \n"
     ]
    }
   ],
   "source": [
    "# DO NOT INCREASE batch size\n",
    "dataset = dataset.map(tts_tokenize_row, remove_columns=\"codes\", num_proc=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (8/8 shards): 100%|██████████| 149658/149658 [00:02<00:00, 69612.60 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5736/5736 [00:00<00:00, 80247.52 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 4837/4837 [00:00<00:00, 74725.32 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(\"tokenized_libritts_bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWLINE_SEPARATOR = torch.tensor(tokenizer.encode(\"\\n\") + [0] * 8).unsqueeze(1)\n",
    "\n",
    "def batch_pack_sequences(examples, window_size=768, max_items=5):\n",
    "   \"\"\"\n",
    "   Pack sequences with system prompt and metrics\n",
    "   \"\"\"\n",
    "   packed_tokens = []\n",
    "   packed_labels = []\n",
    "   packed_speakers = []\n",
    "   pack_lengths = []\n",
    "   items_per_pack = []\n",
    "   \n",
    "   tokens = examples['tokens']\n",
    "   labels = examples['labels']\n",
    "   speakers = examples['speaker_id']\n",
    "   \n",
    "   # Account for system prompt in window size\n",
    "   effective_window = window_size - tts_sysprompt.shape[1]\n",
    "   \n",
    "   for i in range(len(tokens)):\n",
    "       seq_len = tokens[i].shape[1]\n",
    "       \n",
    "       # Start new pack\n",
    "       if i == 0 or current_length + seq_len > effective_window or \\\n",
    "          current_speaker != speakers[i] or current_items >= max_items:\n",
    "           \n",
    "           # Save previous pack if it exists\n",
    "           if i > 0 and current_tokens:\n",
    "               packed_tokens.append(torch.cat(current_tokens, dim=1))\n",
    "               packed_labels.append(torch.cat(current_labels, dim=1))\n",
    "               packed_speakers.append(current_speaker)\n",
    "               pack_lengths.append(current_length + tts_sysprompt.shape[1])\n",
    "               items_per_pack.append(current_items)\n",
    "           \n",
    "           # Initialize new pack with system prompt\n",
    "           current_tokens = [tts_sysprompt, tokens[i]]\n",
    "           current_labels = [tts_sysprompt, labels[i]]\n",
    "           current_speaker = speakers[i]\n",
    "           current_length = seq_len\n",
    "           current_items = 1\n",
    "           continue\n",
    "           \n",
    "       # Add to current pack with separator\n",
    "       current_tokens.extend([NEWLINE_SEPARATOR, tokens[i]])\n",
    "       current_labels.extend([NEWLINE_SEPARATOR, labels[i]])\n",
    "       current_length += seq_len + 1\n",
    "       current_items += 1\n",
    "   \n",
    "   # Don't forget last pack\n",
    "   if current_tokens:\n",
    "       packed_tokens.append(torch.cat(current_tokens, dim=1))\n",
    "       packed_labels.append(torch.cat(current_labels, dim=1))\n",
    "       packed_speakers.append(current_speaker)\n",
    "       pack_lengths.append(current_length + tts_sysprompt.shape[1])\n",
    "       items_per_pack.append(current_items)\n",
    "   \n",
    "   return {\n",
    "       'tokens': packed_tokens,\n",
    "       'labels': packed_labels,\n",
    "       'speaker_id': packed_speakers,\n",
    "       'pack_length': pack_lengths,\n",
    "       'items_in_pack': items_per_pack\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 149658/149658 [00:31<00:00, 4684.03 examples/s]\n",
      "Map: 100%|██████████| 5736/5736 [00:01<00:00, 4746.63 examples/s]\n",
      "Map: 100%|██████████| 4837/4837 [00:01<00:00, 4760.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Usage:\n",
    "packed_dataset = dataset.map(\n",
    "    lambda row: batch_pack_sequences(row, max_items=3),\n",
    "    batched=True,\n",
    "    remove_columns=dataset['val'].column_names,\n",
    "    batch_size=1000  # Adjust based on memory constraints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nSpeak out the provided text<|im_end|>\\n<|im_start|>assistant\\n<|im_start|>user\\nThe weapon must still have been there.<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1049|><|semantic:1114|><|semantic:1609|><|semantic:784|><|semantic:499|><|semantic:260|><|semantic:1011|><|semantic:8|><|semantic:1407|><|semantic:540|><|semantic:1615|><|semantic:561|><|semantic:1945|><|semantic:201|><|semantic:1324|><|semantic:668|><|semantic:376|><|semantic:1849|><|semantic:9|><|semantic:1921|><|semantic:1921|><|semantic:1683|><|semantic:228|><|semantic:897|><|semantic:1677|><|semantic:518|><|im_end|>\\n<|im_start|>user\\nHow quickly he disappeared!\"<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1698|><|semantic:1848|><|semantic:1021|><|semantic:414|><|semantic:972|><|semantic:1252|><|semantic:1545|><|semantic:1363|><|semantic:307|><|semantic:722|><|semantic:1169|><|semantic:170|><|semantic:1701|><|semantic:1967|><|semantic:886|><|semantic:1540|><|semantic:1540|><|semantic:1113|><|semantic:902|><|semantic:1655|><|semantic:2009|><|semantic:56|><|semantic:1640|><|im_end|>\\n<|im_start|>user\\n\"But the blood?<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1049|><|semantic:503|><|semantic:1249|><|semantic:1039|><|semantic:1066|><|semantic:1522|><|semantic:680|><|semantic:1344|><|semantic:145|><|semantic:145|><|semantic:56|><|semantic:857|><|im_end|>\\n<|im_start|>user\\n\"So they tell me.\"<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1049|><|semantic:738|><|semantic:1669|><|semantic:1627|><|semantic:1144|><|semantic:578|><|semantic:847|><|semantic:184|><|semantic:1348|><|semantic:779|><|semantic:1715|><|semantic:658|><|semantic:141|><|semantic:844|><|im_end|>\\n<|im_start|>user\\nWhat do you make of it, Gryce?\"<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1415|><|semantic:167|><|semantic:1865|><|semantic:557|><|semantic:1000|><|semantic:49|><|semantic:1730|><|semantic:512|><|semantic:510|><|semantic:483|><|semantic:1352|><|semantic:1039|><|semantic:579|><|semantic:489|><|semantic:863|><|semantic:1388|><|semantic:874|><|semantic:632|><|semantic:527|><|semantic:1472|><|semantic:1602|><|semantic:1940|><|semantic:489|><|semantic:1331|><|im_end|>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_row = packed_dataset['val'][0]\n",
    "tokenizer.decode(example_row[\"tokens\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (5/5 shards): 100%|██████████| 50735/50735 [00:01<00:00, 33495.02 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1937/1937 [00:00<00:00, 33569.13 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1649/1649 [00:00<00:00, 31396.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "packed_dataset.save_to_disk(\"tokenized_libritts_packed_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system\\nSpeak out the provided text<|im_end|>\\n<|im_start|>user\\nI felt it in my bones when I woke this morning that something splendid was going to turn up.<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1049|><|semantic:1268|><|semantic:549|><|semantic:1324|><|semantic:668|><|semantic:1538|><|semantic:1593|><|semantic:95|><|semantic:629|><|semantic:1281|><|semantic:1281|><|semantic:680|><|semantic:536|><|semantic:536|><|semantic:230|><|semantic:1018|><|semantic:1117|><|semantic:244|><|semantic:507|><|semantic:997|><|semantic:1399|><|semantic:640|><|semantic:1591|><|semantic:1967|><|semantic:1161|><|semantic:690|><|semantic:67|><|semantic:1772|><|semantic:830|><|semantic:1612|><|semantic:561|><|semantic:119|><|semantic:1052|><|semantic:880|><|semantic:1029|><|semantic:1532|><|semantic:1161|><|semantic:1344|><|semantic:1109|><|semantic:6|><|semantic:1001|><|semantic:382|><|semantic:596|><|semantic:99|><|semantic:1726|><|semantic:2030|><|semantic:531|><|semantic:616|><|semantic:367|><|semantic:1271|><|semantic:1868|><|semantic:978|><|semantic:729|><|semantic:396|><|semantic:1544|><|im_end|>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example_row[0][\"labels\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system\\nTranscribe the provided speech<|im_end|>\\n<|im_start|>user\\n<|semantic:1049|><|semantic:1268|><|semantic:549|><|semantic:1324|><|semantic:668|><|semantic:1538|><|semantic:1593|><|semantic:95|><|semantic:629|><|semantic:1281|><|semantic:1281|><|semantic:680|><|semantic:536|><|semantic:536|><|semantic:230|><|semantic:1018|><|semantic:1117|><|semantic:244|><|semantic:507|><|semantic:997|><|semantic:1399|><|semantic:640|><|semantic:1591|><|semantic:1967|><|semantic:1161|><|semantic:690|><|semantic:67|><|semantic:1772|><|semantic:830|><|semantic:1612|><|semantic:561|><|semantic:119|><|semantic:1052|><|semantic:880|><|semantic:1029|><|semantic:1532|><|semantic:1161|><|semantic:1344|><|semantic:1109|><|semantic:6|><|semantic:1001|><|semantic:382|><|semantic:596|><|semantic:99|><|semantic:1726|><|semantic:2030|><|semantic:531|><|semantic:616|><|semantic:367|><|semantic:1271|><|semantic:1868|><|semantic:978|><|semantic:729|><|semantic:396|><|semantic:1544|><|im_end|>\\n<|im_start|>assistant\\nI felt it in my bones when I woke this morning that something splendid was going to turn up.<|im_end|>\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example_row[1][\"labels\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0, 1698, 1719,  204, 1389,  851, 1772,  186, 1307, 1895,  832,\n",
       "        1633,  771,  648, 1530, 1989, 1574, 1348,  722,  144, 1945,  278, 1109,\n",
       "          29,  611,   46,  622,  628, 1740,  572,  572,  345, 1989, 1676,  929,\n",
       "        1776,  749,  313, 1997, 1571,  819, 1238, 1054, 1054, 1135, 1506, 1393,\n",
       "         616, 1702,  993,  579,  486,  486, 2039,  148,  657,  664,  339,  339,\n",
       "         588,  212, 1443,   32, 1320, 1549,  440,    8, 1407, 1722, 1650, 1615,\n",
       "         798,  121,  303,  697,  837,  358, 1882,  440, 1992, 1992,  587,  178,\n",
       "         178, 1627, 1530,  929, 1610, 1916,  523,  213, 1252, 1480, 1468, 1899,\n",
       "         773, 2033, 2033,   83, 1146,  784, 1295,  199, 1109,  268,    6,    6,\n",
       "        1781, 1479, 1530, 1530,  146, 2038,  984, 1403,  606, 1379, 1840, 1172,\n",
       "        1680, 1162, 1928])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_row[\"tokens\"][0][1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, 1049, 1268,  549, 1324,  668, 1538, 1593,   95,  629, 1281, 1281,\n",
       "         680,  536,  536,  230, 1018, 1117,  244,  507,  997, 1399,  640, 1591,\n",
       "        1967, 1161,  690,   67, 1772,  830, 1612,  561,  119, 1052,  880, 1029,\n",
       "        1532, 1161, 1344, 1109,    6, 1001,  382,  596,   99, 1726, 2030,  531,\n",
       "         616,  367, 1271, 1868,  978,  729,  396, 1544,    0, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_row[1][\"labels\"][1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'normalized_text': 'I felt it in my bones when I woke this morning that something splendid was going to turn up.',\n",
       " 'speaker_id': '4446',\n",
       " 'id': '4446_2275_000002_000009',\n",
       " 'tokens': tensor([[    1,  9690,   198, 15024,   494,   578,   260,  2711,  1694,     2,\n",
       "            198,     1,  4093,   198,    57,  4592,   357,   281,   957,  6542,\n",
       "            645,   339, 40652,   451,  5738,   338,  1488, 33494,   436,  2045,\n",
       "            288,  1607,   614,    30,     2,   198,     1,   520,  9531,   198,\n",
       "          50201, 50420, 49701, 50476, 49820, 50690, 50745, 49247, 49781, 50433,\n",
       "          50433, 49832, 49688, 49688, 49382, 50170, 50269, 49396, 49659, 50149,\n",
       "          50551, 49792, 50743, 51119, 50313, 49842, 49219, 50924, 49982, 50764,\n",
       "          49713, 49271, 50204, 50032, 50181, 50684, 50313, 50496, 50261, 49158,\n",
       "          50153, 49534, 49748, 49251, 50878, 51182, 49683, 49768, 49519, 50423,\n",
       "          51020, 50130, 49881, 49548, 50696],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1049,  1268,   549,  1324,   668,  1538,  1593,    95,   629,  1281,\n",
       "           1281,   680,   536,   536,   230,  1018,  1117,   244,   507,   997,\n",
       "           1399,   640,  1591,  1967,  1161,   690,    67,  1772,   830,  1612,\n",
       "            561,   119,  1052,   880,  1029,  1532,  1161,  1344,  1109,     6,\n",
       "           1001,   382,   596,    99,  1726,  2030,   531,   616,   367,  1271,\n",
       "           1868,   978,   729,   396,  1544],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1470,  1879,   712,   283,   220,   137,  1610,   263,   531,  1845,\n",
       "           1428,  1132,   359,  1904,  1458,  1876,   895,   149,   190,   116,\n",
       "            603,   786,  1884,  1455,  1928,   677,   914,  1122,   436,   618,\n",
       "            850,  1766,  2005,  1618,   966,   850,  1663,   172,   274,   612,\n",
       "           1013,  1928,  1262,  1169,  1006,  1777,  1755,  2026,  1714,   788,\n",
       "            786,  1520,   811,    91,  1700],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            373,   602,  2016,  1148,    98,   790,  1570,   944,  1872,   807,\n",
       "           1427,   817,   602,  1090,   341,  1609,   774,   327,   479,   229,\n",
       "           1988,  1596,  1595,   339,  1801,  1360,  1415,   492,     3,  1064,\n",
       "            378,  1530,   800,  1369,  1331,   782,  1822,   511,   349,  1534,\n",
       "           1245,  1788,   146,  1195,  1510,   584,  1782,   662,  1523,  1523,\n",
       "             68,  2016,  1240,   783,  1178],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1225,  1475,    17,   148,   656,  2036,   663,   632,  1926,   358,\n",
       "           1443,    33,  1316,  1993,  1038,   324,  1240,   861,   787,  1994,\n",
       "           1721,   832,   707,   435,   113,   138,   494,   920,  1707,  1134,\n",
       "           1003,   842,  1511,   193,  1936,  1478,  1261,   332,   756,   115,\n",
       "           1700,   324,  1074,  1631,  1575,   526,  1957,   756,   318,    19,\n",
       "            860,  1562,   164,  1477,  1450],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            457,  2035,  1113,   504,  1301,     5,   739,   334,   178,  1832,\n",
       "           1803,   266,  1719,   516,   147,  1016,  1402,   709,  1046,  1620,\n",
       "            440,   996,   100,   551,  1799,  1930,  1811,  1551,  1026,  1440,\n",
       "            684,   313,    13,   629,  1930,   723,   457,   328,  1455,   796,\n",
       "            300,  1610,   435,   635,  1517,   958,  1607,  1013,   911,   958,\n",
       "            524,   457,  1019,  1019,   481],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1547,   592,   601,  2022,  1795,    42,  1631,  1174,   496,  1640,\n",
       "            935,  1687,   654,  1105,   668,  1995,   918,  1002,  1668,   190,\n",
       "           1148,  1509,  1563,    30,  1669,   882,  1529,  1794,  1086,   900,\n",
       "            274,  1001,   482,    42,  1852,   780,   704,   275,  1073,  1174,\n",
       "           1618,  1540,  1344,  1371,   263,  1245,  1382,   253,   484,  2044,\n",
       "           1717,  1942,   165,  1832,   478],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1433,  1154,  1672,   879,  1683,  1529,   561,   508,  1648,   358,\n",
       "            646,  1289,  1307,  1867,  1937,  1076,  1694,  1730,  1199,  1758,\n",
       "           1476,   369,   801,  1249,   260,  1759,   740,    53,   618,  2023,\n",
       "            119,   835,  2015,   835,    66,  1209,  1285,  1744,   804,  1863,\n",
       "           1240,   392,   741,  1834,  1193,  1440,  1696,  1470,  1810,  1464,\n",
       "            840,    43,  1978,   666,  1978],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            948,   232,  1109,  1286,  1371,    94,  1516,   700,   714,   537,\n",
       "           1939,   334,    17,  1093,  1769,  1511,  1063,   444,  1399,  1594,\n",
       "             10,   532,   208,  1803,   945,  1193,  1082,  1667,   198,   382,\n",
       "            101,  1426,   458,  1587,  1450,  1722,  1173,  1286,   127,   225,\n",
       "           1482,  1368,   370,  1003,  1808,  1772,   952,   992,  1977,   820,\n",
       "            481,   945,   945,  1477,  1665]]),\n",
       " 'labels': tensor([[ 9690,   198, 15024,   494,   578,   260,  2711,  1694,     2,   198,\n",
       "              1,  4093,   198,    57,  4592,   357,   281,   957,  6542,   645,\n",
       "            339, 40652,   451,  5738,   338,  1488, 33494,   436,  2045,   288,\n",
       "           1607,   614,    30,     2,   198,     1,   520,  9531,   198, 50201,\n",
       "          50420, 49701, 50476, 49820, 50690, 50745, 49247, 49781, 50433, 50433,\n",
       "          49832, 49688, 49688, 49382, 50170, 50269, 49396, 49659, 50149, 50551,\n",
       "          49792, 50743, 51119, 50313, 49842, 49219, 50924, 49982, 50764, 49713,\n",
       "          49271, 50204, 50032, 50181, 50684, 50313, 50496, 50261, 49158, 50153,\n",
       "          49534, 49748, 49251, 50878, 51182, 49683, 49768, 49519, 50423, 51020,\n",
       "          50130, 49881, 49548, 50696,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1049,\n",
       "           1268,   549,  1324,   668,  1538,  1593,    95,   629,  1281,  1281,\n",
       "            680,   536,   536,   230,  1018,  1117,   244,   507,   997,  1399,\n",
       "            640,  1591,  1967,  1161,   690,    67,  1772,   830,  1612,   561,\n",
       "            119,  1052,   880,  1029,  1532,  1161,  1344,  1109,     6,  1001,\n",
       "            382,   596,    99,  1726,  2030,   531,   616,   367,  1271,  1868,\n",
       "            978,   729,   396,  1544,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1470,\n",
       "           1879,   712,   283,   220,   137,  1610,   263,   531,  1845,  1428,\n",
       "           1132,   359,  1904,  1458,  1876,   895,   149,   190,   116,   603,\n",
       "            786,  1884,  1455,  1928,   677,   914,  1122,   436,   618,   850,\n",
       "           1766,  2005,  1618,   966,   850,  1663,   172,   274,   612,  1013,\n",
       "           1928,  1262,  1169,  1006,  1777,  1755,  2026,  1714,   788,   786,\n",
       "           1520,   811,    91,  1700,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   373,\n",
       "            602,  2016,  1148,    98,   790,  1570,   944,  1872,   807,  1427,\n",
       "            817,   602,  1090,   341,  1609,   774,   327,   479,   229,  1988,\n",
       "           1596,  1595,   339,  1801,  1360,  1415,   492,     3,  1064,   378,\n",
       "           1530,   800,  1369,  1331,   782,  1822,   511,   349,  1534,  1245,\n",
       "           1788,   146,  1195,  1510,   584,  1782,   662,  1523,  1523,    68,\n",
       "           2016,  1240,   783,  1178,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1225,\n",
       "           1475,    17,   148,   656,  2036,   663,   632,  1926,   358,  1443,\n",
       "             33,  1316,  1993,  1038,   324,  1240,   861,   787,  1994,  1721,\n",
       "            832,   707,   435,   113,   138,   494,   920,  1707,  1134,  1003,\n",
       "            842,  1511,   193,  1936,  1478,  1261,   332,   756,   115,  1700,\n",
       "            324,  1074,  1631,  1575,   526,  1957,   756,   318,    19,   860,\n",
       "           1562,   164,  1477,  1450,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   457,\n",
       "           2035,  1113,   504,  1301,     5,   739,   334,   178,  1832,  1803,\n",
       "            266,  1719,   516,   147,  1016,  1402,   709,  1046,  1620,   440,\n",
       "            996,   100,   551,  1799,  1930,  1811,  1551,  1026,  1440,   684,\n",
       "            313,    13,   629,  1930,   723,   457,   328,  1455,   796,   300,\n",
       "           1610,   435,   635,  1517,   958,  1607,  1013,   911,   958,   524,\n",
       "            457,  1019,  1019,   481,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1547,\n",
       "            592,   601,  2022,  1795,    42,  1631,  1174,   496,  1640,   935,\n",
       "           1687,   654,  1105,   668,  1995,   918,  1002,  1668,   190,  1148,\n",
       "           1509,  1563,    30,  1669,   882,  1529,  1794,  1086,   900,   274,\n",
       "           1001,   482,    42,  1852,   780,   704,   275,  1073,  1174,  1618,\n",
       "           1540,  1344,  1371,   263,  1245,  1382,   253,   484,  2044,  1717,\n",
       "           1942,   165,  1832,   478,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1433,\n",
       "           1154,  1672,   879,  1683,  1529,   561,   508,  1648,   358,   646,\n",
       "           1289,  1307,  1867,  1937,  1076,  1694,  1730,  1199,  1758,  1476,\n",
       "            369,   801,  1249,   260,  1759,   740,    53,   618,  2023,   119,\n",
       "            835,  2015,   835,    66,  1209,  1285,  1744,   804,  1863,  1240,\n",
       "            392,   741,  1834,  1193,  1440,  1696,  1470,  1810,  1464,   840,\n",
       "             43,  1978,   666,  1978,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   948,\n",
       "            232,  1109,  1286,  1371,    94,  1516,   700,   714,   537,  1939,\n",
       "            334,    17,  1093,  1769,  1511,  1063,   444,  1399,  1594,    10,\n",
       "            532,   208,  1803,   945,  1193,  1082,  1667,   198,   382,   101,\n",
       "           1426,   458,  1587,  1450,  1722,  1173,  1286,   127,   225,  1482,\n",
       "           1368,   370,  1003,  1808,  1772,   952,   992,  1977,   820,   481,\n",
       "            945,   945,  1477,  1665,     0]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
