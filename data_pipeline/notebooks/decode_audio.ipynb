{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ritsuko/projects/ai/audio/dual-ar/.venv/lib/python3.9/site-packages/transformers/models/mimi/modeling_mimi.py:164: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.register_buffer(\"padding_total\", torch.tensor(kernel_size - stride, dtype=torch.int64), persistent=False)\n"
     ]
    }
   ],
   "source": [
    "from transformers import MimiModel, AutoFeatureExtractor\n",
    "\n",
    "device = \"cpu\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"kyutai/mimi\")\n",
    "model = MimiModel.from_pretrained(\"kyutai/mimi\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "def load_and_process_wav(file_path):\n",
    "    \"\"\"\n",
    "    Load a WAV file, convert it to mono, resample it to 24kHz, and return as a tensor.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the WAV file.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Processed audio tensor.\n",
    "    \"\"\"\n",
    "    # Load the audio file\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "\n",
    "    # Convert to mono if not already\n",
    "    if waveform.size(0) > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "\n",
    "    # Resample to 24kHz if needed\n",
    "    target_sample_rate = 24000\n",
    "    if sample_rate != target_sample_rate:\n",
    "        resampler = T.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def run_llama_generate(\n",
    "    text=\"Can you generate five simple sentences for my child to practice speaking\",\n",
    "    temp=0.1,\n",
    "    checkpoint_path=\"../dual-ar/checkpoints/smoltts_scratch/\",\n",
    "    working_dir=\"../../fish-speech.rs\"  # Replace with your desired working directory\n",
    "):\n",
    "    # Store current working directory\n",
    "    original_dir = os.getcwd()\n",
    "    \n",
    "    try:\n",
    "        # Change to desired working directory\n",
    "        os.chdir(working_dir)\n",
    "        \n",
    "        # Construct the command\n",
    "        cmd = f'cargo run --release --features cuda --bin llama_generate -- '\\\n",
    "              f'--text \"{text}\" '\\\n",
    "              f'--checkpoint {checkpoint_path} '\\\n",
    "              f'--temp {temp}'\n",
    "        \n",
    "        # Execute command\n",
    "        return os.system(cmd)\n",
    "        \n",
    "    finally:\n",
    "        # Always return to original directory\n",
    "        os.chdir(original_dir)\n",
    "\n",
    "# Example usage:\n",
    "# run_llama_generate(\n",
    "#     text=\"Write a short story about a cat\",\n",
    "#     temp=0.2,\n",
    "#     working_dir=\"/path/to/your/project\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 8, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_input\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Generate audio\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m out_pcm \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Convert to CPU and get numpy array for playback\u001b[39;00m\n\u001b[1;32m     18\u001b[0m audio_data \u001b[38;5;241m=\u001b[39m out_pcm\u001b[38;5;241m.\u001b[39maudio_values[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/projects/ai/audio/dual-ar/.venv/lib/python3.9/site-packages/transformers/models/mimi/modeling_mimi.py:1712\u001b[0m, in \u001b[0;36mMimiModel.decode\u001b[0;34m(self, audio_codes, padding_mask, decoder_past_key_values, return_dict)\u001b[0m\n\u001b[1;32m   1686\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;124;03mDecodes the given frames into an output audio waveform.\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1708\u001b[0m \n\u001b[1;32m   1709\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mreturn_dict\n\u001b[0;32m-> 1712\u001b[0m audio_values, decoder_past_key_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode_frame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43maudio_codes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_past_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;66;03m# truncate based on padding mask\u001b[39;00m\n\u001b[1;32m   1717\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m padding_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m<\u001b[39m audio_values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m~/projects/ai/audio/dual-ar/.venv/lib/python3.9/site-packages/transformers/models/mimi/modeling_mimi.py:1665\u001b[0m, in \u001b[0;36mMimiModel._decode_frame\u001b[0;34m(self, codes, past_key_values, return_dict)\u001b[0m\n\u001b[1;32m   1659\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_decode_frame\u001b[39m(\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1661\u001b[0m     codes: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   1662\u001b[0m     past_key_values: Optional[Union[Cache, List[torch\u001b[38;5;241m.\u001b[39mFloatTensor]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1663\u001b[0m     return_dict: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1664\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m-> 1665\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1667\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupsample(embeddings)\n\u001b[1;32m   1668\u001b[0m     decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_transformer(\n\u001b[1;32m   1669\u001b[0m         embeddings\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values, return_dict\u001b[38;5;241m=\u001b[39mreturn_dict\n\u001b[1;32m   1670\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/ai/audio/dual-ar/.venv/lib/python3.9/site-packages/transformers/models/mimi/modeling_mimi.py:1413\u001b[0m, in \u001b[0;36mMimiSplitResidualVectorQuantizer.decode\u001b[0;34m(self, codes)\u001b[0m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode the given codes to the quantized representation.\"\"\"\u001b[39;00m\n\u001b[1;32m   1412\u001b[0m \u001b[38;5;66;03m# The first num_semantic_quantizers codebooks are decoded using the semantic RVQ\u001b[39;00m\n\u001b[0;32m-> 1413\u001b[0m quantized_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msemantic_residual_vector_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodes\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_semantic_quantizers\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1415\u001b[0m \u001b[38;5;66;03m# The rest of the codebooks are decoded using the acoustic RVQ\u001b[39;00m\n\u001b[1;32m   1416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m codes\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_semantic_quantizers:\n",
      "File \u001b[0;32m~/projects/ai/audio/dual-ar/.venv/lib/python3.9/site-packages/transformers/models/mimi/modeling_mimi.py:1357\u001b[0m, in \u001b[0;36mMimiResidualVectorQuantizer.decode\u001b[0;34m(self, codes)\u001b[0m\n\u001b[1;32m   1355\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, indices \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(codes):\n\u001b[1;32m   1356\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\n\u001b[0;32m-> 1357\u001b[0m     quantized \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m     quantized_out \u001b[38;5;241m=\u001b[39m quantized_out \u001b[38;5;241m+\u001b[39m quantized\n\u001b[1;32m   1360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_proj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/projects/ai/audio/dual-ar/.venv/lib/python3.9/site-packages/transformers/models/mimi/modeling_mimi.py:1307\u001b[0m, in \u001b[0;36mMimiVectorQuantization.decode\u001b[0;34m(self, embed_ind)\u001b[0m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed_ind):\n\u001b[1;32m   1306\u001b[0m     quantize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodebook\u001b[38;5;241m.\u001b[39mdecode(embed_ind)\n\u001b[0;32m-> 1307\u001b[0m     quantize \u001b[38;5;241m=\u001b[39m \u001b[43mquantize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1308\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m quantize\n",
      "\u001b[0;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "# run_llama_generate(\n",
    "#     text=\"Here's how Bob talks, here's what language is, now speak like Bob saying this new thing\",\n",
    "#     temp=0.05\n",
    "# )\n",
    "# Load and process the data\n",
    "test_arr = np.load(\"../../out.npy\")\n",
    "test_input = torch.from_numpy(test_arr[:,:200]).to(device).to(torch.long)\n",
    "print(test_input.shape)\n",
    "\n",
    "# Generate audio\n",
    "out_pcm = model.decode(test_input)\n",
    "\n",
    "# Convert to CPU and get numpy array for playback\n",
    "audio_data = out_pcm.audio_values[0].detach().to(\"cpu\").numpy()\n",
    "\n",
    "# Create and display audio widget\n",
    "# Note: sample_rate=24000 matches your original save command\n",
    "display(Audio(audio_data, rate=24000, autoplay=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1049, 1904, 1839, 1460, 1194, 1480, 1985,  740,  112, 1228, 1803,  813,\n",
       "         235,  551, 1043,  610, 1148,  386,   69, 1702, 1677,  502,  769,   84,\n",
       "          84,   84,  752,  752, 1934,  814, 1614,  282], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input[0, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pcm = load_and_process_wav(\"../../fish-speech.rs/voices/nova.wav\")\n",
    "codes = model.encode(pcm.to(\"cuda\").unsqueeze(0))\n",
    "np.save(\"nova.npy\", codes[\"audio_codes\"].squeeze(0)[:8, :].cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 38])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes[\"audio_codes\"].squeeze(0)[:8,:].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
