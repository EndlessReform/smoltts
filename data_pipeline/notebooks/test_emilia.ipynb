{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_pipeline'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdata_pipeline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MimiCodec\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_pipeline'"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from data_pipeline.utils.codec import MimiCodec\n",
    "from IPython.display import Audio\n",
    "import numpy as np\n",
    "\n",
    "codec = MimiCodec()\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking encoded audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, DatasetDict\n",
    "\n",
    "root_dir = os.path.expanduser(\"~/local_datasets/emilia_chunks\")\n",
    "shard_dirs = [\n",
    "    os.path.join(root_dir, name)\n",
    "    for name in os.listdir(root_dir)\n",
    "    if os.path.isdir(os.path.join(root_dir, name))\n",
    "]\n",
    "\n",
    "# Optionally sort the shard directories (if they have sortable names)\n",
    "shard_dirs.sort()\n",
    "shard_datasets = {}\n",
    "for shard_dir in shard_dirs:\n",
    "    # You can name each split using the directory name or a custom name\n",
    "    split_name = os.path.basename(shard_dir)\n",
    "    shard_datasets[split_name] = load_from_disk(shard_dir)\n",
    "\n",
    "# Combine them into a DatasetDict if that suits your workflow:\n",
    "ds = DatasetDict(shard_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "collapsed_dataset = concatenate_datasets([ds[split] for split in ds.keys()])\n",
    "token = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "len(collapsed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(row):\n",
    "    json = row[\"json\"]\n",
    "    del json[\"wav\"]\n",
    "    return {\n",
    "        'dnsmos': json['dnsmos'],\n",
    "        'duration': json['duration'],\n",
    "        'id': json['id'],\n",
    "        'speaker': json['speaker'],\n",
    "        'text': json['text']\n",
    "    }\n",
    "\n",
    "ds = collapsed_dataset.map(extract_json, num_proc=12, remove_columns=\"json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = ds[\"duration\"]\n",
    "\n",
    "total_hours = durations.sum().item()/3600\n",
    "print(f\"First 300 shards: {total_hours:02f} hours\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.quantile(ds[\"dnsmos\"], torch.tensor([0.2, 0.4, 0.6, 0.8, 1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.push_to_hub(\"jkeisling/emilia_en_mimi\", token=token, max_shard_size=\"2GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pcm = codec.decode(collapsed_dataset[1_000_001][\"codes\"])\n",
    "Audio(np.array(pcm), rate=24_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [f\"Emilia/EN/EN-B00{i:04d}.tar\" for i in range(200,202)]\n",
    "dataset = load_dataset(\n",
    "    \"amphion/Emilia-Dataset\",\n",
    "    data_files=paths,\n",
    "    split=\"train\",\n",
    "    token=os.getenv(\"HUGGINGFACE_TOKEN\"),\n",
    ")\n",
    "print(dataset)  # here should only shows 90 n_shards\n",
    "dataset = dataset.with_format('pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[21_000][\"mp3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcm = dataset[28_000][\"mp3\"][\"array\"]\n",
    "Audio(np.array(pcm), rate=24_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice = dataset[160:180]\n",
    "\n",
    "codes = codec.encode_batch([s[\"array\"] for s in slice[\"mp3\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcm = codec.decode(codes[18])\n",
    "Audio(np.array(pcm), rate=24_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import Wav2Vec2FeatureExtractor, WavLMForXVector\n",
    "# please do not run this on a mac, i'm warning you\n",
    "device = \"cuda\"\n",
    "\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained('microsoft/wavlm-base-plus-sv')\n",
    "model = WavLMForXVector.from_pretrained('microsoft/wavlm-base-plus-sv')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start timing dataset fetching\n",
    "t4 = time.perf_counter()\n",
    "sample = dataset[:5]\n",
    "t5 = time.perf_counter()\n",
    "print(f\"Dataset fetch time: {t5 - t4:.3f}s\")\n",
    "sample['mp3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audio = [s['array'] for s in sample[\"mp3\"]]\n",
    "\n",
    "# TODO resampling, fix it immediately you f***ing fool\n",
    "\n",
    "inputs = feature_extractor(audio, padding=\"max_length\", return_tensors=\"pt\", device=device)\n",
    "\n",
    "# Start timing model inference\n",
    "t8 = time.perf_counter()\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "embeddings = model(**inputs).embeddings\n",
    "t9 = time.perf_counter()\n",
    "print(f\"Model inference time: {t9 - t8:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the path and expand ~\n",
    "dataset_dir = os.path.expanduser(\"~/.cache/huggingface/datasets/amphion___emilia-dataset\")\n",
    "\n",
    "# Say goodbye\n",
    "try:\n",
    "    shutil.rmtree(dataset_dir, ignore_errors=True)  # IGNORE ERRORS: NO MERCY\n",
    "    print(f\"ðŸ’¥ Nuked: {dataset_dir}\")\n",
    "except Exception as e:  # Just in case something dares to resist\n",
    "    print(f\"ðŸ”¥ Failed to nuke {dataset_dir}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(dataset))\n",
    "t1 = time.perf_counter()\n",
    "inputs = codec.encode(sample[\"mp3\"][\"array\"].unsqueeze(0))\n",
    "t2 = time.perf_counter()\n",
    "print(f\"Model inference time: {t2 - t1:.3f}s\")\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from torchaudio.transforms import Resample\n",
    "\n",
    "downsample_16k = Resample(orig_freq=24_000)\n",
    "# intentionally naÃ¯ve, adding \n",
    "def encode_row(row: Dict):\n",
    "    audio = row[\"mp3\"][\"array\"]\n",
    "    downsampled = downsample_16k(audio)\n",
    "    inputs = feature_extractor(downsampled, padding=True, return_tensors=\"pt\", sampling_rate=24_000)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    embeddings = model(**inputs).embeddings\n",
    "    embeddings = embeddings.cpu()\n",
    "\n",
    "    encoded = codec.encode(audio.unsqueeze(0))\n",
    "    return({\n",
    "        \"codes\": encoded,\n",
    "        \"speaker_emb\": embeddings\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = dataset.take(1_000)\n",
    "test_ds.map(encode_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "ds = load_from_disk(\"../../datasets/byte-tokenized-emilia-v1\")[\"train\"].shard(16, 0)\n",
    "ds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../../inits/smoltts_byte_kokoro_layer\")\n",
    "tokenizer.decode(ds[20][\"ground_truth\"][0,:])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
