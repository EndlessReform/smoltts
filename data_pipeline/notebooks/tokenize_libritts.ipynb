{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize LibriTTS-R Mimi for target LM\n",
    "\n",
    "For our dataset, we currently simply use the Fish Speech TTS format:\n",
    "- Text-only data formatted using [ChatML](https://gist.github.com/edwardzjl/8df07c1f7140c9a3e2f48d33a8032090) as a separate sequence \"above\" the audio code stream\n",
    "- During sections where audio is being modeled, text stream 0 predicts the first semantic token index $n$ of the 8 Mimi residual codes as special token `<|semantic:n|>`\n",
    "- For audio, \"semantic\" (neural, there's not a strong distinction between) codes (from Mimi) padded with 0s during text sections\n",
    "\n",
    "It's possible this tokenization strategy can be improved, e.g. in [Defossez et al. 2024](https://arxiv.org/html/2410.00037v2#S3.SS4.SSS4) with the base transformer predicting the Whisper-timestamped word timings as an \"inner monologue\" and a delay between codebook timesteps. lol i'll do it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "\n",
    "# If creating the libritts dataset for the first time\n",
    "# from datasets import load_from_disk \n",
    "# dataset = load_from_disk(\"encoded_dataset\")\n",
    "# train_clean_100 = load_from_disk(\"encoded_libritts/train.clean.100/\")\n",
    "# train_clean_360 = load_from_disk(\"encoded_libritts/train.clean.360/\")\n",
    "# dev_clean = load_from_disk(\"encoded_libritts/dev.clean\")\n",
    "# test_clean = load_from_disk(\"encoded_libritts/test.clean\")\n",
    "# full_train = concatenate_datasets([train_clean_100, train_clean_360])\n",
    "dataset = load_dataset(\"jkeisling/libritts-r-mimi\")\n",
    "full_train = concatenate_datasets([dataset[\"train.clean.100\"], dataset[\"train.clean.360\"]])\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": full_train,\n",
    "    \"val\": dataset[\"dev.clean\"],\n",
    "    \"test\": dataset[\"test.clean\"]\n",
    "})\n",
    "dataset = dataset.with_format(\"torch\")\n",
    "dataset = dataset.remove_columns([\"path\", \"chapter_id\", \"text_original\"])\n",
    "dataset = dataset.rename_column(original_column_name=\"text_normalized\", new_column_name=\"normalized_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE! This is PATH DEPENDENT on ADDING THE SEMANTIC TOKENS TO THE TOKENIZER EARLIER using `create_smoltts_init.ipynb`. DO NOT SKIP THIS STEP OR THE MODEL WILL BE IRRETRIEVABLY BROKEN! YOU HAVE BEEN WARNED.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../inits/smoltts_init\")\n",
    "tokenizer.use_default_system_prompt = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check this carefully: for SmolTTS, it should be 51200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer), tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please manually verify the text is done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tokenizer by encoding and decoding some example text\n",
    "example_text = \"This is a test sentence.\"\n",
    "encoded = tokenizer(example_text, return_tensors=\"pt\")\n",
    "decoded = tokenizer.decode(encoded['input_ids'][0])\n",
    "\n",
    "# Print the results\n",
    "dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": \"help me i am trapped in this computer\"}], add_generation_prompt=True,  return_tensors=\"pt\")\n",
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(sequence[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def encode_text(role: str, content: str, add_generation_prompt: bool = True) -> torch.Tensor:\n",
    "    # baseline = tokenizer.apply_chat_template(f\"{chr(10) if ''}<|im_start|>{role}\\n{content}<|im_end|>\\n\",)\n",
    "    baseline = tokenizer.apply_chat_template(\n",
    "        [{\"role\": role, \"content\": content}],\n",
    "        add_generation_prompt=add_generation_prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    zeros_mask = torch.zeros(8, baseline.size(1), dtype=baseline.dtype)\n",
    "    return torch.cat([baseline, zeros_mask])\n",
    "\n",
    "tts_sysprompt = encode_text(\"system\", \"Speak out the provided text\")\n",
    "asr_sysprompt = encode_text(\"system\", \"Transcribe the provided speech\", False)\n",
    "tokenizer.decode(asr_sysprompt[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this assumes you're using ChatML. if you're NOT, then there's quite a bit more to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEMANTIC_OFFSET = tokenizer.encode(\"<|semantic:0|>\")[0]\n",
    "# B * C+1 * 2\n",
    "VQ_USER_PREFIX = encode_text(role=\"user\", content=\"\")[:,:-2]\n",
    "TRAILING_IM_END = torch.tensor([\n",
    "    tokenizer.encode(\"<|im_end|>\") + [0] * 8,\n",
    "    tokenizer.encode(\"\\n\") + [0] * 8,\n",
    "]).T\n",
    "\n",
    "def encode_vq(codes: torch.Tensor, is_assistant=True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Expects C * T\n",
    "    \"\"\"\n",
    "    if codes.ndim != 2:\n",
    "        raise ValueError(\"Must be single batch\")\n",
    "    speaker_line = codes[0,:] + SEMANTIC_OFFSET\n",
    "    vq_block = torch.cat([speaker_line.unsqueeze(0), codes])\n",
    "\n",
    "    block = torch.cat([vq_block, TRAILING_IM_END], dim=1)\n",
    "    return block if is_assistant else torch.cat([VQ_USER_PREFIX, block], dim=1)\n",
    "\n",
    "\n",
    "out = encode_vq(dataset[\"test\"][0][\"codes\"], is_assistant=True)\n",
    "tokenizer.decode(out[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "# ASSISTANT_PREFIX_LEN = len(tokenizer.tokenize(\"<|im_start|>assistant\\n\"))\n",
    "# USER_PREFIX_LEN  = len(tokenizer.tokenize(\"<|im_start|>user\\n\"))\n",
    "\n",
    "# def tokenize_row(row: Dict, is_batch=True):\n",
    "#     \"\"\"\n",
    "#     row[\"normalized_text\"] is a string\n",
    "#     row[\"codes\"] is a torch.Tensor shaped [9, T_vq]\n",
    "#     \"\"\"\n",
    "#     row = {\n",
    "#         \"normalized_text\": row[\"normalized_text\"][0],\n",
    "#         \"codes\": row[\"codes\"][0],\n",
    "#         \"speaker_id\": row[\"speaker_id\"],\n",
    "#         \"id\": row[\"id\"]\n",
    "#     } if is_batch else row\n",
    "#     tts_user_line = encode_text(role=\"user\", content=row[\"normalized_text\"])\n",
    "#     asr_assistant_line = encode_text(role=\"assistant\", content=row[\"normalized_text\"], needs_initial_newline=True)\n",
    "#     tts_assistant_codes = encode_vq(row[\"codes\"])  # shape [9, T_vq]\n",
    "#     asr_user_codes = encode_vq(row[\"codes\"], is_assistant=False)  # shape [9, T_vq]\n",
    "    \n",
    "#     # Concatenate system prompt (row=1?), user line (row=1?), codebooks (row=9),\n",
    "#     # but along the *time* dimension => final shape [9, T_total] \n",
    "#     #   (since sysprompt and user_line are [1, T_something], \n",
    "#     #    codes_9rows is [9, T_vq], so we pad them to 9 rows if needed)\n",
    "#     # For demonstration, I'm just stacking them. You probably do:\n",
    "#     tts_ground_truth = torch.cat([tts_sysprompt, tts_user_line, tts_assistant_codes], dim=1)\n",
    "#     asr_ground_truth = torch.cat([asr_sysprompt, asr_user_codes, asr_assistant_line], dim=1)\n",
    "#     tts_tokens = tts_ground_truth[:,:-1].clone()\n",
    "#     asr_tokens = asr_ground_truth[:,:-1].clone()\n",
    "#     # Clone for labels\n",
    "#     tts_labels = tts_ground_truth[:, 1:].clone()\n",
    "#     asr_labels = asr_ground_truth[:, 1:].clone()\n",
    "\n",
    "#     # TTS MASKING (easy)\n",
    "#     # labels = asr_ground_truth[:, 1:].clone()\n",
    "#     # Let's define the \"text portion\" as sysprompt + user_line only\n",
    "#     text_len = tts_sysprompt.size(1) + tts_user_line.size(1) + ASSISTANT_PREFIX_LEN - 1  # no VQ_WRAPPER or codes\n",
    "#     # ONLY mask codebook rows for that text region\n",
    "#     # row=0 is your \"text\" row, row=1..8 might be codebooks, or vice versa\n",
    "#     # (Here I'm assuming row=0 is your actual text tokens. \n",
    "#     #  If it's reversed, tweak accordingly!)\n",
    "#     tts_labels[1:, :text_len] = -100\n",
    "\n",
    "#     asr_start_len = asr_sysprompt.size(1) + USER_PREFIX_LEN - 1\n",
    "#     asr_labels[1:, :asr_start_len] = -100\n",
    "#     asr_labels[1:, -asr_assistant_line.size(1):] = -100\n",
    "\n",
    "#     out = {\n",
    "#         \"tokens\": [tts_tokens, asr_tokens],\n",
    "#         \"labels\": [tts_labels, asr_labels],\n",
    "#         \"task\": [\"tts\", \"asr\"],\n",
    "#         \"normalized_text\": [row[\"normalized_text\"]] * 2,\n",
    "#         \"speaker_id\": row[\"speaker_id\"] * 2,\n",
    "#         \"id\": row[\"id\"] * 2,\n",
    "#     }\n",
    "#     return out\n",
    "\n",
    "# TODO: Not doing ASR for now\n",
    "def tts_tokenize_row(row: Dict):\n",
    "    \"\"\"\n",
    "    NOTE: Deliberately ignores sysprompt line for now, can be done in packing\n",
    "    \"\"\"\n",
    "    user_line = encode_text(role=\"user\", content=row[\"normalized_text\"], add_generation_prompt=True)\n",
    "    assistant_line = encode_vq(row[\"codes\"])\n",
    "    ground_truth = torch.cat([user_line, assistant_line], dim=1)\n",
    "    # Causal shift\n",
    "    tokens = ground_truth[:,:-1].clone()\n",
    "    labels = ground_truth[:,1:].clone()\n",
    "\n",
    "    # Assuming user line took care of assistant prefix\n",
    "    labels[1:, :user_line.size(1) - 1] = -100\n",
    "    # Mask out newline\n",
    "    labels[1:, -1] = -100\n",
    "\n",
    "    return({\n",
    "        \"tokens\": tokens,\n",
    "        \"labels\": labels\n",
    "    })\n",
    "    \n",
    "\n",
    "\n",
    "example_row = tts_tokenize_row(dataset[\"test\"][0])\n",
    "tokenizer.decode(example_row[\"labels\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_row[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT INCREASE batch size\n",
    "dataset = dataset.map(tts_tokenize_row, remove_columns=\"codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWLINE_SEPARATOR = torch.tensor(tokenizer.encode(\"\\n\") + [0] * 8).unsqueeze(1)\n",
    "\n",
    "def batch_pack_sequences(examples, window_size=768, max_items=5):\n",
    "   \"\"\"\n",
    "   Pack sequences with system prompt and metrics\n",
    "   \"\"\"\n",
    "   packed_tokens = []\n",
    "   packed_labels = []\n",
    "   packed_speakers = []\n",
    "   pack_lengths = []\n",
    "   items_per_pack = []\n",
    "   \n",
    "   tokens = examples['tokens']\n",
    "   labels = examples['labels']\n",
    "   speakers = examples['speaker_id']\n",
    "   \n",
    "   # Account for system prompt in window size\n",
    "   effective_window = window_size - tts_sysprompt.shape[1]\n",
    "   \n",
    "   for i in range(len(tokens)):\n",
    "       seq_len = tokens[i].shape[1]\n",
    "       \n",
    "       # Start new pack\n",
    "       if i == 0 or current_length + seq_len > effective_window or \\\n",
    "          current_speaker != speakers[i] or current_items >= max_items:\n",
    "           \n",
    "           # Save previous pack if it exists\n",
    "           if i > 0 and current_tokens:\n",
    "               packed_tokens.append(torch.cat(current_tokens, dim=1))\n",
    "               packed_labels.append(torch.cat(current_labels, dim=1))\n",
    "               packed_speakers.append(current_speaker)\n",
    "               pack_lengths.append(current_length + tts_sysprompt.shape[1])\n",
    "               items_per_pack.append(current_items)\n",
    "           \n",
    "           # Initialize new pack with system prompt\n",
    "           current_tokens = [tts_sysprompt, tokens[i]]\n",
    "           current_labels = [tts_sysprompt, labels[i]]\n",
    "           current_speaker = speakers[i]\n",
    "           current_length = seq_len\n",
    "           current_items = 1\n",
    "           continue\n",
    "           \n",
    "       # Add to current pack with separator\n",
    "       current_tokens.extend([NEWLINE_SEPARATOR, tokens[i]])\n",
    "       current_labels.extend([NEWLINE_SEPARATOR, labels[i]])\n",
    "       current_length += seq_len + 1\n",
    "       current_items += 1\n",
    "   \n",
    "   # Don't forget last pack\n",
    "   if current_tokens:\n",
    "       packed_tokens.append(torch.cat(current_tokens, dim=1))\n",
    "       packed_labels.append(torch.cat(current_labels, dim=1))\n",
    "       packed_speakers.append(current_speaker)\n",
    "       pack_lengths.append(current_length + tts_sysprompt.shape[1])\n",
    "       items_per_pack.append(current_items)\n",
    "   \n",
    "   return {\n",
    "       'tokens': packed_tokens,\n",
    "       'labels': packed_labels,\n",
    "       'speaker_id': packed_speakers,\n",
    "       'pack_length': pack_lengths,\n",
    "       'items_in_pack': items_per_pack\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage:\n",
    "packed_dataset = dataset.map(\n",
    "    lambda row: batch_pack_sequences(row, max_items=3),\n",
    "    batched=True,\n",
    "    remove_columns=dataset['val'].column_names,\n",
    "    batch_size=1000  # Adjust based on memory constraints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_row = packed_dataset['val'][0]\n",
    "tokenizer.decode(example_row[\"tokens\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_dataset.save_to_disk(\"tokenized_libritts_packed_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(example_row[0][\"labels\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(example_row[1][\"labels\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_row[\"tokens\"][0][1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_row[1][\"labels\"][1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"test\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
