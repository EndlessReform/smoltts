{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize LibriTTS-R Mimi for target LM\n",
    "\n",
    "For our dataset, we currently simply use the Fish Speech TTS format:\n",
    "- Text-only data formatted using [ChatML](https://gist.github.com/edwardzjl/8df07c1f7140c9a3e2f48d33a8032090) as a separate sequence \"above\" the audio code stream\n",
    "- During sections where audio is being modeled, text stream 0 predicts the first semantic token index $n$ of the 8 Mimi residual codes as special token `<|semantic:n|>`\n",
    "- For audio, \"semantic\" (neural, there's not a strong distinction between) codes (from Mimi) padded with 0s during text sections\n",
    "\n",
    "It's possible this tokenization strategy can be improved, e.g. in [Defossez et al. 2024](https://arxiv.org/html/2410.00037v2#S3.SS4.SSS4) with the base transformer predicting the Whisper-timestamped word timings as an \"inner monologue\" and a delay between codebook timesteps. lol i'll do it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets, load_from_disk\n",
    "from data_pipeline.utils.prompt import PromptEncoder, TokenizationConfig\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "# If creating the libritts dataset for the first time\n",
    "# dataset = load_from_disk(\"../../Kokoro-82M/libritts_r_mimi_kokoro\")\n",
    "dataset = load_dataset(\"jkeisling/project-gutenberg-kokoro-2K\", token=os.getenv(\"HUGGINGFACE_TOKEN\"))\n",
    "# full_train = concatenate_datasets([dataset[\"train.clean.100\"], dataset[\"train.clean.360\"]])\n",
    "\n",
    "# dataset = DatasetDict({\n",
    "#     \"train\": full_train,\n",
    "#     \"val\": dataset[\"dev.clean\"],\n",
    "#     \"test\": dataset[\"test.clean\"]\n",
    "# })\n",
    "# dataset = DatasetDict({\"full\": dataset})\n",
    "dataset = dataset.with_format(\"torch\")\n",
    "# dataset = dataset.remove_columns([\"chapter_id\", \"text_original\"])\n",
    "# dataset = dataset.rename_column(original_column_name=\"text_normalized\", new_column_name=\"normalized_text\")\n",
    "dataset = dataset.rename_column(original_column_name=\"sentences\", new_column_name=\"text_normalized\")\n",
    "\n",
    "config = TokenizationConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAMERATE = 12.5\n",
    "# NOTE: DELETE THIS, HARD-CODED ASSUMPTION\n",
    "dataset = dataset.filter(lambda row: row[\"codes\"].size(-1) <= 15 * FRAMERATE, num_proc=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE! This is PATH DEPENDENT on ADDING THE SEMANTIC TOKENS TO THE TOKENIZER EARLIER using `create_bytelevel_init.ipynb`. DO NOT SKIP THIS STEP OR THE MODEL WILL BE IRRETRIEVABLY BROKEN! YOU HAVE BEEN WARNED.**\n",
    "\n",
    "==**THIS IS BYTE LEVEL!**=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../../inits/smoltts_byte_kokoro\")\n",
    "tokenizer.use_default_system_prompt = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check this carefully: for byte level, it should be 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenizer), tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please manually verify the text is done correctly. However, DECODE will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the tokenizer by encoding and decoding some example text\n",
    "example_text = \"<|im_start|>system\\n<|american|><|male|><|im_end|>\"\n",
    "encoded = tokenizer(example_text, return_tensors=\"pt\")\n",
    "print(f\"Encoded: {encoded['input_ids']}\")\n",
    "decoded = tokenizer.decode(encoded['input_ids'][0])\n",
    "\n",
    "# Print the results\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": \"help me i am trapped in this computer\"}], add_generation_prompt=True,  return_tensors=\"pt\")\n",
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "prompt_encoder = PromptEncoder(tokenizer, config)\n",
    "tts_sysprompt = prompt_encoder.encode_text_turn(role=\"system\", content=\"<|speaker:40|>\", add_generation_prompt=False)\n",
    "tokenizer.decode(tts_sysprompt[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this assumes you're using ChatML. if you're NOT, then there's quite a bit more to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = prompt_encoder.encode_vq(dataset[\"full\"][0][\"codes\"])\n",
    "tokenizer.decode(out[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_corrupt = prompt_encoder.encode_vq_corrupt(dataset[\"full\"][0][\"codes\"])\n",
    "tokenizer.decode(out_corrupt[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_names = [\"default\", \"sarah\", \"sky\", \"adam\", \"emma\", \"isabella\", \"george\", \"lewis\"]\n",
    "speaker_ids = {value: index for index, value in enumerate(speaker_names)}\n",
    "speaker_ids[\"adam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "# import random\n",
    "\n",
    "# TODO: Not doing ASR for now\n",
    "def tts_tokenize_row(row: Dict):\n",
    "    \"\"\"\n",
    "    NOTE: Deliberately ignores sysprompt line for now, can be done in packing\n",
    "    \"\"\"\n",
    "    # TODO: Fix this upstream in the data gen!\n",
    "    # gender = \"<|male|>\" if row[\"speaker_id\"] in [\"george\", \"lewis\", \"adam\", \"michael\"] else \"<|female|>\"\n",
    "    # accent = f\"<|{row['accent']}|>\"\n",
    "    # speaker = f\"<|speaker:{speaker_ids[row['speaker_id']]}|>\" if random.random() < 0.7 else \"\"\n",
    "    speaker = f\"<|speaker:{speaker_ids[row['speaker_id']]}|>\"\n",
    "\n",
    "    # Just keep it all for now, will test generalization later\n",
    "    system_line = prompt_encoder.encode_text_turn(role=\"system\", content=\"\".join([speaker]))\n",
    "    user_line = prompt_encoder.encode_text_turn(\n",
    "        role=\"user\", \n",
    "        content=row[\"text_normalized\"].encode(\"utf-8\").decode(\"latin-1\"), \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    assistant_line = prompt_encoder.encode_vq(row[\"codes\"])\n",
    "    ground_truth = torch.cat([system_line, user_line, assistant_line], dim=1)\n",
    "    # ground_truth = torch.cat([user_line, assistant_line], dim=1)\n",
    "    # Causal shift\n",
    "    tokens = ground_truth[:,:-1].clone()\n",
    "    labels = ground_truth[:,1:].clone()\n",
    "\n",
    "    # Assuming user line took care of assistant prefix \n",
    "    # Offsetting by 1 since labels were shifted\n",
    "    text_only_length = system_line.size(1) + user_line.size(1) - 1\n",
    "    labels[1:, :text_only_length] = -100\n",
    "    # Mask out im_end and newline\n",
    "    labels[1:, -2:] = -100\n",
    "\n",
    "    return({\n",
    "        \"tokens\": tokens,\n",
    "        \"labels\": labels\n",
    "    })\n",
    "    \n",
    "\n",
    "\n",
    "example_row = tts_tokenize_row(dataset[\"full\"][10])\n",
    "tokenizer.decode(example_row[\"tokens\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict\n",
    "# import random\n",
    "\n",
    "# TODO: Not doing ASR for now\n",
    "def tts_tokenize_row_dropout(row: Dict):\n",
    "    \"\"\"\n",
    "    NOTE: Deliberately ignores sysprompt line for now, can be done in packing\n",
    "    \"\"\"\n",
    "    # TODO: Fix this upstream in the data gen!\n",
    "    # gender = \"<|male|>\" if row[\"speaker_id\"] in [\"george\", \"lewis\", \"adam\", \"michael\"] else \"<|female|>\"\n",
    "    # accent = f\"<|{row['accent']}|>\"\n",
    "    # speaker = f\"<|speaker:{speaker_ids[row['speaker_id']]}|>\" if random.random() < 0.7 else \"\"\n",
    "    speaker = f\"<|speaker:{speaker_ids[row['speaker_id']]}|>\"\n",
    "\n",
    "    # Just keep it all for now, will test generalization later\n",
    "    system_line = prompt_encoder.encode_text_turn(role=\"system\", content=\"\".join([speaker]))\n",
    "    user_line = prompt_encoder.encode_text_turn(\n",
    "        role=\"user\", \n",
    "        content=row[\"text_normalized\"].encode(\"utf-8\").decode(\"latin-1\"), \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    assistant_line_true = prompt_encoder.encode_vq(row[\"codes\"])\n",
    "    assistant_line_dropout = prompt_encoder.encode_vq_corrupt(row[\"codes\"], dropout=0.3)\n",
    "    messy_input = torch.cat([system_line, user_line, assistant_line_dropout], dim=1)\n",
    "    ground_truth = torch.cat([system_line, user_line, assistant_line_true], dim=1)\n",
    "    # Causal shift\n",
    "    tokens = messy_input[:,:-1]\n",
    "    labels = ground_truth[:,1:]\n",
    "\n",
    "    # Assuming user line took care of assistant prefix \n",
    "    # Offsetting by 1 since labels were shifted\n",
    "    text_only_length = system_line.size(1) + user_line.size(1) - 1\n",
    "    labels[1:, :text_only_length] = -100\n",
    "    # Mask out im_end and newline\n",
    "    labels[1:, -2:] = -100\n",
    "\n",
    "    return({\n",
    "        \"tokens\": tokens,\n",
    "        \"labels\": labels\n",
    "    })\n",
    "    \n",
    "\n",
    "\n",
    "example_row = tts_tokenize_row_dropout(dataset[\"full\"][10])\n",
    "tokenizer.decode(example_row[\"tokens\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_row[\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_row[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT INCREASE batch size\n",
    "dataset = dataset.map(tts_tokenize_row, remove_columns=\"codes\", num_proc=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk(\"../../datasets/tokenized_project_gutenberg_bytes_kokoro_tau\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch, semantic_pad_id: int):\n",
    "    \"\"\"\n",
    "    batch is a list of dicts: each dict has \"tokens\" shape [9, T],\n",
    "    and \"labels\" shape [9, T].\n",
    "    We pad them into [B, 9, T_max].\n",
    "    \"\"\"\n",
    "    max_input_len = max(item[\"tokens\"].shape[1] for item in batch)\n",
    "\n",
    "    B = len(batch)\n",
    "    # We'll create padded arrays:\n",
    "    tokens = torch.full((B, 9, max_input_len), 0, dtype=torch.long)  # 2=some <PAD>\n",
    "    tokens[:, 0, :] = semantic_pad_id\n",
    "    labels = torch.full(\n",
    "        (B, 9, max_input_len), -100, dtype=torch.long\n",
    "    )  # default is ignore_index\n",
    "\n",
    "    pad_mask = torch.ones(B, max_input_len)\n",
    "\n",
    "    for i, item in enumerate(batch):\n",
    "        seq_len = item[\"tokens\"].shape[1]\n",
    "        tokens[i, :, :seq_len] = item[\"tokens\"]\n",
    "        labels[i, :, :seq_len] = item[\"labels\"][:, :seq_len]\n",
    "        pad_mask[i, :seq_len] = False\n",
    "\n",
    "    return {\"tokens\": tokens, \"labels\": labels, \"pad_mask\": pad_mask}\n",
    "\n",
    "# Create two test sequences of different lengths\n",
    "seq1 = torch.randint(1, 100, (9, 5))  # Short sequence\n",
    "seq2 = torch.randint(1, 100, (9, 8))  # Longer sequence\n",
    "\n",
    "batch = [\n",
    "    {\"tokens\": seq1, \"labels\": seq1},\n",
    "    {\"tokens\": seq2, \"labels\": seq2}\n",
    "]\n",
    "\n",
    "# Test the collation\n",
    "semantic_pad_id = 999\n",
    "result = collate_fn(batch, semantic_pad_id)\n",
    "\n",
    "print(\"Tokens shape:\", result[\"tokens\"].shape)\n",
    "print(\"\\nFirst sequence tokens:\")\n",
    "print(result[\"tokens\"][0])\n",
    "print(\"\\nSecond sequence tokens:\")\n",
    "print(result[\"tokens\"][1])\n",
    "print(\"\\nPadding mask:\")\n",
    "print(result[\"pad_mask\"])\n",
    "\n",
    "# Let's verify:\n",
    "# 1. Sequences are left-aligned\n",
    "# 2. Padding is applied correctly\n",
    "# 3. Padding mask matches content\n",
    "\n",
    "# Check alignment of first sequence (should be at start)\n",
    "print(\"\\nFirst 5 tokens of first sequence row 1:\")\n",
    "print(result[\"tokens\"][0, 1, :5])\n",
    "print(\"Next 3 tokens (should be 0s):\")\n",
    "print(result[\"tokens\"][0, 1, 5:8])\n",
    "\n",
    "# Check padding of first row\n",
    "print(\"\\nFirst row padding for batch item 0:\")\n",
    "print(result[\"tokens\"][0, 0, :8])  # Should be semantic_pad_id\n",
    "\n",
    "# Check mask alignment\n",
    "print(\"\\nFirst sequence mask (False=content, True=padding):\")\n",
    "print(result[\"pad_mask\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length(example):\n",
    "    return {'length': example['labels'].shape[1]}\n",
    "\n",
    "max_len = 0\n",
    "def update_max(example):\n",
    "    global max_len\n",
    "    max_len = max(max_len, example['length'])\n",
    "    return example\n",
    "\n",
    "# Apply the transformations\n",
    "dataset[\"train\"].map(\n",
    "    get_length,\n",
    "    desc=\"Getting sequence lengths\"\n",
    ").map(\n",
    "    update_max,\n",
    "    desc=\"Finding maximum\"\n",
    ")\n",
    "\n",
    "print(f\"Maximum sequence length: {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get arrays from dataset\n",
    "text_lengths = np.array([len(x) for x in dataset[\"train\"]['normalized_text']])\n",
    "seq_lengths = np.array([x.shape[1] for x in dataset[\"train\"]['labels']])\n",
    "\n",
    "# Calculate ratios\n",
    "ratios = text_lengths / seq_lengths\n",
    "\n",
    "# Basic stats\n",
    "print(f\"Mean ratio: {ratios.mean():.3f}\")\n",
    "print(f\"Std ratio: {ratios.std():.3f}\")\n",
    "print(f\"\\nPercentile distribution:\")\n",
    "for p in [1, 5, 25, 50, 75, 95, 99]:\n",
    "    print(f\"{p}th percentile: {np.percentile(ratios, p):.3f}\")\n",
    "\n",
    "# Find extreme outliers (3 std from mean)\n",
    "mean, std = ratios.mean(), ratios.std()\n",
    "outliers = np.where(np.abs(ratios - mean) > 3 * std)[0]\n",
    "if len(outliers) > 0:\n",
    "    print(f\"\\nFound {len(outliers)} outliers\")\n",
    "    print(\"\\nSample of 5 outlier examples:\")\n",
    "    for idx in outliers[:5]:\n",
    "        print(f\"\\nIndex {int(idx)}\")  # Convert numpy int to Python int\n",
    "        print(f\"Text ({text_lengths[idx]} chars): {dataset['val'][int(idx)]['normalized_text'][:100]}...\")  # Convert idx\n",
    "        print(f\"Sequence length: {seq_lengths[idx]}\")\n",
    "        print(f\"Ratio: {ratios[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWLINE_SEPARATOR = torch.tensor(tokenizer.encode(\"\\n\") + [0] * 8).unsqueeze(1)\n",
    "\n",
    "def batch_pack_sequences(examples, window_size=768, max_items=5):\n",
    "   \"\"\"\n",
    "   Pack sequences with system prompt and metrics\n",
    "   \"\"\"\n",
    "   packed_tokens = []\n",
    "   packed_labels = []\n",
    "   packed_speakers = []\n",
    "   pack_lengths = []\n",
    "   items_per_pack = []\n",
    "   \n",
    "   tokens = examples['tokens']\n",
    "   labels = examples['labels']\n",
    "   speakers = examples['speaker_id']\n",
    "   \n",
    "   # Account for system prompt in window size\n",
    "   effective_window = window_size - tts_sysprompt.shape[1]\n",
    "   \n",
    "   for i in range(len(tokens)):\n",
    "       seq_len = tokens[i].shape[1]\n",
    "       \n",
    "       # Start new pack\n",
    "       if i == 0 or current_length + seq_len > effective_window or \\\n",
    "          current_speaker != speakers[i] or current_items >= max_items:\n",
    "           \n",
    "           # Save previous pack if it exists\n",
    "           if i > 0 and current_tokens:\n",
    "               packed_tokens.append(torch.cat(current_tokens, dim=1))\n",
    "               packed_labels.append(torch.cat(current_labels, dim=1))\n",
    "               packed_speakers.append(current_speaker)\n",
    "               pack_lengths.append(current_length + tts_sysprompt.shape[1])\n",
    "               items_per_pack.append(current_items)\n",
    "           \n",
    "           # Initialize new pack with system prompt\n",
    "           current_tokens = [tts_sysprompt, tokens[i]]\n",
    "           current_labels = [tts_sysprompt, labels[i]]\n",
    "           current_speaker = speakers[i]\n",
    "           current_length = seq_len\n",
    "           current_items = 1\n",
    "           continue\n",
    "           \n",
    "       # Add to current pack with separator\n",
    "       current_tokens.extend([NEWLINE_SEPARATOR, tokens[i]])\n",
    "       current_labels.extend([NEWLINE_SEPARATOR, labels[i]])\n",
    "       current_length += seq_len + 1\n",
    "       current_items += 1\n",
    "   \n",
    "   # Don't forget last pack\n",
    "   if current_tokens:\n",
    "       packed_tokens.append(torch.cat(current_tokens, dim=1))\n",
    "       packed_labels.append(torch.cat(current_labels, dim=1))\n",
    "       packed_speakers.append(current_speaker)\n",
    "       pack_lengths.append(current_length + tts_sysprompt.shape[1])\n",
    "       items_per_pack.append(current_items)\n",
    "   \n",
    "   return {\n",
    "       'tokens': packed_tokens,\n",
    "       'labels': packed_labels,\n",
    "       'speaker_id': packed_speakers,\n",
    "       'pack_length': pack_lengths,\n",
    "       'items_in_pack': items_per_pack\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage:\n",
    "packed_dataset = dataset.map(\n",
    "    lambda row: batch_pack_sequences(row, max_items=3),\n",
    "    batched=True,\n",
    "    remove_columns=dataset['val'].column_names,\n",
    "    batch_size=1000  # Adjust based on memory constraints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_row = packed_dataset['val'][0]\n",
    "tokenizer.decode(example_row[\"tokens\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_dataset.save_to_disk(\"tokenized_libritts_packed_3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
