{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte level ablation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make a spurious \"BPE\" tokenizer without any actual byte pairs. \n",
    "\n",
    "This assumes you copied and pasted a folder to `../checkpoints/smoltts_byte` with the `config.json` of a normal model in it. If you didn't, do that now, by running the regular \"create init\" notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, normalizers, decoders, pre_tokenizers\n",
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# Configure trainer\n",
    "trainer = BpeTrainer(vocab_size=256, special_tokens=[])\n",
    "\n",
    "# Generate actual bytes for training\n",
    "byte_data = [bytes([i]) for i in range(256)]  # Create actual bytes\n",
    "# Convert to strings that preserve the byte values\n",
    "byte_strings = [b.decode('latin-1') for b in byte_data]  \n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train_from_iterator(byte_strings, trainer=trainer)\n",
    "# tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "tokenizer.pre_tokenizer = None\n",
    "tokenizer.normalizer = None\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "\n",
    "# Check the result\n",
    "print(tokenizer.get_vocab())  # Should show all 256 bytes + special tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it works quickly as a round-trip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evil_string = \"å¿ƒ\".encode(\"utf-8\").decode(\"latin-1\")\n",
    "print(f\"Evil string: {evil_string}\")\n",
    "enc = tokenizer.encode(evil_string)\n",
    "print(enc.ids)\n",
    "decoded_bytes = bytes(enc.ids).decode('utf-8')\n",
    "decoded_bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CODEBOOK_SIZE=2048\n",
    "semantic_tokens = [f\"<|semantic:{i}|>\" for i in range(CODEBOOK_SIZE)]\n",
    "control_tokens = [\n",
    "    \"system\", \n",
    "    \"user\", \n",
    "    \"assistant\",\n",
    "    \"<|british|>\",\n",
    "    \"<|american|>\",\n",
    "    \"<|male|>\",\n",
    "    \"<|female|>\",\n",
    "    \"<|unknown|>\",\n",
    "    \"<|endoftext|>\", \n",
    "    \"<|voice|>\", \n",
    "    \"<|semantic|>\",\n",
    "    \"<|pad|>\",\n",
    "    \"<|epad|>\",\n",
    "    \"<|im_start|>\", \n",
    "    \"<|im_end|>\", \n",
    "]\n",
    "# Reserve individual speaker IDs as control tokens\n",
    "unused_tokens = [f\"<|speaker:{i}|>\" for i in range(64 - len(control_tokens))]\n",
    "charset = [*control_tokens, *unused_tokens, *semantic_tokens]\n",
    "print(len(charset))\n",
    "charset[:67]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens(charset)\n",
    "tokenizer.pad_token = \"<|pad|>\"\n",
    "tokenizer.eos_token = \"<|endoftext|>\"\n",
    "tokenizer.bos_token = \"<|im_start|>\"\n",
    "tokenizer.unk_token = \"<|unknown|>\"\n",
    "tokenizer.chat_template = \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Create the fast tokenizer with all settings in one shot\n",
    "final_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,  # your existing byte-level tokenizer\n",
    "    bos_token=\"<|im_start|>\",\n",
    "    eos_token=\"<|endoftext|>\",\n",
    "    unk_token=\"<|unknown|>\",\n",
    "    pad_token=\"<|pad|>\",\n",
    "    chat_template=\"\"\"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"\"\"\n",
    ")\n",
    "\n",
    "# Save it\n",
    "final_tokenizer.save_pretrained(\"../checkpoints/smoltts_byte_kokoro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give this a final test before we dump compute into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test encoding of ASCII + special tokens + semantic tokens\n",
    "test_prompt = \"<|im_start|>system\\n<|american|><|female|><|speaker:4|><|im_end|>\\n<|im_start|>user\\nHello!<|im_end|>\\n<|semantic:42|>\"\n",
    "\n",
    "# Encode and look at IDs\n",
    "ids = tokenizer.encode(test_prompt.encode(\"utf-8\").decode(\"latin-1\"))\n",
    "print(\"Token IDs:\", ids)\n",
    "\n",
    "# Test decoding individual tokens\n",
    "print(\"\\nDecoding each token:\")\n",
    "for id in ids.ids:\n",
    "    if id <= 255:\n",
    "        print(f\"Byte {id}: {repr(tokenizer.decode([id]))}\")\n",
    "    else:\n",
    "        print(f\"Special {id}: {repr(tokenizer.id_to_token(id))}\")\n",
    "\n",
    "# Verify our semantic token ID maps correctly\n",
    "semantic_42 = tokenizer.encode(\"<|semantic:42|>\")\n",
    "print(\"\\nSemantic token 42:\", semantic_42.ids)\n",
    "print(\"Decodes back to:\", repr(tokenizer.decode(semantic_42.ids)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save back the vocab size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load config\n",
    "with open('../checkpoints/smoltts_byte/config.json', 'r') as f:\n",
    "   config = json.load(f)\n",
    "\n",
    "# Get vocab size from tokenizer \n",
    "vocab_size = 256 + len(charset)  # Base bytes + special tokens\n",
    "config['vocab_size'] = vocab_size\n",
    "\n",
    "# Save updated config\n",
    "with open('../checkpoints/smoltts_byte_kokoro/config.json', 'w') as f:\n",
    "   json.dump(config, f, indent=4)\n",
    "\n",
    "print(f\"Updated vocab_size to {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug space encoding\n",
    "print(\"Raw space char code:\", ord(\" \"))  # Should be 32\n",
    "print(\"Space as bytes:\", \" \".encode('utf-8'))  # Should be b' '\n",
    "print(\"Space as latin1:\", \" \".encode('latin1'))  # Should be b' '\n",
    "\n",
    "# Test different space characters\n",
    "print(\"\\nTokenizer tests:\")\n",
    "print('ASCII space (32):', tokenizer.encode(\" \").ids)\n",
    "print('NBSP (160):', tokenizer.encode(\"\\u00A0\"))\n",
    "print('Raw byte 32:', tokenizer.encode(bytes([32]).decode('latin1')))\n",
    "\n",
    "# Look at normalizer config\n",
    "print(\"\\nTokenizer config:\")\n",
    "\n",
    "# Try encoding a string with spaces\n",
    "print(\"\\nString with spaces:\")\n",
    "test = \"a b c\"\n",
    "print(\"String:\", repr(test))\n",
    "print(\"Encoded:\", tokenizer.encode(test).ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare AutoTokenizer vs PreTrainedTokenizerFast\n",
    "from transformers import AutoTokenizer, PreTrainedTokenizerFast\n",
    "\n",
    "auto = AutoTokenizer.from_pretrained(\"../checkpoints/smoltts_byte\")\n",
    "fast = PreTrainedTokenizerFast.from_pretrained(\"../checkpoints/smoltts_byte\")\n",
    "\n",
    "test = \"a, b\"\n",
    "print(\"AutoTokenizer config:\")\n",
    "print(\"Type:\", type(auto))\n",
    "print(\"Normalizer:\", auto.backend_tokenizer.normalizer)\n",
    "print(\"Pre-tokenizer:\", auto.backend_tokenizer.pre_tokenizer)\n",
    "print(\"Post-processor:\", auto.backend_tokenizer.post_processor)\n",
    "\n",
    "print(\"\\nPreTrainedTokenizerFast config:\")\n",
    "print(\"Type:\", type(fast))\n",
    "print(\"Normalizer:\", fast.backend_tokenizer.normalizer)\n",
    "print(\"Pre-tokenizer:\", fast.backend_tokenizer.pre_tokenizer)\n",
    "print(\"Post-processor:\", fast.backend_tokenizer.post_processor)\n",
    "\n",
    "print(\"\\nEncoding tests:\")\n",
    "print(\"Auto:\", auto.encode(test))\n",
    "print(\"Fast:\", fast.encode(test))\n",
    "\n",
    "# Check what tokenizer_config.json looks like\n",
    "import json\n",
    "with open(\"../checkpoints/smoltts_byte/tokenizer_config.json\") as f:\n",
    "    config = json.load(f)\n",
    "print(\"\\nTokenizer config file:\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [265, 256, 10, 83, 112, 101, 97, 107, 32, 111, 117, 116, 32, 116, 104, 101, 32, 112, 114, 111, 118, 105, 100, 101, 100, 32, 116, 101, 120, 116, 266, 265, 257, 10, 116, 101, 115, 116, 266, 265, 258, 10]\n",
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode([265])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
