{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize LibriTTS-R Mimi for target LM\n",
    "\n",
    "For our dataset, we currently simply use the Fish Speech TTS format:\n",
    "- Text-only data formatted using [ChatML](https://gist.github.com/edwardzjl/8df07c1f7140c9a3e2f48d33a8032090) as a separate sequence \"above\" the audio code stream\n",
    "- During sections where audio is being modeled, text stream 0 predicts the first semantic token index $n$ of the 8 Mimi residual codes as special token `<|semantic:n|>`\n",
    "- For audio, \"semantic\" (neural, there's not a strong distinction between) codes (from Mimi) padded with 0s during text sections\n",
    "\n",
    "It's possible this tokenization strategy can be improved, e.g. in [Defossez et al. 2024](https://arxiv.org/html/2410.00037v2#S3.SS4.SSS4) with the base transformer predicting the Whisper-timestamped word timings as an \"inner monologue\" and a delay between codebook timesteps. lol i'll do it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ritsuko/ai/audio/dual-ar/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "\n",
    "# If creating the libritts dataset for the first time\n",
    "# from datasets import load_from_disk \n",
    "# dataset = load_from_disk(\"encoded_dataset\")\n",
    "# train_clean_100 = load_from_disk(\"encoded_libritts/train.clean.100/\")\n",
    "# train_clean_360 = load_from_disk(\"encoded_libritts/train.clean.360/\")\n",
    "# dev_clean = load_from_disk(\"encoded_libritts/dev.clean\")\n",
    "# test_clean = load_from_disk(\"encoded_libritts/test.clean\")\n",
    "# full_train = concatenate_datasets([train_clean_100, train_clean_360])\n",
    "dataset = load_dataset(\"jkeisling/libritts-r-mimi-kokoro\")\n",
    "full_train = concatenate_datasets([dataset[\"train.clean.100\"], dataset[\"train.clean.360\"]])\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": full_train,\n",
    "    \"val\": dataset[\"dev.clean\"],\n",
    "    \"test\": dataset[\"test.clean\"]\n",
    "})\n",
    "dataset = dataset.with_format(\"torch\")\n",
    "dataset = dataset.remove_columns([\"chapter_id\", \"text_original\"])\n",
    "dataset = dataset.rename_column(original_column_name=\"text_normalized\", new_column_name=\"normalized_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE! This is PATH DEPENDENT on ADDING THE SEMANTIC TOKENS TO THE TOKENIZER EARLIER using `create_bytelevel_init.ipynb`. DO NOT SKIP THIS STEP OR THE MODEL WILL BE IRRETRIEVABLY BROKEN! YOU HAVE BEEN WARNED.**\n",
    "\n",
    "==**THIS IS BYTE LEVEL!**=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../checkpoints/smoltts_byte_kokoro\")\n",
    "tokenizer.use_default_system_prompt = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check this carefully: for byte level, it should be 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2368, 256)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer), tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please manually verify the text is done correctly. However, DECODE will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: tensor([[269, 256,  10, 260, 261, 270]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n<|american|><|male|><|im_end|>'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the tokenizer by encoding and decoding some example text\n",
    "example_text = \"<|im_start|>system\\n<|american|><|male|><|im_end|>\"\n",
    "encoded = tokenizer(example_text, return_tensors=\"pt\")\n",
    "print(f\"Encoded: {encoded['input_ids']}\")\n",
    "decoded = tokenizer.decode(encoded['input_ids'][0])\n",
    "\n",
    "# Print the results\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[269, 257,  10, 104, 101, 108, 112,  32, 109, 101,  32, 105,  32,  97,\n",
       "         109,  32, 116, 114,  97, 112, 112, 101, 100,  32, 105, 110,  32, 116,\n",
       "         104, 105, 115,  32,  99, 111, 109, 112, 117, 116, 101, 114, 270,  10,\n",
       "         269, 258,  10]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": \"help me i am trapped in this computer\"}], add_generation_prompt=True,  return_tensors=\"pt\")\n",
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nhelp me i am trapped in this computer<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(sequence[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n<|speaker:40|><|im_end|>\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def encode_text(role: str, content: str, add_generation_prompt: bool = True) -> torch.Tensor:\n",
    "    # baseline = tokenizer.apply_chat_template(f\"{chr(10) if ''}<|im_start|>{role}\\n{content}<|im_end|>\\n\",)\n",
    "    baseline = tokenizer.apply_chat_template(\n",
    "        [{\"role\": role, \"content\": content}],\n",
    "        add_generation_prompt=add_generation_prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    zeros_mask = torch.zeros(8, baseline.size(1), dtype=baseline.dtype)\n",
    "    return torch.cat([baseline, zeros_mask])\n",
    "\n",
    "tts_sysprompt = encode_text(\"system\", \"<|speaker:40|>\", add_generation_prompt=False)\n",
    "tokenizer.decode(tts_sysprompt[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this assumes you're using ChatML. if you're NOT, then there's quite a bit more to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|semantic:1049|><|semantic:127|><|semantic:1880|><|semantic:1031|><|semantic:1031|><|semantic:1031|><|semantic:1799|><|semantic:1268|><|semantic:549|><|semantic:1768|><|semantic:184|><|semantic:1538|><|semantic:95|><|semantic:648|><|semantic:1281|><|semantic:162|><|semantic:1782|><|semantic:536|><|semantic:230|><|semantic:2040|><|semantic:133|><|semantic:1117|><|semantic:674|><|semantic:507|><|semantic:507|><|semantic:1399|><|semantic:640|><|semantic:1277|><|semantic:330|><|semantic:740|><|semantic:690|><|semantic:875|><|semantic:851|><|semantic:1772|><|semantic:186|><|semantic:670|><|semantic:1612|><|semantic:1532|><|semantic:119|><|semantic:1052|><|semantic:1029|><|semantic:1029|><|semantic:1532|><|semantic:1161|><|semantic:1344|><|semantic:1109|><|semantic:544|><|semantic:6|><|semantic:56|><|semantic:1640|><|semantic:1928|><|semantic:1968|><|semantic:657|><|semantic:382|><|semantic:382|><|semantic:596|><|semantic:553|><|semantic:2030|><|semantic:578|><|semantic:531|><|semantic:616|><|semantic:1271|><|semantic:1271|><|semantic:1271|><|semantic:1640|><|semantic:769|><|semantic:666|><|semantic:84|><|semantic:84|><|semantic:752|><|semantic:752|><|semantic:752|><|semantic:752|><|semantic:481|><|im_end|>\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEMANTIC_OFFSET = tokenizer.encode(\"<|semantic:0|>\")[0]\n",
    "# B * C+1 * 2\n",
    "VQ_USER_PREFIX = encode_text(role=\"user\", content=\"\")[:,:-2]\n",
    "TRAILING_IM_END = torch.tensor([\n",
    "    tokenizer.encode(\"<|im_end|>\") + [0] * 8,\n",
    "    tokenizer.encode(\"\\n\") + [0] * 8,\n",
    "]).T\n",
    "\n",
    "def encode_vq(codes: torch.Tensor, is_assistant=True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Expects C * T\n",
    "    \"\"\"\n",
    "    if codes.ndim != 2:\n",
    "        raise ValueError(\"Must be single batch\")\n",
    "    speaker_line = codes[0,:] + SEMANTIC_OFFSET\n",
    "    vq_block = torch.cat([speaker_line.unsqueeze(0), codes])\n",
    "\n",
    "    block = torch.cat([vq_block, TRAILING_IM_END], dim=1)\n",
    "    return block if is_assistant else torch.cat([VQ_USER_PREFIX, block], dim=1)\n",
    "\n",
    "\n",
    "out = encode_vq(dataset[\"test\"][0][\"codes\"], is_assistant=True)\n",
    "tokenizer.decode(out[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_names = [\"default\", \"bella\", \"nicole\", \"sarah\", \"sky\", \"adam\", \"michael\", \"emma\", \"isabella\", \"george\", \"lewis\"]\n",
    "speaker_ids = {value: index for index, value in enumerate(speaker_names)}\n",
    "speaker_ids[\"adam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system\\n<|american|><|female|><|im_end|>\\n<|im_start|>assistant\\n<|im_start|>user\\nYou haven\\'t spoken a word.\"<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1049|><|semantic:127|><|semantic:1880|><|semantic:1031|><|semantic:1031|><|semantic:1031|><|semantic:1492|><|semantic:1492|><|semantic:1923|><|semantic:1488|><|semantic:376|><|semantic:1849|><|semantic:1906|><|semantic:1135|><|semantic:334|><|semantic:1782|><|semantic:1399|><|semantic:808|><|semantic:689|><|semantic:997|><|semantic:168|><|semantic:1702|><|semantic:2009|><|semantic:56|><|semantic:769|><|semantic:666|><|semantic:84|><|semantic:84|><|semantic:752|><|semantic:752|><|semantic:1850|><|semantic:752|><|semantic:481|><|semantic:481|><|im_end|>\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "import random\n",
    "\n",
    "# TODO: Not doing ASR for now\n",
    "def tts_tokenize_row(row: Dict):\n",
    "    \"\"\"\n",
    "    NOTE: Deliberately ignores sysprompt line for now, can be done in packing\n",
    "    \"\"\"\n",
    "    # TODO: Fix this upstream in the data gen!\n",
    "    gender = \"<|male|>\" if row[\"speaker_id\"] in [\"george\", \"lewis\", \"adam\", \"michael\"] else \"<|female|>\"\n",
    "    accent = f\"<|{row['accent']}|>\"\n",
    "    speaker = f\"<|speaker:{speaker_ids[row['speaker_id']]}|>\" if random.random() < 0.7 else \"\"\n",
    "\n",
    "    # Just keep it all for now, will test generalization later\n",
    "    system_line = encode_text(role=\"system\", content=\"\".join([accent, gender, speaker]))\n",
    "    user_line = encode_text(\n",
    "        role=\"user\", \n",
    "        content=row[\"normalized_text\"].encode(\"utf-8\").decode(\"latin-1\"), \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    assistant_line = encode_vq(row[\"codes\"])\n",
    "    ground_truth = torch.cat([system_line, user_line, assistant_line], dim=1)\n",
    "    # Causal shift\n",
    "    tokens = ground_truth[:,:-1].clone()\n",
    "    labels = ground_truth[:,1:].clone()\n",
    "\n",
    "    # Assuming user line took care of assistant prefix\n",
    "    labels[1:, :user_line.size(1) - 1] = -100\n",
    "    # Mask out newline\n",
    "    labels[1:, -1] = -100\n",
    "\n",
    "    return({\n",
    "        \"tokens\": tokens,\n",
    "        \"labels\": labels\n",
    "    })\n",
    "    \n",
    "\n",
    "\n",
    "example_row = tts_tokenize_row(dataset[\"test\"][10])\n",
    "tokenizer.decode(example_row[\"labels\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'normalized_text': \"It's abominable of you to ask me.\",\n",
       " 'speaker_id': 'default',\n",
       " 'id': '4446_2275_000042_000004',\n",
       " 'codes': tensor([[1049,  127, 1880, 1031, 1031, 1031, 1492, 1492,  322,  301, 1237,  680,\n",
       "          1477, 1275, 2003,   22, 1137, 1137, 1347, 1293,  786,   70,  336, 1323,\n",
       "          1508, 1421, 1215, 1424,  779,  779,  705,  582,  666,  716,   84,   84,\n",
       "           752,  752,  752,  752,  752],\n",
       "         [1700,  243,  243,  243,  243, 1211, 1211, 1211, 1469,  678,  106,  393,\n",
       "          1032, 1377,  359,  457, 1114, 1859, 1196,  653,   56, 1731,  352,  375,\n",
       "          1295, 1847,   96,  446,  724,  317, 1966, 1700,  243,  243,  243,  243,\n",
       "           243,  243,  243,  243,  243],\n",
       "         [1626,  783, 1178, 1178, 1178, 1697, 1697, 1697, 1591, 1221, 2025, 1302,\n",
       "           600, 1294, 1214,  790,   38, 1890, 2040,  738, 1298, 1672,  713,  360,\n",
       "          1739,   87, 1627,  710,  466, 1063, 1039, 1029,  783, 1178,  783, 1178,\n",
       "          1178, 1178, 1178,  783, 1178],\n",
       "         [ 546,  546,  290,  290,  142,  546,  546,  290,  259, 1478, 1341,  193,\n",
       "           817,   95, 1112, 1185, 1219,  745, 1241,  923,  589, 1726, 1581, 1675,\n",
       "          1960,  695,  580,   67,  863,  664, 1937, 1562,  290,  546,  290,  546,\n",
       "           546,  546,  546, 1348,  546],\n",
       "         [ 306,  481, 1736, 1736,  481, 1736,  481,  267,  315,  117, 1614, 1477,\n",
       "           667, 1398, 1555, 1341,  795,  386, 1252, 1080,  679,  485, 1212, 1073,\n",
       "          1277,  709,  655,  640, 1442, 1559,  787,  267, 1736,  267, 1736,  267,\n",
       "           267,  267,  481, 1335,  481],\n",
       "         [1443,  555, 1572, 1030, 1572, 1572, 1030, 1572,  736,  506,  443, 1494,\n",
       "           216, 1859,  813,  579, 1450,  243, 1307, 1373,   66, 1117,  933, 1758,\n",
       "          1773,  566, 1104,  481, 1421,  846,  438, 1030, 1030, 1030, 1030, 1030,\n",
       "          1030, 1030, 1030, 1030, 1572],\n",
       "         [1871,  666,  666,  666, 1978, 1978,  825,  976, 1664,  935, 1101, 1296,\n",
       "           407, 1152,  988, 1304, 1662,  361,  231, 1313, 1018,  307,  314, 1889,\n",
       "          1326,  981, 1759,  665,  511, 1995,  205,  183,  976, 1978,  976,  976,\n",
       "           976,  976,  666,  976,  976],\n",
       "         [2008, 1648, 1744, 2008, 1648, 1648, 1648, 2008, 1874, 2013,   58,  479,\n",
       "          1023,  766,  605, 1629, 1209,  215,  130, 1411,  718,  348, 1110, 2022,\n",
       "           927, 1417,  216, 1266,  804, 1062, 1409, 1648, 1744, 2008, 2008, 2008,\n",
       "          2008, 2008, 1744, 2008, 2008]]),\n",
       " 'accent': 'american',\n",
       " 'gender': 'female'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 256,   10,  260,  262,  270,   10,  269,  258,   10,  269,  257,   10,\n",
       "           89,  111,  117,   32,  104,   97,  118,  101,  110,   39,  116,   32,\n",
       "          115,  112,  111,  107,  101,  110,   32,   97,   32,  119,  111,  114,\n",
       "          100,   46,   34,  270,   10,  269,  258,   10, 1369,  447, 2200, 1351,\n",
       "         1351, 1351, 1812, 1812, 2243, 1808,  696, 2169, 2226, 1455,  654, 2102,\n",
       "         1719, 1128, 1009, 1317,  488, 2022, 2329,  376, 1089,  986,  404,  404,\n",
       "         1072, 1072, 2170, 1072,  801,  801,  270,   10],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0, 1049,  127, 1880, 1031,\n",
       "         1031, 1031, 1492, 1492, 1923, 1488,  376, 1849, 1906, 1135,  334, 1782,\n",
       "         1399,  808,  689,  997,  168, 1702, 2009,   56,  769,  666,   84,   84,\n",
       "          752,  752, 1850,  752,  481,  481,    0, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0, 1700,  243,  243,  243,\n",
       "          243, 1211, 1211, 1056,   46, 1890,  621, 1284,  179, 1101,   14, 1664,\n",
       "          451, 1458,  190, 1466, 1964, 1536,   81, 1093, 1700,  243,  243,  243,\n",
       "          243,  243,  243,  243,  243,  243,    0, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0, 1626,  783, 1178, 1178,\n",
       "         1178, 1697, 1697, 1178, 1679, 1831, 1097,  219,    6, 1749, 1822, 1171,\n",
       "         1234, 1402,  391,  208, 1524, 1632, 1325, 1534, 1697, 1178, 1697, 1559,\n",
       "         1559, 1559, 1178,  783, 1178, 1178,    0, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,  546,  546,  290,  290,\n",
       "          142,  546,  546,  164, 1006, 1413, 1853, 1445,  730,  730, 1261,  632,\n",
       "          521, 1924,  448,  309,   26,  362, 2012,  254,  164,  546,  164,  546,\n",
       "          546,  546, 1348, 1348,  546,  546,    0, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,  306,  481, 1736, 1736,\n",
       "          481, 1736,  481,  267, 1838, 1049, 1360,  939, 1508,  204,  190, 1856,\n",
       "          439,  160, 1021, 1544, 1963,  935, 1419,  754,  351,  267, 1736, 1736,\n",
       "         1736, 1736,  267, 1335,  481, 1736,    0, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0, 1443,  555, 1572, 1030,\n",
       "         1572, 1572, 1030, 1572, 1338, 1663,  320, 1038,  431, 1604, 1188, 1279,\n",
       "         1758,  773,  193, 1656, 2020,  951, 1218, 1739, 1030, 1030, 1030, 1572,\n",
       "         1572, 1572, 1443, 1030, 1572, 1572,    0, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0, 1871,  666,  666,  666,\n",
       "         1978, 1978,  825,  825,  282, 1537, 1482,  820,  881,  361, 1919,  782,\n",
       "         1563, 1695, 1562,  839,  133,  711, 1019, 1003,  326,  976,  666, 1978,\n",
       "         1978, 1978, 1978,  976,  976,  976,    0, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0, 2008, 1648, 1744, 2008,\n",
       "         1648, 1648, 1648, 2008,  219,  819, 1143,   82, 1484, 1115, 1545,  269,\n",
       "         2045,  856, 1642,  138, 1768,  396,  877, 1712, 1665, 2008, 1744, 1744,\n",
       "         1744, 1744, 1744, 2008, 1744, 1744,    0, -100]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_row[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 149658/149658 [01:49<00:00, 1365.53 examples/s]\n",
      "Map: 100%|██████████| 5736/5736 [00:04<00:00, 1282.53 examples/s]\n",
      "Map: 100%|██████████| 4837/4837 [00:03<00:00, 1277.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT INCREASE batch size\n",
    "dataset = dataset.map(tts_tokenize_row, remove_columns=\"codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (9/9 shards): 100%|██████████| 149658/149658 [00:02<00:00, 60335.46 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 5736/5736 [00:00<00:00, 67734.06 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 4837/4837 [00:00<00:00, 61758.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(\"tokenized_libritts_bytes_kokoro_speaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWLINE_SEPARATOR = torch.tensor(tokenizer.encode(\"\\n\") + [0] * 8).unsqueeze(1)\n",
    "\n",
    "def batch_pack_sequences(examples, window_size=768, max_items=5):\n",
    "   \"\"\"\n",
    "   Pack sequences with system prompt and metrics\n",
    "   \"\"\"\n",
    "   packed_tokens = []\n",
    "   packed_labels = []\n",
    "   packed_speakers = []\n",
    "   pack_lengths = []\n",
    "   items_per_pack = []\n",
    "   \n",
    "   tokens = examples['tokens']\n",
    "   labels = examples['labels']\n",
    "   speakers = examples['speaker_id']\n",
    "   \n",
    "   # Account for system prompt in window size\n",
    "   effective_window = window_size - tts_sysprompt.shape[1]\n",
    "   \n",
    "   for i in range(len(tokens)):\n",
    "       seq_len = tokens[i].shape[1]\n",
    "       \n",
    "       # Start new pack\n",
    "       if i == 0 or current_length + seq_len > effective_window or \\\n",
    "          current_speaker != speakers[i] or current_items >= max_items:\n",
    "           \n",
    "           # Save previous pack if it exists\n",
    "           if i > 0 and current_tokens:\n",
    "               packed_tokens.append(torch.cat(current_tokens, dim=1))\n",
    "               packed_labels.append(torch.cat(current_labels, dim=1))\n",
    "               packed_speakers.append(current_speaker)\n",
    "               pack_lengths.append(current_length + tts_sysprompt.shape[1])\n",
    "               items_per_pack.append(current_items)\n",
    "           \n",
    "           # Initialize new pack with system prompt\n",
    "           current_tokens = [tts_sysprompt, tokens[i]]\n",
    "           current_labels = [tts_sysprompt, labels[i]]\n",
    "           current_speaker = speakers[i]\n",
    "           current_length = seq_len\n",
    "           current_items = 1\n",
    "           continue\n",
    "           \n",
    "       # Add to current pack with separator\n",
    "       current_tokens.extend([NEWLINE_SEPARATOR, tokens[i]])\n",
    "       current_labels.extend([NEWLINE_SEPARATOR, labels[i]])\n",
    "       current_length += seq_len + 1\n",
    "       current_items += 1\n",
    "   \n",
    "   # Don't forget last pack\n",
    "   if current_tokens:\n",
    "       packed_tokens.append(torch.cat(current_tokens, dim=1))\n",
    "       packed_labels.append(torch.cat(current_labels, dim=1))\n",
    "       packed_speakers.append(current_speaker)\n",
    "       pack_lengths.append(current_length + tts_sysprompt.shape[1])\n",
    "       items_per_pack.append(current_items)\n",
    "   \n",
    "   return {\n",
    "       'tokens': packed_tokens,\n",
    "       'labels': packed_labels,\n",
    "       'speaker_id': packed_speakers,\n",
    "       'pack_length': pack_lengths,\n",
    "       'items_in_pack': items_per_pack\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 149658/149658 [00:31<00:00, 4684.03 examples/s]\n",
      "Map: 100%|██████████| 5736/5736 [00:01<00:00, 4746.63 examples/s]\n",
      "Map: 100%|██████████| 4837/4837 [00:01<00:00, 4760.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Usage:\n",
    "packed_dataset = dataset.map(\n",
    "    lambda row: batch_pack_sequences(row, max_items=3),\n",
    "    batched=True,\n",
    "    remove_columns=dataset['val'].column_names,\n",
    "    batch_size=1000  # Adjust based on memory constraints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nSpeak out the provided text<|im_end|>\\n<|im_start|>assistant\\n<|im_start|>user\\nThe weapon must still have been there.<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1049|><|semantic:1114|><|semantic:1609|><|semantic:784|><|semantic:499|><|semantic:260|><|semantic:1011|><|semantic:8|><|semantic:1407|><|semantic:540|><|semantic:1615|><|semantic:561|><|semantic:1945|><|semantic:201|><|semantic:1324|><|semantic:668|><|semantic:376|><|semantic:1849|><|semantic:9|><|semantic:1921|><|semantic:1921|><|semantic:1683|><|semantic:228|><|semantic:897|><|semantic:1677|><|semantic:518|><|im_end|>\\n<|im_start|>user\\nHow quickly he disappeared!\"<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1698|><|semantic:1848|><|semantic:1021|><|semantic:414|><|semantic:972|><|semantic:1252|><|semantic:1545|><|semantic:1363|><|semantic:307|><|semantic:722|><|semantic:1169|><|semantic:170|><|semantic:1701|><|semantic:1967|><|semantic:886|><|semantic:1540|><|semantic:1540|><|semantic:1113|><|semantic:902|><|semantic:1655|><|semantic:2009|><|semantic:56|><|semantic:1640|><|im_end|>\\n<|im_start|>user\\n\"But the blood?<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1049|><|semantic:503|><|semantic:1249|><|semantic:1039|><|semantic:1066|><|semantic:1522|><|semantic:680|><|semantic:1344|><|semantic:145|><|semantic:145|><|semantic:56|><|semantic:857|><|im_end|>\\n<|im_start|>user\\n\"So they tell me.\"<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1049|><|semantic:738|><|semantic:1669|><|semantic:1627|><|semantic:1144|><|semantic:578|><|semantic:847|><|semantic:184|><|semantic:1348|><|semantic:779|><|semantic:1715|><|semantic:658|><|semantic:141|><|semantic:844|><|im_end|>\\n<|im_start|>user\\nWhat do you make of it, Gryce?\"<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1415|><|semantic:167|><|semantic:1865|><|semantic:557|><|semantic:1000|><|semantic:49|><|semantic:1730|><|semantic:512|><|semantic:510|><|semantic:483|><|semantic:1352|><|semantic:1039|><|semantic:579|><|semantic:489|><|semantic:863|><|semantic:1388|><|semantic:874|><|semantic:632|><|semantic:527|><|semantic:1472|><|semantic:1602|><|semantic:1940|><|semantic:489|><|semantic:1331|><|im_end|>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_row = packed_dataset['val'][0]\n",
    "tokenizer.decode(example_row[\"tokens\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (5/5 shards): 100%|██████████| 50735/50735 [00:01<00:00, 33495.02 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1937/1937 [00:00<00:00, 33569.13 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1649/1649 [00:00<00:00, 31396.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "packed_dataset.save_to_disk(\"tokenized_libritts_packed_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system\\nSpeak out the provided text<|im_end|>\\n<|im_start|>user\\nI felt it in my bones when I woke this morning that something splendid was going to turn up.<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1049|><|semantic:1268|><|semantic:549|><|semantic:1324|><|semantic:668|><|semantic:1538|><|semantic:1593|><|semantic:95|><|semantic:629|><|semantic:1281|><|semantic:1281|><|semantic:680|><|semantic:536|><|semantic:536|><|semantic:230|><|semantic:1018|><|semantic:1117|><|semantic:244|><|semantic:507|><|semantic:997|><|semantic:1399|><|semantic:640|><|semantic:1591|><|semantic:1967|><|semantic:1161|><|semantic:690|><|semantic:67|><|semantic:1772|><|semantic:830|><|semantic:1612|><|semantic:561|><|semantic:119|><|semantic:1052|><|semantic:880|><|semantic:1029|><|semantic:1532|><|semantic:1161|><|semantic:1344|><|semantic:1109|><|semantic:6|><|semantic:1001|><|semantic:382|><|semantic:596|><|semantic:99|><|semantic:1726|><|semantic:2030|><|semantic:531|><|semantic:616|><|semantic:367|><|semantic:1271|><|semantic:1868|><|semantic:978|><|semantic:729|><|semantic:396|><|semantic:1544|><|im_end|>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example_row[0][\"labels\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system\\nTranscribe the provided speech<|im_end|>\\n<|im_start|>user\\n<|semantic:1049|><|semantic:1268|><|semantic:549|><|semantic:1324|><|semantic:668|><|semantic:1538|><|semantic:1593|><|semantic:95|><|semantic:629|><|semantic:1281|><|semantic:1281|><|semantic:680|><|semantic:536|><|semantic:536|><|semantic:230|><|semantic:1018|><|semantic:1117|><|semantic:244|><|semantic:507|><|semantic:997|><|semantic:1399|><|semantic:640|><|semantic:1591|><|semantic:1967|><|semantic:1161|><|semantic:690|><|semantic:67|><|semantic:1772|><|semantic:830|><|semantic:1612|><|semantic:561|><|semantic:119|><|semantic:1052|><|semantic:880|><|semantic:1029|><|semantic:1532|><|semantic:1161|><|semantic:1344|><|semantic:1109|><|semantic:6|><|semantic:1001|><|semantic:382|><|semantic:596|><|semantic:99|><|semantic:1726|><|semantic:2030|><|semantic:531|><|semantic:616|><|semantic:367|><|semantic:1271|><|semantic:1868|><|semantic:978|><|semantic:729|><|semantic:396|><|semantic:1544|><|im_end|>\\n<|im_start|>assistant\\nI felt it in my bones when I woke this morning that something splendid was going to turn up.<|im_end|>\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example_row[1][\"labels\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0, 1698, 1719,  204, 1389,  851, 1772,  186, 1307, 1895,  832,\n",
       "        1633,  771,  648, 1530, 1989, 1574, 1348,  722,  144, 1945,  278, 1109,\n",
       "          29,  611,   46,  622,  628, 1740,  572,  572,  345, 1989, 1676,  929,\n",
       "        1776,  749,  313, 1997, 1571,  819, 1238, 1054, 1054, 1135, 1506, 1393,\n",
       "         616, 1702,  993,  579,  486,  486, 2039,  148,  657,  664,  339,  339,\n",
       "         588,  212, 1443,   32, 1320, 1549,  440,    8, 1407, 1722, 1650, 1615,\n",
       "         798,  121,  303,  697,  837,  358, 1882,  440, 1992, 1992,  587,  178,\n",
       "         178, 1627, 1530,  929, 1610, 1916,  523,  213, 1252, 1480, 1468, 1899,\n",
       "         773, 2033, 2033,   83, 1146,  784, 1295,  199, 1109,  268,    6,    6,\n",
       "        1781, 1479, 1530, 1530,  146, 2038,  984, 1403,  606, 1379, 1840, 1172,\n",
       "        1680, 1162, 1928])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_row[\"tokens\"][0][1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, 1049, 1268,  549, 1324,  668, 1538, 1593,   95,  629, 1281, 1281,\n",
       "         680,  536,  536,  230, 1018, 1117,  244,  507,  997, 1399,  640, 1591,\n",
       "        1967, 1161,  690,   67, 1772,  830, 1612,  561,  119, 1052,  880, 1029,\n",
       "        1532, 1161, 1344, 1109,    6, 1001,  382,  596,   99, 1726, 2030,  531,\n",
       "         616,  367, 1271, 1868,  978,  729,  396, 1544,    0, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_row[1][\"labels\"][1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'normalized_text': 'I felt it in my bones when I woke this morning that something splendid was going to turn up.',\n",
       " 'speaker_id': '4446',\n",
       " 'id': '4446_2275_000002_000009',\n",
       " 'tokens': tensor([[    1,  9690,   198, 15024,   494,   578,   260,  2711,  1694,     2,\n",
       "            198,     1,  4093,   198,    57,  4592,   357,   281,   957,  6542,\n",
       "            645,   339, 40652,   451,  5738,   338,  1488, 33494,   436,  2045,\n",
       "            288,  1607,   614,    30,     2,   198,     1,   520,  9531,   198,\n",
       "          50201, 50420, 49701, 50476, 49820, 50690, 50745, 49247, 49781, 50433,\n",
       "          50433, 49832, 49688, 49688, 49382, 50170, 50269, 49396, 49659, 50149,\n",
       "          50551, 49792, 50743, 51119, 50313, 49842, 49219, 50924, 49982, 50764,\n",
       "          49713, 49271, 50204, 50032, 50181, 50684, 50313, 50496, 50261, 49158,\n",
       "          50153, 49534, 49748, 49251, 50878, 51182, 49683, 49768, 49519, 50423,\n",
       "          51020, 50130, 49881, 49548, 50696],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1049,  1268,   549,  1324,   668,  1538,  1593,    95,   629,  1281,\n",
       "           1281,   680,   536,   536,   230,  1018,  1117,   244,   507,   997,\n",
       "           1399,   640,  1591,  1967,  1161,   690,    67,  1772,   830,  1612,\n",
       "            561,   119,  1052,   880,  1029,  1532,  1161,  1344,  1109,     6,\n",
       "           1001,   382,   596,    99,  1726,  2030,   531,   616,   367,  1271,\n",
       "           1868,   978,   729,   396,  1544],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1470,  1879,   712,   283,   220,   137,  1610,   263,   531,  1845,\n",
       "           1428,  1132,   359,  1904,  1458,  1876,   895,   149,   190,   116,\n",
       "            603,   786,  1884,  1455,  1928,   677,   914,  1122,   436,   618,\n",
       "            850,  1766,  2005,  1618,   966,   850,  1663,   172,   274,   612,\n",
       "           1013,  1928,  1262,  1169,  1006,  1777,  1755,  2026,  1714,   788,\n",
       "            786,  1520,   811,    91,  1700],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            373,   602,  2016,  1148,    98,   790,  1570,   944,  1872,   807,\n",
       "           1427,   817,   602,  1090,   341,  1609,   774,   327,   479,   229,\n",
       "           1988,  1596,  1595,   339,  1801,  1360,  1415,   492,     3,  1064,\n",
       "            378,  1530,   800,  1369,  1331,   782,  1822,   511,   349,  1534,\n",
       "           1245,  1788,   146,  1195,  1510,   584,  1782,   662,  1523,  1523,\n",
       "             68,  2016,  1240,   783,  1178],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1225,  1475,    17,   148,   656,  2036,   663,   632,  1926,   358,\n",
       "           1443,    33,  1316,  1993,  1038,   324,  1240,   861,   787,  1994,\n",
       "           1721,   832,   707,   435,   113,   138,   494,   920,  1707,  1134,\n",
       "           1003,   842,  1511,   193,  1936,  1478,  1261,   332,   756,   115,\n",
       "           1700,   324,  1074,  1631,  1575,   526,  1957,   756,   318,    19,\n",
       "            860,  1562,   164,  1477,  1450],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            457,  2035,  1113,   504,  1301,     5,   739,   334,   178,  1832,\n",
       "           1803,   266,  1719,   516,   147,  1016,  1402,   709,  1046,  1620,\n",
       "            440,   996,   100,   551,  1799,  1930,  1811,  1551,  1026,  1440,\n",
       "            684,   313,    13,   629,  1930,   723,   457,   328,  1455,   796,\n",
       "            300,  1610,   435,   635,  1517,   958,  1607,  1013,   911,   958,\n",
       "            524,   457,  1019,  1019,   481],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1547,   592,   601,  2022,  1795,    42,  1631,  1174,   496,  1640,\n",
       "            935,  1687,   654,  1105,   668,  1995,   918,  1002,  1668,   190,\n",
       "           1148,  1509,  1563,    30,  1669,   882,  1529,  1794,  1086,   900,\n",
       "            274,  1001,   482,    42,  1852,   780,   704,   275,  1073,  1174,\n",
       "           1618,  1540,  1344,  1371,   263,  1245,  1382,   253,   484,  2044,\n",
       "           1717,  1942,   165,  1832,   478],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1433,  1154,  1672,   879,  1683,  1529,   561,   508,  1648,   358,\n",
       "            646,  1289,  1307,  1867,  1937,  1076,  1694,  1730,  1199,  1758,\n",
       "           1476,   369,   801,  1249,   260,  1759,   740,    53,   618,  2023,\n",
       "            119,   835,  2015,   835,    66,  1209,  1285,  1744,   804,  1863,\n",
       "           1240,   392,   741,  1834,  1193,  1440,  1696,  1470,  1810,  1464,\n",
       "            840,    43,  1978,   666,  1978],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            948,   232,  1109,  1286,  1371,    94,  1516,   700,   714,   537,\n",
       "           1939,   334,    17,  1093,  1769,  1511,  1063,   444,  1399,  1594,\n",
       "             10,   532,   208,  1803,   945,  1193,  1082,  1667,   198,   382,\n",
       "            101,  1426,   458,  1587,  1450,  1722,  1173,  1286,   127,   225,\n",
       "           1482,  1368,   370,  1003,  1808,  1772,   952,   992,  1977,   820,\n",
       "            481,   945,   945,  1477,  1665]]),\n",
       " 'labels': tensor([[ 9690,   198, 15024,   494,   578,   260,  2711,  1694,     2,   198,\n",
       "              1,  4093,   198,    57,  4592,   357,   281,   957,  6542,   645,\n",
       "            339, 40652,   451,  5738,   338,  1488, 33494,   436,  2045,   288,\n",
       "           1607,   614,    30,     2,   198,     1,   520,  9531,   198, 50201,\n",
       "          50420, 49701, 50476, 49820, 50690, 50745, 49247, 49781, 50433, 50433,\n",
       "          49832, 49688, 49688, 49382, 50170, 50269, 49396, 49659, 50149, 50551,\n",
       "          49792, 50743, 51119, 50313, 49842, 49219, 50924, 49982, 50764, 49713,\n",
       "          49271, 50204, 50032, 50181, 50684, 50313, 50496, 50261, 49158, 50153,\n",
       "          49534, 49748, 49251, 50878, 51182, 49683, 49768, 49519, 50423, 51020,\n",
       "          50130, 49881, 49548, 50696,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1049,\n",
       "           1268,   549,  1324,   668,  1538,  1593,    95,   629,  1281,  1281,\n",
       "            680,   536,   536,   230,  1018,  1117,   244,   507,   997,  1399,\n",
       "            640,  1591,  1967,  1161,   690,    67,  1772,   830,  1612,   561,\n",
       "            119,  1052,   880,  1029,  1532,  1161,  1344,  1109,     6,  1001,\n",
       "            382,   596,    99,  1726,  2030,   531,   616,   367,  1271,  1868,\n",
       "            978,   729,   396,  1544,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1470,\n",
       "           1879,   712,   283,   220,   137,  1610,   263,   531,  1845,  1428,\n",
       "           1132,   359,  1904,  1458,  1876,   895,   149,   190,   116,   603,\n",
       "            786,  1884,  1455,  1928,   677,   914,  1122,   436,   618,   850,\n",
       "           1766,  2005,  1618,   966,   850,  1663,   172,   274,   612,  1013,\n",
       "           1928,  1262,  1169,  1006,  1777,  1755,  2026,  1714,   788,   786,\n",
       "           1520,   811,    91,  1700,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   373,\n",
       "            602,  2016,  1148,    98,   790,  1570,   944,  1872,   807,  1427,\n",
       "            817,   602,  1090,   341,  1609,   774,   327,   479,   229,  1988,\n",
       "           1596,  1595,   339,  1801,  1360,  1415,   492,     3,  1064,   378,\n",
       "           1530,   800,  1369,  1331,   782,  1822,   511,   349,  1534,  1245,\n",
       "           1788,   146,  1195,  1510,   584,  1782,   662,  1523,  1523,    68,\n",
       "           2016,  1240,   783,  1178,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1225,\n",
       "           1475,    17,   148,   656,  2036,   663,   632,  1926,   358,  1443,\n",
       "             33,  1316,  1993,  1038,   324,  1240,   861,   787,  1994,  1721,\n",
       "            832,   707,   435,   113,   138,   494,   920,  1707,  1134,  1003,\n",
       "            842,  1511,   193,  1936,  1478,  1261,   332,   756,   115,  1700,\n",
       "            324,  1074,  1631,  1575,   526,  1957,   756,   318,    19,   860,\n",
       "           1562,   164,  1477,  1450,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   457,\n",
       "           2035,  1113,   504,  1301,     5,   739,   334,   178,  1832,  1803,\n",
       "            266,  1719,   516,   147,  1016,  1402,   709,  1046,  1620,   440,\n",
       "            996,   100,   551,  1799,  1930,  1811,  1551,  1026,  1440,   684,\n",
       "            313,    13,   629,  1930,   723,   457,   328,  1455,   796,   300,\n",
       "           1610,   435,   635,  1517,   958,  1607,  1013,   911,   958,   524,\n",
       "            457,  1019,  1019,   481,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1547,\n",
       "            592,   601,  2022,  1795,    42,  1631,  1174,   496,  1640,   935,\n",
       "           1687,   654,  1105,   668,  1995,   918,  1002,  1668,   190,  1148,\n",
       "           1509,  1563,    30,  1669,   882,  1529,  1794,  1086,   900,   274,\n",
       "           1001,   482,    42,  1852,   780,   704,   275,  1073,  1174,  1618,\n",
       "           1540,  1344,  1371,   263,  1245,  1382,   253,   484,  2044,  1717,\n",
       "           1942,   165,  1832,   478,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1433,\n",
       "           1154,  1672,   879,  1683,  1529,   561,   508,  1648,   358,   646,\n",
       "           1289,  1307,  1867,  1937,  1076,  1694,  1730,  1199,  1758,  1476,\n",
       "            369,   801,  1249,   260,  1759,   740,    53,   618,  2023,   119,\n",
       "            835,  2015,   835,    66,  1209,  1285,  1744,   804,  1863,  1240,\n",
       "            392,   741,  1834,  1193,  1440,  1696,  1470,  1810,  1464,   840,\n",
       "             43,  1978,   666,  1978,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   948,\n",
       "            232,  1109,  1286,  1371,    94,  1516,   700,   714,   537,  1939,\n",
       "            334,    17,  1093,  1769,  1511,  1063,   444,  1399,  1594,    10,\n",
       "            532,   208,  1803,   945,  1193,  1082,  1667,   198,   382,   101,\n",
       "           1426,   458,  1587,  1450,  1722,  1173,  1286,   127,   225,  1482,\n",
       "           1368,   370,  1003,  1808,  1772,   952,   992,  1977,   820,   481,\n",
       "            945,   945,  1477,  1665,     0]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
