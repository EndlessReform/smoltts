{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize LibriTTS-R Mimi for target LM\n",
    "\n",
    "For our dataset, we currently simply use the Fish Speech TTS format:\n",
    "- Text-only data formatted using [ChatML](https://gist.github.com/edwardzjl/8df07c1f7140c9a3e2f48d33a8032090) as a separate sequence \"above\" the audio code stream\n",
    "- During sections where audio is being modeled, text stream 0 predicts the first semantic token index $n$ of the 8 Mimi residual codes as special token `<|semantic:n|>`\n",
    "- For audio, \"semantic\" (neural, there's not a strong distinction between) codes (from Mimi) padded with 0s during text sections\n",
    "\n",
    "It's possible this tokenization strategy can be improved, e.g. in [Defossez et al. 2024](https://arxiv.org/html/2410.00037v2#S3.SS4.SSS4) with the base transformer predicting the Whisper-timestamped word timings as an \"inner monologue\" and a delay between codebook timesteps. lol i'll do it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, concatenate_datasets, load_from_disk\n",
    "\n",
    "# If creating the libritts dataset for the first time\n",
    "# from datasets import load_from_disk \n",
    "# dataset = load_from_disk(\"encoded_dataset\")\n",
    "# train_clean_100 = load_from_disk(\"encoded_libritts/train.clean.100/\")\n",
    "# train_clean_360 = load_from_disk(\"encoded_libritts/train.clean.360/\")\n",
    "# dev_clean = load_from_disk(\"encoded_libritts/dev.clean\")\n",
    "# test_clean = load_from_disk(\"encoded_libritts/test.clean\")\n",
    "# full_train = concatenate_datasets([train_clean_100, train_clean_360])\n",
    "# dataset = load_dataset(\"jkeisling/libritts-r-mimi-kokoro\")\n",
    "# dataset = load_from_disk(\"../../Kokoro-82M/libritts_r_mimi_kokoro\")\n",
    "dataset = load_from_disk(\"../../Kokoro-82M/project_gutenberg_mimi_kokoro\")\n",
    "# full_train = concatenate_datasets([dataset[\"train.clean.100\"], dataset[\"train.clean.360\"]])\n",
    "\n",
    "# dataset = DatasetDict({\n",
    "#     \"train\": full_train,\n",
    "#     \"val\": dataset[\"dev.clean\"],\n",
    "#     \"test\": dataset[\"test.clean\"]\n",
    "# })\n",
    "dataset = DatasetDict({\"full\": dataset})\n",
    "dataset = dataset.with_format(\"torch\")\n",
    "# dataset = dataset.remove_columns([\"chapter_id\", \"text_original\"])\n",
    "# dataset = dataset.rename_column(original_column_name=\"text_normalized\", new_column_name=\"normalized_text\")\n",
    "dataset = dataset.rename_column(original_column_name=\"sentences\", new_column_name=\"normalized_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE! This is PATH DEPENDENT on ADDING THE SEMANTIC TOKENS TO THE TOKENIZER EARLIER using `create_bytelevel_init.ipynb`. DO NOT SKIP THIS STEP OR THE MODEL WILL BE IRRETRIEVABLY BROKEN! YOU HAVE BEEN WARNED.**\n",
    "\n",
    "==**THIS IS BYTE LEVEL!**=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"../inits/smoltts_byte_kokoro\")\n",
    "tokenizer.use_default_system_prompt = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check this carefully: for byte level, it should be 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2368, 256)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer), tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please manually verify the text is done correctly. However, DECODE will not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: tensor([[269, 256,  10, 260, 261, 270]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n<|american|><|male|><|im_end|>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the tokenizer by encoding and decoding some example text\n",
    "example_text = \"<|im_start|>system\\n<|american|><|male|><|im_end|>\"\n",
    "encoded = tokenizer(example_text, return_tensors=\"pt\")\n",
    "print(f\"Encoded: {encoded['input_ids']}\")\n",
    "decoded = tokenizer.decode(encoded['input_ids'][0])\n",
    "\n",
    "# Print the results\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[269, 257,  10, 104, 101, 108, 112,  32, 109, 101,  32, 105,  32,  97,\n",
       "         109,  32, 116, 114,  97, 112, 112, 101, 100,  32, 105, 110,  32, 116,\n",
       "         104, 105, 115,  32,  99, 111, 109, 112, 117, 116, 101, 114, 270,  10,\n",
       "         269, 258,  10]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": \"help me i am trapped in this computer\"}], add_generation_prompt=True,  return_tensors=\"pt\")\n",
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>user\\nhelp me i am trapped in this computer<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(sequence[0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n<|speaker:40|><|im_end|>\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def encode_text(role: str, content: str, add_generation_prompt: bool = True) -> torch.Tensor:\n",
    "    # baseline = tokenizer.apply_chat_template(f\"{chr(10) if ''}<|im_start|>{role}\\n{content}<|im_end|>\\n\",)\n",
    "    baseline = tokenizer.apply_chat_template(\n",
    "        [{\"role\": role, \"content\": content}],\n",
    "        add_generation_prompt=add_generation_prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    zeros_mask = torch.zeros(8, baseline.size(1), dtype=baseline.dtype)\n",
    "    return torch.cat([baseline, zeros_mask])\n",
    "\n",
    "tts_sysprompt = encode_text(\"system\", \"<|speaker:40|>\", add_generation_prompt=False)\n",
    "tokenizer.decode(tts_sysprompt[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this assumes you're using ChatML. if you're NOT, then there's quite a bit more to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|semantic:1049|><|semantic:127|><|semantic:1880|><|semantic:1031|><|semantic:1156|><|semantic:1268|><|semantic:1001|><|semantic:382|><|semantic:587|><|semantic:178|><|semantic:1380|><|semantic:1380|><|semantic:1790|><|semantic:1803|><|semantic:125|><|semantic:130|><|semantic:531|><|semantic:1825|><|semantic:722|><|semantic:539|><|semantic:526|><|semantic:632|><|semantic:728|><|semantic:735|><|semantic:774|><|semantic:518|><|semantic:502|><|semantic:666|><|semantic:448|><|semantic:84|><|semantic:752|><|semantic:752|><|im_end|>\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEMANTIC_OFFSET = tokenizer.encode(\"<|semantic:0|>\")[0]\n",
    "# B * C+1 * 2\n",
    "VQ_USER_PREFIX = encode_text(role=\"user\", content=\"\")[:,:-2]\n",
    "TRAILING_IM_END = torch.tensor([\n",
    "    tokenizer.encode(\"<|im_end|>\") + [0] * 8,\n",
    "    tokenizer.encode(\"\\n\") + [0] * 8,\n",
    "]).T\n",
    "\n",
    "def encode_vq(codes: torch.Tensor, is_assistant=True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Expects C * T\n",
    "    \"\"\"\n",
    "    if codes.ndim != 2:\n",
    "        raise ValueError(\"Must be single batch\")\n",
    "    speaker_line = codes[0,:] + SEMANTIC_OFFSET\n",
    "    vq_block = torch.cat([speaker_line.unsqueeze(0), codes])\n",
    "\n",
    "    block = torch.cat([vq_block, TRAILING_IM_END], dim=1)\n",
    "    return block if is_assistant else torch.cat([VQ_USER_PREFIX, block], dim=1)\n",
    "\n",
    "\n",
    "out = encode_vq(dataset[\"full\"][0][\"codes\"], is_assistant=True)\n",
    "tokenizer.decode(out[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_names = [\"default\", \"sarah\", \"sky\", \"adam\", \"emma\", \"isabella\", \"george\", \"lewis\"]\n",
    "speaker_ids = {value: index for index, value in enumerate(speaker_names)}\n",
    "speaker_ids[\"adam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n<|american|><|female|><|speaker:2|><|im_end|>\\n<|im_start|>assistant\\n<|im_start|>user\\nUncle Roland was gone.<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1049|><|semantic:127|><|semantic:1880|><|semantic:1031|><|semantic:1031|><|semantic:1272|><|semantic:713|><|semantic:1252|><|semantic:1823|><|semantic:526|><|semantic:1260|><|semantic:1998|><|semantic:213|><|semantic:544|><|semantic:382|><|semantic:1238|><|semantic:160|><|semantic:1597|><|semantic:1663|><|semantic:1680|><|semantic:226|><|semantic:709|><|semantic:666|><|semantic:716|><|semantic:84|><|semantic:84|><|semantic:752|><|im_end|>'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "import random\n",
    "\n",
    "# TODO: Not doing ASR for now\n",
    "def tts_tokenize_row(row: Dict):\n",
    "    \"\"\"\n",
    "    NOTE: Deliberately ignores sysprompt line for now, can be done in packing\n",
    "    \"\"\"\n",
    "    # TODO: Fix this upstream in the data gen!\n",
    "    gender = \"<|male|>\" if row[\"speaker_id\"] in [\"george\", \"lewis\", \"adam\", \"michael\"] else \"<|female|>\"\n",
    "    accent = f\"<|{row['accent']}|>\"\n",
    "    speaker = f\"<|speaker:{speaker_ids[row['speaker_id']]}|>\" if random.random() < 0.7 else \"\"\n",
    "\n",
    "    # Just keep it all for now, will test generalization later\n",
    "    system_line = encode_text(role=\"system\", content=\"\".join([accent, gender, speaker]))\n",
    "    user_line = encode_text(\n",
    "        role=\"user\", \n",
    "        content=row[\"normalized_text\"].encode(\"utf-8\").decode(\"latin-1\"), \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    assistant_line = encode_vq(row[\"codes\"])\n",
    "    ground_truth = torch.cat([system_line, user_line, assistant_line], dim=1)\n",
    "    # Causal shift\n",
    "    tokens = ground_truth[:,:-1].clone()\n",
    "    labels = ground_truth[:,1:].clone()\n",
    "\n",
    "    # Assuming user line took care of assistant prefix \n",
    "    # Offsetting by 1 since labels were shifted\n",
    "    text_only_length = system_line.size(1) + user_line.size(1) - 1\n",
    "    labels[1:, :text_only_length] = -100\n",
    "    # Mask out im_end and newline\n",
    "    labels[1:, -2:] = -100\n",
    "\n",
    "    return({\n",
    "        \"tokens\": tokens,\n",
    "        \"labels\": labels\n",
    "    })\n",
    "    \n",
    "\n",
    "\n",
    "example_row = tts_tokenize_row(dataset[\"full\"][10])\n",
    "tokenizer.decode(example_row[\"tokens\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': tensor([[ 269,  256,   10,  260,  262,  273,  270,   10,  269,  258,   10,  269,\n",
       "           257,   10,   85,  110,   99,  108,  101,   32,   82,  111,  108,   97,\n",
       "           110,  100,   32,  119,   97,  115,   32,  103,  111,  110,  101,   46,\n",
       "           270,   10,  269,  258,   10, 1369,  447, 2200, 1351, 1351, 1592, 1033,\n",
       "          1572, 2143,  846, 1580, 2318,  533,  864,  702, 1558,  480, 1917, 1983,\n",
       "          2000,  546, 1029,  986, 1036,  404,  404, 1072,  270],\n",
       "         [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0, 1049,  127, 1880, 1031, 1031, 1272,  713,\n",
       "          1252, 1823,  526, 1260, 1998,  213,  544,  382, 1238,  160, 1597, 1663,\n",
       "          1680,  226,  709,  666,  716,   84,   84,  752,    0],\n",
       "         [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0, 1700,  243,  243,  243, 1211,  861, 1428,\n",
       "           950, 1377, 1132, 1739,  164, 1457,  691, 1366, 1159, 1736, 2042, 1673,\n",
       "          1769,  520, 1700,  243,  243,  243,  243,  243,    0],\n",
       "         [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0, 1626,  783, 1559, 1559, 1697, 1404,  800,\n",
       "          1698, 1275,  288, 2027, 1070,  404,  284,  487, 1501,  546,  534, 1649,\n",
       "           138, 1335, 1559, 1697, 1559, 1697,  783,  783,    0],\n",
       "         [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,  546,  546,  164,  164,  546, 2043, 1275,\n",
       "          1481, 1809,  579,  124,  110,  134,  738, 1605, 1918, 1890, 1702, 1704,\n",
       "           795,  231, 1348,  164,  546,  164,  290, 1348,    0],\n",
       "         [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,  306,  481, 1736,  267,  481,  670, 1707,\n",
       "          1655,  679, 1748,  729,   72,  748,  981, 1972, 1311, 1552, 2042,  342,\n",
       "          2028, 1412,  481,  481,  481, 1736, 1736, 1335,    0],\n",
       "         [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0, 1443,  555, 1572, 1030, 1030, 1949, 1544,\n",
       "          1644, 1901,    8, 1789,  928,  795, 1376, 1622,  347,  429,  459,  217,\n",
       "           500, 1335,  555,  555, 1572, 1030, 1572, 1030,    0],\n",
       "         [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0, 1871,  666,  666,  825,  666,  921,  530,\n",
       "           115,  496, 1444, 1620,   96,  962, 1841, 1464,  812, 1846,  359,  968,\n",
       "           741, 1016,  825,  976,  825,  666,  976,  976,    0],\n",
       "         [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0, 2008, 1648, 1744, 2008, 2008,  142, 2022,\n",
       "          1617, 1743, 1742, 1953,   36,  644,  910, 1963,  820,  893,  676, 1158,\n",
       "           981, 1829, 1180, 1648, 1744, 1744, 1744, 2008,    0]]),\n",
       " 'labels': tensor([[ 256,   10,  260,  262,  273,  270,   10,  269,  258,   10,  269,  257,\n",
       "            10,   85,  110,   99,  108,  101,   32,   82,  111,  108,   97,  110,\n",
       "           100,   32,  119,   97,  115,   32,  103,  111,  110,  101,   46,  270,\n",
       "            10,  269,  258,   10, 1369,  447, 2200, 1351, 1351, 1592, 1033, 1572,\n",
       "          2143,  846, 1580, 2318,  533,  864,  702, 1558,  480, 1917, 1983, 2000,\n",
       "           546, 1029,  986, 1036,  404,  404, 1072,  270,   10],\n",
       "         [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, 1049,  127, 1880, 1031, 1031, 1272,  713, 1252,\n",
       "          1823,  526, 1260, 1998,  213,  544,  382, 1238,  160, 1597, 1663, 1680,\n",
       "           226,  709,  666,  716,   84,   84,  752, -100, -100],\n",
       "         [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, 1700,  243,  243,  243, 1211,  861, 1428,  950,\n",
       "          1377, 1132, 1739,  164, 1457,  691, 1366, 1159, 1736, 2042, 1673, 1769,\n",
       "           520, 1700,  243,  243,  243,  243,  243, -100, -100],\n",
       "         [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, 1626,  783, 1559, 1559, 1697, 1404,  800, 1698,\n",
       "          1275,  288, 2027, 1070,  404,  284,  487, 1501,  546,  534, 1649,  138,\n",
       "          1335, 1559, 1697, 1559, 1697,  783,  783, -100, -100],\n",
       "         [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100,  546,  546,  164,  164,  546, 2043, 1275, 1481,\n",
       "          1809,  579,  124,  110,  134,  738, 1605, 1918, 1890, 1702, 1704,  795,\n",
       "           231, 1348,  164,  546,  164,  290, 1348, -100, -100],\n",
       "         [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100,  306,  481, 1736,  267,  481,  670, 1707, 1655,\n",
       "           679, 1748,  729,   72,  748,  981, 1972, 1311, 1552, 2042,  342, 2028,\n",
       "          1412,  481,  481,  481, 1736, 1736, 1335, -100, -100],\n",
       "         [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, 1443,  555, 1572, 1030, 1030, 1949, 1544, 1644,\n",
       "          1901,    8, 1789,  928,  795, 1376, 1622,  347,  429,  459,  217,  500,\n",
       "          1335,  555,  555, 1572, 1030, 1572, 1030, -100, -100],\n",
       "         [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, 1871,  666,  666,  825,  666,  921,  530,  115,\n",
       "           496, 1444, 1620,   96,  962, 1841, 1464,  812, 1846,  359,  968,  741,\n",
       "          1016,  825,  976,  825,  666,  976,  976, -100, -100],\n",
       "         [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, 2008, 1648, 1744, 2008, 2008,  142, 2022, 1617,\n",
       "          1743, 1742, 1953,   36,  644,  910, 1963,  820,  893,  676, 1158,  981,\n",
       "          1829, 1180, 1648, 1744, 1744, 1744, 2008, -100, -100]])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 256,   10,  260,  262,  273,  270,   10,  269,  258,   10,  269,  257,\n",
       "           10,   85,  110,   99,  108,  101,   32,   82,  111,  108,   97,  110,\n",
       "          100,   32,  119,   97,  115,   32,  103,  111,  110,  101,   46,  270,\n",
       "           10,  269,  258,   10, 1369,  447, 2200, 1351, 1351, 1592, 1033, 1572,\n",
       "         2143,  846, 1580, 2318,  533,  864,  702, 1558,  480, 1917, 1983, 2000,\n",
       "          546, 1029,  986, 1036,  404,  404, 1072,  270,   10],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, 1049,  127, 1880, 1031, 1031, 1272,  713, 1252,\n",
       "         1823,  526, 1260, 1998,  213,  544,  382, 1238,  160, 1597, 1663, 1680,\n",
       "          226,  709,  666,  716,   84,   84,  752, -100, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, 1700,  243,  243,  243, 1211,  861, 1428,  950,\n",
       "         1377, 1132, 1739,  164, 1457,  691, 1366, 1159, 1736, 2042, 1673, 1769,\n",
       "          520, 1700,  243,  243,  243,  243,  243, -100, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, 1626,  783, 1559, 1559, 1697, 1404,  800, 1698,\n",
       "         1275,  288, 2027, 1070,  404,  284,  487, 1501,  546,  534, 1649,  138,\n",
       "         1335, 1559, 1697, 1559, 1697,  783,  783, -100, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100,  546,  546,  164,  164,  546, 2043, 1275, 1481,\n",
       "         1809,  579,  124,  110,  134,  738, 1605, 1918, 1890, 1702, 1704,  795,\n",
       "          231, 1348,  164,  546,  164,  290, 1348, -100, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100,  306,  481, 1736,  267,  481,  670, 1707, 1655,\n",
       "          679, 1748,  729,   72,  748,  981, 1972, 1311, 1552, 2042,  342, 2028,\n",
       "         1412,  481,  481,  481, 1736, 1736, 1335, -100, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, 1443,  555, 1572, 1030, 1030, 1949, 1544, 1644,\n",
       "         1901,    8, 1789,  928,  795, 1376, 1622,  347,  429,  459,  217,  500,\n",
       "         1335,  555,  555, 1572, 1030, 1572, 1030, -100, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, 1871,  666,  666,  825,  666,  921,  530,  115,\n",
       "          496, 1444, 1620,   96,  962, 1841, 1464,  812, 1846,  359,  968,  741,\n",
       "         1016,  825,  976,  825,  666,  976,  976, -100, -100],\n",
       "        [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, 2008, 1648, 1744, 2008, 2008,  142, 2022, 1617,\n",
       "         1743, 1742, 1953,   36,  644,  910, 1963,  820,  893,  676, 1158,  981,\n",
       "         1829, 1180, 1648, 1744, 1744, 1744, 2008, -100, -100]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_row[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 576820/576820 [06:53<00:00, 1394.31 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT INCREASE batch size\n",
    "dataset = dataset.map(tts_tokenize_row, remove_columns=\"codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/41 shards):   0%|          | 0/576820 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (41/41 shards): 100%|██████████| 576820/576820 [00:21<00:00, 27116.75 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk(\"tokenized_project_gutenberg_bytes_kokoro_speaker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens shape: torch.Size([2, 9, 8])\n",
      "\n",
      "First sequence tokens:\n",
      "tensor([[ 21,  17,  10,  78,  48, 999, 999, 999],\n",
      "        [ 58,   6,  85,  54,  88,   0,   0,   0],\n",
      "        [ 97,  89,   7,  37,  36,   0,   0,   0],\n",
      "        [ 91,  37,  32,   9,  76,   0,   0,   0],\n",
      "        [ 73,  23,  67,  80,  82,   0,   0,   0],\n",
      "        [ 91,  63,  17,  87,  50,   0,   0,   0],\n",
      "        [ 28,  65,  62,  31,  80,   0,   0,   0],\n",
      "        [  5,  30,  70,  23,  44,   0,   0,   0],\n",
      "        [ 36,  82,  58,  98,  18,   0,   0,   0]])\n",
      "\n",
      "Second sequence tokens:\n",
      "tensor([[37, 99, 29, 58, 78, 19, 17, 26],\n",
      "        [23, 59,  2, 53, 83, 66,  1, 38],\n",
      "        [74, 24,  9, 70, 49, 61, 70, 54],\n",
      "        [51, 61, 86, 16, 99, 93, 12, 14],\n",
      "        [39, 79,  4, 84, 33, 17, 98, 79],\n",
      "        [19, 68,  8, 97, 55, 93, 65, 65],\n",
      "        [ 6, 56, 99, 23,  6, 93, 64,  1],\n",
      "        [39, 57, 27, 59, 85, 61, 49, 27],\n",
      "        [91, 28, 76, 47, 86, 95, 30, 19]])\n",
      "\n",
      "Padding mask:\n",
      "tensor([[0., 0., 0., 0., 0., 1., 1., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "\n",
      "First 5 tokens of first sequence row 1:\n",
      "tensor([58,  6, 85, 54, 88])\n",
      "Next 3 tokens (should be 0s):\n",
      "tensor([0, 0, 0])\n",
      "\n",
      "First row padding for batch item 0:\n",
      "tensor([ 21,  17,  10,  78,  48, 999, 999, 999])\n",
      "\n",
      "First sequence mask (False=content, True=padding):\n",
      "tensor([0., 0., 0., 0., 0., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch, semantic_pad_id: int):\n",
    "    \"\"\"\n",
    "    batch is a list of dicts: each dict has \"tokens\" shape [9, T],\n",
    "    and \"labels\" shape [9, T].\n",
    "    We pad them into [B, 9, T_max].\n",
    "    \"\"\"\n",
    "    max_input_len = max(item[\"tokens\"].shape[1] for item in batch)\n",
    "\n",
    "    B = len(batch)\n",
    "    # We'll create padded arrays:\n",
    "    tokens = torch.full((B, 9, max_input_len), 0, dtype=torch.long)  # 2=some <PAD>\n",
    "    tokens[:, 0, :] = semantic_pad_id\n",
    "    labels = torch.full(\n",
    "        (B, 9, max_input_len), -100, dtype=torch.long\n",
    "    )  # default is ignore_index\n",
    "\n",
    "    pad_mask = torch.ones(B, max_input_len)\n",
    "\n",
    "    for i, item in enumerate(batch):\n",
    "        seq_len = item[\"tokens\"].shape[1]\n",
    "        tokens[i, :, :seq_len] = item[\"tokens\"]\n",
    "        labels[i, :, :seq_len] = item[\"labels\"][:, :seq_len]\n",
    "        pad_mask[i, :seq_len] = False\n",
    "\n",
    "    return {\"tokens\": tokens, \"labels\": labels, \"pad_mask\": pad_mask}\n",
    "\n",
    "# Create two test sequences of different lengths\n",
    "seq1 = torch.randint(1, 100, (9, 5))  # Short sequence\n",
    "seq2 = torch.randint(1, 100, (9, 8))  # Longer sequence\n",
    "\n",
    "batch = [\n",
    "    {\"tokens\": seq1, \"labels\": seq1},\n",
    "    {\"tokens\": seq2, \"labels\": seq2}\n",
    "]\n",
    "\n",
    "# Test the collation\n",
    "semantic_pad_id = 999\n",
    "result = collate_fn(batch, semantic_pad_id)\n",
    "\n",
    "print(\"Tokens shape:\", result[\"tokens\"].shape)\n",
    "print(\"\\nFirst sequence tokens:\")\n",
    "print(result[\"tokens\"][0])\n",
    "print(\"\\nSecond sequence tokens:\")\n",
    "print(result[\"tokens\"][1])\n",
    "print(\"\\nPadding mask:\")\n",
    "print(result[\"pad_mask\"])\n",
    "\n",
    "# Let's verify:\n",
    "# 1. Sequences are left-aligned\n",
    "# 2. Padding is applied correctly\n",
    "# 3. Padding mask matches content\n",
    "\n",
    "# Check alignment of first sequence (should be at start)\n",
    "print(\"\\nFirst 5 tokens of first sequence row 1:\")\n",
    "print(result[\"tokens\"][0, 1, :5])\n",
    "print(\"Next 3 tokens (should be 0s):\")\n",
    "print(result[\"tokens\"][0, 1, 5:8])\n",
    "\n",
    "# Check padding of first row\n",
    "print(\"\\nFirst row padding for batch item 0:\")\n",
    "print(result[\"tokens\"][0, 0, :8])  # Should be semantic_pad_id\n",
    "\n",
    "# Check mask alignment\n",
    "print(\"\\nFirst sequence mask (False=content, True=padding):\")\n",
    "print(result[\"pad_mask\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting sequence lengths: 100%|██████████| 149658/149658 [01:09<00:00, 2150.41 examples/s]\n",
      "Finding maximum: 100%|██████████| 149658/149658 [01:20<00:00, 1859.10 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_length(example):\n",
    "    return {'length': example['labels'].shape[1]}\n",
    "\n",
    "max_len = 0\n",
    "def update_max(example):\n",
    "    global max_len\n",
    "    max_len = max(max_len, example['length'])\n",
    "    return example\n",
    "\n",
    "# Apply the transformations\n",
    "dataset[\"train\"].map(\n",
    "    get_length,\n",
    "    desc=\"Getting sequence lengths\"\n",
    ").map(\n",
    "    update_max,\n",
    "    desc=\"Finding maximum\"\n",
    ")\n",
    "\n",
    "print(f\"Maximum sequence length: {max_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean ratio: 0.442\n",
      "Std ratio: 0.139\n",
      "\n",
      "Percentile distribution:\n",
      "1th percentile: 0.089\n",
      "5th percentile: 0.189\n",
      "25th percentile: 0.366\n",
      "50th percentile: 0.455\n",
      "75th percentile: 0.515\n",
      "95th percentile: 0.691\n",
      "99th percentile: 0.816\n",
      "\n",
      "Found 521 outliers\n",
      "\n",
      "Sample of 5 outlier examples:\n",
      "\n",
      "Index 245\n",
      "Text (278 chars): The passers by were immediately struck with wonder....\n",
      "Sequence length: 318\n",
      "Ratio: 0.874\n",
      "\n",
      "Index 1426\n",
      "Text (285 chars): \"What would you suggest!\"...\n",
      "Sequence length: 330\n",
      "Ratio: 0.864\n",
      "\n",
      "Index 1446\n",
      "Text (368 chars): As he did so, Tad felt himself gradually sinking into the sombre depths....\n",
      "Sequence length: 426\n",
      "Ratio: 0.864\n",
      "\n",
      "Index 2947\n",
      "Text (371 chars): It had its points....\n",
      "Sequence length: 419\n",
      "Ratio: 0.885\n",
      "\n",
      "Index 2971\n",
      "Text (408 chars): No answer; though I allowed a more than decent interval....\n",
      "Sequence length: 474\n",
      "Ratio: 0.861\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get arrays from dataset\n",
    "text_lengths = np.array([len(x) for x in dataset[\"train\"]['normalized_text']])\n",
    "seq_lengths = np.array([x.shape[1] for x in dataset[\"train\"]['labels']])\n",
    "\n",
    "# Calculate ratios\n",
    "ratios = text_lengths / seq_lengths\n",
    "\n",
    "# Basic stats\n",
    "print(f\"Mean ratio: {ratios.mean():.3f}\")\n",
    "print(f\"Std ratio: {ratios.std():.3f}\")\n",
    "print(f\"\\nPercentile distribution:\")\n",
    "for p in [1, 5, 25, 50, 75, 95, 99]:\n",
    "    print(f\"{p}th percentile: {np.percentile(ratios, p):.3f}\")\n",
    "\n",
    "# Find extreme outliers (3 std from mean)\n",
    "mean, std = ratios.mean(), ratios.std()\n",
    "outliers = np.where(np.abs(ratios - mean) > 3 * std)[0]\n",
    "if len(outliers) > 0:\n",
    "    print(f\"\\nFound {len(outliers)} outliers\")\n",
    "    print(\"\\nSample of 5 outlier examples:\")\n",
    "    for idx in outliers[:5]:\n",
    "        print(f\"\\nIndex {int(idx)}\")  # Convert numpy int to Python int\n",
    "        print(f\"Text ({text_lengths[idx]} chars): {dataset['val'][int(idx)]['normalized_text'][:100]}...\")  # Convert idx\n",
    "        print(f\"Sequence length: {seq_lengths[idx]}\")\n",
    "        print(f\"Ratio: {ratios[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWLINE_SEPARATOR = torch.tensor(tokenizer.encode(\"\\n\") + [0] * 8).unsqueeze(1)\n",
    "\n",
    "def batch_pack_sequences(examples, window_size=768, max_items=5):\n",
    "   \"\"\"\n",
    "   Pack sequences with system prompt and metrics\n",
    "   \"\"\"\n",
    "   packed_tokens = []\n",
    "   packed_labels = []\n",
    "   packed_speakers = []\n",
    "   pack_lengths = []\n",
    "   items_per_pack = []\n",
    "   \n",
    "   tokens = examples['tokens']\n",
    "   labels = examples['labels']\n",
    "   speakers = examples['speaker_id']\n",
    "   \n",
    "   # Account for system prompt in window size\n",
    "   effective_window = window_size - tts_sysprompt.shape[1]\n",
    "   \n",
    "   for i in range(len(tokens)):\n",
    "       seq_len = tokens[i].shape[1]\n",
    "       \n",
    "       # Start new pack\n",
    "       if i == 0 or current_length + seq_len > effective_window or \\\n",
    "          current_speaker != speakers[i] or current_items >= max_items:\n",
    "           \n",
    "           # Save previous pack if it exists\n",
    "           if i > 0 and current_tokens:\n",
    "               packed_tokens.append(torch.cat(current_tokens, dim=1))\n",
    "               packed_labels.append(torch.cat(current_labels, dim=1))\n",
    "               packed_speakers.append(current_speaker)\n",
    "               pack_lengths.append(current_length + tts_sysprompt.shape[1])\n",
    "               items_per_pack.append(current_items)\n",
    "           \n",
    "           # Initialize new pack with system prompt\n",
    "           current_tokens = [tts_sysprompt, tokens[i]]\n",
    "           current_labels = [tts_sysprompt, labels[i]]\n",
    "           current_speaker = speakers[i]\n",
    "           current_length = seq_len\n",
    "           current_items = 1\n",
    "           continue\n",
    "           \n",
    "       # Add to current pack with separator\n",
    "       current_tokens.extend([NEWLINE_SEPARATOR, tokens[i]])\n",
    "       current_labels.extend([NEWLINE_SEPARATOR, labels[i]])\n",
    "       current_length += seq_len + 1\n",
    "       current_items += 1\n",
    "   \n",
    "   # Don't forget last pack\n",
    "   if current_tokens:\n",
    "       packed_tokens.append(torch.cat(current_tokens, dim=1))\n",
    "       packed_labels.append(torch.cat(current_labels, dim=1))\n",
    "       packed_speakers.append(current_speaker)\n",
    "       pack_lengths.append(current_length + tts_sysprompt.shape[1])\n",
    "       items_per_pack.append(current_items)\n",
    "   \n",
    "   return {\n",
    "       'tokens': packed_tokens,\n",
    "       'labels': packed_labels,\n",
    "       'speaker_id': packed_speakers,\n",
    "       'pack_length': pack_lengths,\n",
    "       'items_in_pack': items_per_pack\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 149658/149658 [00:31<00:00, 4684.03 examples/s]\n",
      "Map: 100%|██████████| 5736/5736 [00:01<00:00, 4746.63 examples/s]\n",
      "Map: 100%|██████████| 4837/4837 [00:01<00:00, 4760.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Usage:\n",
    "packed_dataset = dataset.map(\n",
    "    lambda row: batch_pack_sequences(row, max_items=3),\n",
    "    batched=True,\n",
    "    remove_columns=dataset['val'].column_names,\n",
    "    batch_size=1000  # Adjust based on memory constraints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\nSpeak out the provided text<|im_end|>\\n<|im_start|>assistant\\n<|im_start|>user\\nThe weapon must still have been there.<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1049|><|semantic:1114|><|semantic:1609|><|semantic:784|><|semantic:499|><|semantic:260|><|semantic:1011|><|semantic:8|><|semantic:1407|><|semantic:540|><|semantic:1615|><|semantic:561|><|semantic:1945|><|semantic:201|><|semantic:1324|><|semantic:668|><|semantic:376|><|semantic:1849|><|semantic:9|><|semantic:1921|><|semantic:1921|><|semantic:1683|><|semantic:228|><|semantic:897|><|semantic:1677|><|semantic:518|><|im_end|>\\n<|im_start|>user\\nHow quickly he disappeared!\"<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1698|><|semantic:1848|><|semantic:1021|><|semantic:414|><|semantic:972|><|semantic:1252|><|semantic:1545|><|semantic:1363|><|semantic:307|><|semantic:722|><|semantic:1169|><|semantic:170|><|semantic:1701|><|semantic:1967|><|semantic:886|><|semantic:1540|><|semantic:1540|><|semantic:1113|><|semantic:902|><|semantic:1655|><|semantic:2009|><|semantic:56|><|semantic:1640|><|im_end|>\\n<|im_start|>user\\n\"But the blood?<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1049|><|semantic:503|><|semantic:1249|><|semantic:1039|><|semantic:1066|><|semantic:1522|><|semantic:680|><|semantic:1344|><|semantic:145|><|semantic:145|><|semantic:56|><|semantic:857|><|im_end|>\\n<|im_start|>user\\n\"So they tell me.\"<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1049|><|semantic:738|><|semantic:1669|><|semantic:1627|><|semantic:1144|><|semantic:578|><|semantic:847|><|semantic:184|><|semantic:1348|><|semantic:779|><|semantic:1715|><|semantic:658|><|semantic:141|><|semantic:844|><|im_end|>\\n<|im_start|>user\\nWhat do you make of it, Gryce?\"<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1415|><|semantic:167|><|semantic:1865|><|semantic:557|><|semantic:1000|><|semantic:49|><|semantic:1730|><|semantic:512|><|semantic:510|><|semantic:483|><|semantic:1352|><|semantic:1039|><|semantic:579|><|semantic:489|><|semantic:863|><|semantic:1388|><|semantic:874|><|semantic:632|><|semantic:527|><|semantic:1472|><|semantic:1602|><|semantic:1940|><|semantic:489|><|semantic:1331|><|im_end|>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_row = packed_dataset['val'][0]\n",
    "tokenizer.decode(example_row[\"tokens\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (5/5 shards): 100%|██████████| 50735/50735 [00:01<00:00, 33495.02 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1937/1937 [00:00<00:00, 33569.13 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1649/1649 [00:00<00:00, 31396.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "packed_dataset.save_to_disk(\"tokenized_libritts_packed_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix: Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system\\nSpeak out the provided text<|im_end|>\\n<|im_start|>user\\nI felt it in my bones when I woke this morning that something splendid was going to turn up.<|im_end|>\\n<|im_start|>assistant\\n<|semantic:1049|><|semantic:1268|><|semantic:549|><|semantic:1324|><|semantic:668|><|semantic:1538|><|semantic:1593|><|semantic:95|><|semantic:629|><|semantic:1281|><|semantic:1281|><|semantic:680|><|semantic:536|><|semantic:536|><|semantic:230|><|semantic:1018|><|semantic:1117|><|semantic:244|><|semantic:507|><|semantic:997|><|semantic:1399|><|semantic:640|><|semantic:1591|><|semantic:1967|><|semantic:1161|><|semantic:690|><|semantic:67|><|semantic:1772|><|semantic:830|><|semantic:1612|><|semantic:561|><|semantic:119|><|semantic:1052|><|semantic:880|><|semantic:1029|><|semantic:1532|><|semantic:1161|><|semantic:1344|><|semantic:1109|><|semantic:6|><|semantic:1001|><|semantic:382|><|semantic:596|><|semantic:99|><|semantic:1726|><|semantic:2030|><|semantic:531|><|semantic:616|><|semantic:367|><|semantic:1271|><|semantic:1868|><|semantic:978|><|semantic:729|><|semantic:396|><|semantic:1544|><|im_end|>'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example_row[0][\"labels\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system\\nTranscribe the provided speech<|im_end|>\\n<|im_start|>user\\n<|semantic:1049|><|semantic:1268|><|semantic:549|><|semantic:1324|><|semantic:668|><|semantic:1538|><|semantic:1593|><|semantic:95|><|semantic:629|><|semantic:1281|><|semantic:1281|><|semantic:680|><|semantic:536|><|semantic:536|><|semantic:230|><|semantic:1018|><|semantic:1117|><|semantic:244|><|semantic:507|><|semantic:997|><|semantic:1399|><|semantic:640|><|semantic:1591|><|semantic:1967|><|semantic:1161|><|semantic:690|><|semantic:67|><|semantic:1772|><|semantic:830|><|semantic:1612|><|semantic:561|><|semantic:119|><|semantic:1052|><|semantic:880|><|semantic:1029|><|semantic:1532|><|semantic:1161|><|semantic:1344|><|semantic:1109|><|semantic:6|><|semantic:1001|><|semantic:382|><|semantic:596|><|semantic:99|><|semantic:1726|><|semantic:2030|><|semantic:531|><|semantic:616|><|semantic:367|><|semantic:1271|><|semantic:1868|><|semantic:978|><|semantic:729|><|semantic:396|><|semantic:1544|><|im_end|>\\n<|im_start|>assistant\\nI felt it in my bones when I woke this morning that something splendid was going to turn up.<|im_end|>\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(example_row[1][\"labels\"][0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0, 1698, 1719,  204, 1389,  851, 1772,  186, 1307, 1895,  832,\n",
       "        1633,  771,  648, 1530, 1989, 1574, 1348,  722,  144, 1945,  278, 1109,\n",
       "          29,  611,   46,  622,  628, 1740,  572,  572,  345, 1989, 1676,  929,\n",
       "        1776,  749,  313, 1997, 1571,  819, 1238, 1054, 1054, 1135, 1506, 1393,\n",
       "         616, 1702,  993,  579,  486,  486, 2039,  148,  657,  664,  339,  339,\n",
       "         588,  212, 1443,   32, 1320, 1549,  440,    8, 1407, 1722, 1650, 1615,\n",
       "         798,  121,  303,  697,  837,  358, 1882,  440, 1992, 1992,  587,  178,\n",
       "         178, 1627, 1530,  929, 1610, 1916,  523,  213, 1252, 1480, 1468, 1899,\n",
       "         773, 2033, 2033,   83, 1146,  784, 1295,  199, 1109,  268,    6,    6,\n",
       "        1781, 1479, 1530, 1530,  146, 2038,  984, 1403,  606, 1379, 1840, 1172,\n",
       "        1680, 1162, 1928])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_row[\"tokens\"][0][1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, 1049, 1268,  549, 1324,  668, 1538, 1593,   95,  629, 1281, 1281,\n",
       "         680,  536,  536,  230, 1018, 1117,  244,  507,  997, 1399,  640, 1591,\n",
       "        1967, 1161,  690,   67, 1772,  830, 1612,  561,  119, 1052,  880, 1029,\n",
       "        1532, 1161, 1344, 1109,    6, 1001,  382,  596,   99, 1726, 2030,  531,\n",
       "         616,  367, 1271, 1868,  978,  729,  396, 1544,    0, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_row[1][\"labels\"][1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'normalized_text': 'I felt it in my bones when I woke this morning that something splendid was going to turn up.',\n",
       " 'speaker_id': '4446',\n",
       " 'id': '4446_2275_000002_000009',\n",
       " 'tokens': tensor([[    1,  9690,   198, 15024,   494,   578,   260,  2711,  1694,     2,\n",
       "            198,     1,  4093,   198,    57,  4592,   357,   281,   957,  6542,\n",
       "            645,   339, 40652,   451,  5738,   338,  1488, 33494,   436,  2045,\n",
       "            288,  1607,   614,    30,     2,   198,     1,   520,  9531,   198,\n",
       "          50201, 50420, 49701, 50476, 49820, 50690, 50745, 49247, 49781, 50433,\n",
       "          50433, 49832, 49688, 49688, 49382, 50170, 50269, 49396, 49659, 50149,\n",
       "          50551, 49792, 50743, 51119, 50313, 49842, 49219, 50924, 49982, 50764,\n",
       "          49713, 49271, 50204, 50032, 50181, 50684, 50313, 50496, 50261, 49158,\n",
       "          50153, 49534, 49748, 49251, 50878, 51182, 49683, 49768, 49519, 50423,\n",
       "          51020, 50130, 49881, 49548, 50696],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1049,  1268,   549,  1324,   668,  1538,  1593,    95,   629,  1281,\n",
       "           1281,   680,   536,   536,   230,  1018,  1117,   244,   507,   997,\n",
       "           1399,   640,  1591,  1967,  1161,   690,    67,  1772,   830,  1612,\n",
       "            561,   119,  1052,   880,  1029,  1532,  1161,  1344,  1109,     6,\n",
       "           1001,   382,   596,    99,  1726,  2030,   531,   616,   367,  1271,\n",
       "           1868,   978,   729,   396,  1544],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1470,  1879,   712,   283,   220,   137,  1610,   263,   531,  1845,\n",
       "           1428,  1132,   359,  1904,  1458,  1876,   895,   149,   190,   116,\n",
       "            603,   786,  1884,  1455,  1928,   677,   914,  1122,   436,   618,\n",
       "            850,  1766,  2005,  1618,   966,   850,  1663,   172,   274,   612,\n",
       "           1013,  1928,  1262,  1169,  1006,  1777,  1755,  2026,  1714,   788,\n",
       "            786,  1520,   811,    91,  1700],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            373,   602,  2016,  1148,    98,   790,  1570,   944,  1872,   807,\n",
       "           1427,   817,   602,  1090,   341,  1609,   774,   327,   479,   229,\n",
       "           1988,  1596,  1595,   339,  1801,  1360,  1415,   492,     3,  1064,\n",
       "            378,  1530,   800,  1369,  1331,   782,  1822,   511,   349,  1534,\n",
       "           1245,  1788,   146,  1195,  1510,   584,  1782,   662,  1523,  1523,\n",
       "             68,  2016,  1240,   783,  1178],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1225,  1475,    17,   148,   656,  2036,   663,   632,  1926,   358,\n",
       "           1443,    33,  1316,  1993,  1038,   324,  1240,   861,   787,  1994,\n",
       "           1721,   832,   707,   435,   113,   138,   494,   920,  1707,  1134,\n",
       "           1003,   842,  1511,   193,  1936,  1478,  1261,   332,   756,   115,\n",
       "           1700,   324,  1074,  1631,  1575,   526,  1957,   756,   318,    19,\n",
       "            860,  1562,   164,  1477,  1450],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            457,  2035,  1113,   504,  1301,     5,   739,   334,   178,  1832,\n",
       "           1803,   266,  1719,   516,   147,  1016,  1402,   709,  1046,  1620,\n",
       "            440,   996,   100,   551,  1799,  1930,  1811,  1551,  1026,  1440,\n",
       "            684,   313,    13,   629,  1930,   723,   457,   328,  1455,   796,\n",
       "            300,  1610,   435,   635,  1517,   958,  1607,  1013,   911,   958,\n",
       "            524,   457,  1019,  1019,   481],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1547,   592,   601,  2022,  1795,    42,  1631,  1174,   496,  1640,\n",
       "            935,  1687,   654,  1105,   668,  1995,   918,  1002,  1668,   190,\n",
       "           1148,  1509,  1563,    30,  1669,   882,  1529,  1794,  1086,   900,\n",
       "            274,  1001,   482,    42,  1852,   780,   704,   275,  1073,  1174,\n",
       "           1618,  1540,  1344,  1371,   263,  1245,  1382,   253,   484,  2044,\n",
       "           1717,  1942,   165,  1832,   478],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           1433,  1154,  1672,   879,  1683,  1529,   561,   508,  1648,   358,\n",
       "            646,  1289,  1307,  1867,  1937,  1076,  1694,  1730,  1199,  1758,\n",
       "           1476,   369,   801,  1249,   260,  1759,   740,    53,   618,  2023,\n",
       "            119,   835,  2015,   835,    66,  1209,  1285,  1744,   804,  1863,\n",
       "           1240,   392,   741,  1834,  1193,  1440,  1696,  1470,  1810,  1464,\n",
       "            840,    43,  1978,   666,  1978],\n",
       "         [    0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            948,   232,  1109,  1286,  1371,    94,  1516,   700,   714,   537,\n",
       "           1939,   334,    17,  1093,  1769,  1511,  1063,   444,  1399,  1594,\n",
       "             10,   532,   208,  1803,   945,  1193,  1082,  1667,   198,   382,\n",
       "            101,  1426,   458,  1587,  1450,  1722,  1173,  1286,   127,   225,\n",
       "           1482,  1368,   370,  1003,  1808,  1772,   952,   992,  1977,   820,\n",
       "            481,   945,   945,  1477,  1665]]),\n",
       " 'labels': tensor([[ 9690,   198, 15024,   494,   578,   260,  2711,  1694,     2,   198,\n",
       "              1,  4093,   198,    57,  4592,   357,   281,   957,  6542,   645,\n",
       "            339, 40652,   451,  5738,   338,  1488, 33494,   436,  2045,   288,\n",
       "           1607,   614,    30,     2,   198,     1,   520,  9531,   198, 50201,\n",
       "          50420, 49701, 50476, 49820, 50690, 50745, 49247, 49781, 50433, 50433,\n",
       "          49832, 49688, 49688, 49382, 50170, 50269, 49396, 49659, 50149, 50551,\n",
       "          49792, 50743, 51119, 50313, 49842, 49219, 50924, 49982, 50764, 49713,\n",
       "          49271, 50204, 50032, 50181, 50684, 50313, 50496, 50261, 49158, 50153,\n",
       "          49534, 49748, 49251, 50878, 51182, 49683, 49768, 49519, 50423, 51020,\n",
       "          50130, 49881, 49548, 50696,     2],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1049,\n",
       "           1268,   549,  1324,   668,  1538,  1593,    95,   629,  1281,  1281,\n",
       "            680,   536,   536,   230,  1018,  1117,   244,   507,   997,  1399,\n",
       "            640,  1591,  1967,  1161,   690,    67,  1772,   830,  1612,   561,\n",
       "            119,  1052,   880,  1029,  1532,  1161,  1344,  1109,     6,  1001,\n",
       "            382,   596,    99,  1726,  2030,   531,   616,   367,  1271,  1868,\n",
       "            978,   729,   396,  1544,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1470,\n",
       "           1879,   712,   283,   220,   137,  1610,   263,   531,  1845,  1428,\n",
       "           1132,   359,  1904,  1458,  1876,   895,   149,   190,   116,   603,\n",
       "            786,  1884,  1455,  1928,   677,   914,  1122,   436,   618,   850,\n",
       "           1766,  2005,  1618,   966,   850,  1663,   172,   274,   612,  1013,\n",
       "           1928,  1262,  1169,  1006,  1777,  1755,  2026,  1714,   788,   786,\n",
       "           1520,   811,    91,  1700,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   373,\n",
       "            602,  2016,  1148,    98,   790,  1570,   944,  1872,   807,  1427,\n",
       "            817,   602,  1090,   341,  1609,   774,   327,   479,   229,  1988,\n",
       "           1596,  1595,   339,  1801,  1360,  1415,   492,     3,  1064,   378,\n",
       "           1530,   800,  1369,  1331,   782,  1822,   511,   349,  1534,  1245,\n",
       "           1788,   146,  1195,  1510,   584,  1782,   662,  1523,  1523,    68,\n",
       "           2016,  1240,   783,  1178,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1225,\n",
       "           1475,    17,   148,   656,  2036,   663,   632,  1926,   358,  1443,\n",
       "             33,  1316,  1993,  1038,   324,  1240,   861,   787,  1994,  1721,\n",
       "            832,   707,   435,   113,   138,   494,   920,  1707,  1134,  1003,\n",
       "            842,  1511,   193,  1936,  1478,  1261,   332,   756,   115,  1700,\n",
       "            324,  1074,  1631,  1575,   526,  1957,   756,   318,    19,   860,\n",
       "           1562,   164,  1477,  1450,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   457,\n",
       "           2035,  1113,   504,  1301,     5,   739,   334,   178,  1832,  1803,\n",
       "            266,  1719,   516,   147,  1016,  1402,   709,  1046,  1620,   440,\n",
       "            996,   100,   551,  1799,  1930,  1811,  1551,  1026,  1440,   684,\n",
       "            313,    13,   629,  1930,   723,   457,   328,  1455,   796,   300,\n",
       "           1610,   435,   635,  1517,   958,  1607,  1013,   911,   958,   524,\n",
       "            457,  1019,  1019,   481,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1547,\n",
       "            592,   601,  2022,  1795,    42,  1631,  1174,   496,  1640,   935,\n",
       "           1687,   654,  1105,   668,  1995,   918,  1002,  1668,   190,  1148,\n",
       "           1509,  1563,    30,  1669,   882,  1529,  1794,  1086,   900,   274,\n",
       "           1001,   482,    42,  1852,   780,   704,   275,  1073,  1174,  1618,\n",
       "           1540,  1344,  1371,   263,  1245,  1382,   253,   484,  2044,  1717,\n",
       "           1942,   165,  1832,   478,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  1433,\n",
       "           1154,  1672,   879,  1683,  1529,   561,   508,  1648,   358,   646,\n",
       "           1289,  1307,  1867,  1937,  1076,  1694,  1730,  1199,  1758,  1476,\n",
       "            369,   801,  1249,   260,  1759,   740,    53,   618,  2023,   119,\n",
       "            835,  2015,   835,    66,  1209,  1285,  1744,   804,  1863,  1240,\n",
       "            392,   741,  1834,  1193,  1440,  1696,  1470,  1810,  1464,   840,\n",
       "             43,  1978,   666,  1978,     0],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   948,\n",
       "            232,  1109,  1286,  1371,    94,  1516,   700,   714,   537,  1939,\n",
       "            334,    17,  1093,  1769,  1511,  1063,   444,  1399,  1594,    10,\n",
       "            532,   208,  1803,   945,  1193,  1082,  1667,   198,   382,   101,\n",
       "           1426,   458,  1587,  1450,  1722,  1173,  1286,   127,   225,  1482,\n",
       "           1368,   370,  1003,  1808,  1772,   952,   992,  1977,   820,   481,\n",
       "            945,   945,  1477,  1665,     0]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
